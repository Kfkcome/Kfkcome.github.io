<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh-Hans"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="zh-Hans" /><updated>2024-11-29T16:19:03+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ennis’s Blog</title><subtitle>Willing to be a question, willing to be an answer.
</subtitle><author><name>Xingjie Gao</name><email>xingjie-gao@outlook.com</email></author><entry><title type="html">Tool Learning through Simulated Trial and Error论文笔记</title><link href="http://localhost:4000/2024/11/29/Tool-Learning-through-Simulated-Trial-and-Error%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.html" rel="alternate" type="text/html" title="Tool Learning through Simulated Trial and Error论文笔记" /><published>2024-11-29T00:00:00+08:00</published><updated>2024-11-29T00:00:00+08:00</updated><id>http://localhost:4000/2024/11/29/Tool%20Learning%20through%20Simulated%20Trial%20and%20Error%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0</id><content type="html" xml:base="http://localhost:4000/2024/11/29/Tool-Learning-through-Simulated-Trial-and-Error%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.html"><![CDATA[]]></content><author><name>Xingjie Gao</name><email>xingjie-gao@outlook.com</email></author><category term="nlp" /><category term="自然语言处理" /><category term="论文笔记" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">LoRA LOW-RANK ADAPTATION 论文笔记</title><link href="http://localhost:4000/2024/11/18/LoRA-LOW-RANK-ADAPTATION-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.html" rel="alternate" type="text/html" title="LoRA LOW-RANK ADAPTATION 论文笔记" /><published>2024-11-18T00:00:00+08:00</published><updated>2024-11-18T00:00:00+08:00</updated><id>http://localhost:4000/2024/11/18/LoRA%20LOW-RANK%20ADAPTATION%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0</id><content type="html" xml:base="http://localhost:4000/2024/11/18/LoRA-LOW-RANK-ADAPTATION-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.html"><![CDATA[<h1 id="lora-low-rank-adaptation-论文笔记">LoRA LOW-RANK ADAPTATION 论文笔记</h1>

<!---more-->

<h2 id="不懂的问题">不懂的问题</h2>

<h3 id="线性代数基础">线性代数基础</h3>

<p>根据线性代数的性质：</p>
<ul>
  <li>若 $B \in \mathbb{R}^{512 \times 4}$ 和 $A \in \mathbb{R}^{4 \times 512}$，则矩阵乘积 $\Delta W = B A$ 的最大秩 $\text{rank}(\Delta W)$ 由 $B$ 和 $A$ 的秩中较小的那个决定：
\(\text{rank}(\Delta W) \leq \min (\text{rank}(B), \text{rank}(A))\)</li>
  <li>在 LoRA 的低秩分解中：
    <ul>
      <li>$B$ 的列数是 4，其秩最多为 4。</li>
      <li>$A$ 的行数是 4，其秩最多为 4。</li>
      <li>因此，$\Delta W = B A$ 的秩最多为 4。</li>
    </ul>
  </li>
  <li>虽然 $\Delta W$ 的尺寸是 $512 \times 512$，但它的 <strong>列空间维度（rank）</strong> 被限制在 4，即列向量最多线性独立的维度是 4。</li>
</ul>

<p>假设我们有一个 $512 \times 512$ 的矩阵更新：</p>
<ul>
  <li>如果直接更新 $\Delta W$ 的每个元素（完整微调），那么矩阵的秩可以达到最大值 512。</li>
  <li>LoRA 的假设是任务适配过程中权重更新的本质维度较低，因此可以用 <strong>4 个基向量（来自 $B$ 的列空间）</strong> 和它们的线性组合来近似表示更新。</li>
</ul>

<p>这意味着即使 $\Delta W$ 的尺寸是 $512 \times 512$，其独立的方向（秩）是受 $r = 4$ 的限制的。</p>

<p>所以这正是为什么通过调整 $r$ 可以控制 LoRA 的表达能力：$r$ 越大，权重更新的潜在方向越多，表达能力越强；当 $r$ 接近 512 时，LoRA 的表达能力接近完整微调。</p>

<h2 id="摘要">摘要</h2>

<h3 id="问题">问题</h3>

<p>模型越来越大，全参数 SFT 变的不可行</p>

<h3 id="作者的工作">作者的工作</h3>

<p>LORA：该方法冻结预训练模型的权重，并将可训练的秩分解矩阵注入到Transformer架构的每一层，极大地减少了下游任务的可训练参数数量。</p>

<h3 id="结果">结果</h3>

<ul>
  <li>与使用Adam微调的GPT-3 175B相比，LoRA可以将可训练参数的数量减少10,000倍，并将GPU内存需求减少3倍。</li>
  <li>尽管可训练参数较少，训练吞吐量更高，并且与适配器不同，不会增加额外的推理延迟，LoRA在RoBERTa、DeBERTa、GPT-2和GPT-3的模型质量上表现与微调持平或更好</li>
</ul>

<h2 id="引言">引言</h2>

<ul>
  <li>问题：模型越来越大，全参数 SFT 变的不可行</li>
  <li>现有解决方法：针对不同任务添加一些参数或者适配器
    <ul>
      <li>好处：只需要存储和加载少量的任务特定参数，除了每个任务的预训练模型外，极大地提高了部署时的操作效率。</li>
      <li>缺点：适配器会增加推理了延迟、无法与达到微调基础，在效率和质量直接由权衡</li>
    </ul>
  </li>
</ul>

<h3 id="lora">LoRA</h3>

<ol>
  <li>灵感：LoRA 的灵感来源于研究表明，<strong>过参数化的模型其实存在于一个低维空间</strong>。也就是说，在模型适应（fine-tuning）过程中，权重的变化可以用一个低秩结构表示（即低秩分解）。</li>
  <li>核心思想：
    <ul>
      <li><strong>冻结预训练模型权重</strong>：只优化特定层的“变化部分”。</li>
      <li><strong>低秩分解</strong>：通过优化某些密集层变化的低秩分解矩阵（如图中提到的矩阵 AAA 和 BBB），实现模型适应。</li>
      <li>
        <p>例如，GPT-3 175B 预训练模型中的一个矩阵，其原始秩（rank）为 12,288，LoRA 只需优化秩 r=1r=1r=1 或 r=2r=2r=2 的矩阵就能完成 fine-tuning。</p>

        <p><img src="/assets/posts_assets/Pasted%20image%2020241120185140.png" alt="" /></p>
      </li>
    </ul>
  </li>
</ol>

<p>这张图是 <strong>LoRA（Low-Rank Adaptation）</strong> 方法的关键机制示意图，展示了如何对预训练模型中的权重矩阵 $W \in \mathbb{R}^{d \times d}$ 进行低秩分解和优化。</p>

<hr />

<h4 id="图中元素解释">图中元素解释</h4>

<ol>
  <li><strong>Pretrained Weights $W$</strong>：
    <ul>
      <li>$W$ 是预训练模型中的一个权重矩阵，其维度为 $d \times d$。</li>
      <li>在 LoRA 方法中，这个矩阵 <strong>保持冻结（不更新）</strong>。</li>
    </ul>
  </li>
  <li><strong>输入 $x$ 和输出 $h$</strong>：
    <ul>
      <li>输入 $x$ 是特征向量，其维度为 $d$。</li>
      <li>输出 $h$ 是经过 LoRA 适配后的向量，作为网络的输出，维度同样为 $d$。</li>
    </ul>
  </li>
  <li><strong>低秩分解矩阵 $A$ 和 $B$</strong>：
    <ul>
      <li>LoRA 引入两个小规模的低秩矩阵 $A \in \mathbb{R}^{d \times r}$ 和 $B \in \mathbb{R}^{r \times d}$，其中 $r \ll d$（例如 $r=1$ 或 $r=2$）。</li>
      <li>$A$ 和 $B$ 的作用是近似描述 $W$ 的变化（即微调时需要的调整量 $\Delta W$）。</li>
      <li>在初始化时：
        <ul>
          <li>$A$ 通常从高斯分布 $\mathcal{N}(0, \sigma^2)$ 中随机采样。</li>
          <li>$B$ 初始化为零矩阵。</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>模型调整机制</strong>：
    <ul>
      <li>微调时，仅优化 $A$ 和 $B$ 的参数，而不改变 $W$。</li>
      <li>输入 $x$ 经过计算后产生一个调整量：
\(\Delta h = B \cdot (A \cdot x)\)</li>
      <li>最终输出为：
\(H = W \cdot x + \Delta h\)</li>
      <li>这表明模型输出是预训练权重 $W$ 的结果加上一个低秩调整项。</li>
    </ul>
  </li>
</ol>

<h4 id="lora-优势">LoRA 优势</h4>

<ol>
  <li>
    <p><strong>共享和高效任务切换</strong>：</p>

    <ul>
      <li><strong>模块化</strong>：预训练模型可以用于多个任务。LoRA 通过替换不同的低秩矩阵 AAA 和 BBB，实现了高效的任务切换。</li>
      <li><strong>减少存储需求</strong>：因为只需要存储这些小的矩阵，而不是整个模型参数。</li>
    </ul>
  </li>
  <li>
    <p><strong>提高训练效率</strong>：</p>

    <ul>
      <li><strong>降低硬件门槛</strong>：相比传统方法，LoRA <strong>只优化小规模矩阵</strong>，因此显著减少计算量。根据文中描述，可以将所需的硬件资源减少到原来的三分之一。</li>
    </ul>
  </li>
  <li>
    <p><strong>零推理延迟</strong>：</p>

    <ul>
      <li>LoRA 的设计可以在部署时将优化的矩阵与预训练权重合并，因此不会增加推理延迟。</li>
    </ul>
  </li>
  <li>
    <p><strong>方法的通用性和组合性</strong>：</p>

    <ul>
      <li>LoRA <strong>独立于其他方法</strong>，如前缀微调（prefix-tuning），并且可以与这些方法组合使用以增强效果。</li>
    </ul>
  </li>
</ol>

<h3 id="术语约定">术语约定</h3>

<ol>
  <li><strong>关于 Transformer 的术语和维度</strong>
    <ul>
      <li><strong>$d_{\text{model}}$</strong>：
        <ul>
          <li>表示 Transformer 模型中某一层的输入和输出向量的维度。</li>
          <li>在 Transformer 的每一层中，所有的输入和输出张量都会使用这个维度。</li>
        </ul>
      </li>
      <li><strong>投影矩阵（Projection Matrices）</strong>：
        <ul>
          <li>$W_q$：用于生成 <strong>query</strong>（查询向量）的投影矩阵。</li>
          <li>$W_k$：用于生成 <strong>key</strong>（键向量）的投影矩阵。</li>
          <li>$W_v$：用于生成 <strong>value</strong>（值向量）的投影矩阵。</li>
          <li>$W_o$：表示 <strong>输出投影矩阵</strong>。</li>
          <li>这些矩阵都位于 <strong>自注意力模块（self-attention module）</strong> 内，是 Transformer 的核心部分。</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<hr />

<ol>
  <li><strong>权重的定义</strong>
    <ul>
      <li><strong>$W$ 或 $W_0$</strong>：
        <ul>
          <li>代表 Transformer 中某一层的 <strong>预训练权重矩阵</strong>，这些矩阵通过预训练模型获得，在微调时被冻结（即不被更新）。</li>
        </ul>
      </li>
      <li><strong>$\Delta W$</strong>：
        <ul>
          <li>代表 LoRA 在微调过程中累计更新的权重变化量。</li>
          <li>LoRA 的核心思想是通过优化这个权重变化量，而不是直接修改原始的权重矩阵 $W$。</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<hr />

<ol>
  <li><strong>LoRA 的秩（Rank）$r$</strong>
    <ul>
      <li><strong>$r$</strong>：
        <ul>
          <li>表示 LoRA 中低秩分解矩阵的秩（rank）。</li>
          <li>$r$ 通常远小于 $d_{\text{model}}$，用来表示矩阵分解的低维度，从而降低优化问题的计算复杂度。</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<hr />

<ol>
  <li><strong>优化方法和架构设置</strong>
    <ul>
      <li><strong>优化方法</strong>：
        <ul>
          <li>作者采用了 <strong>Adam</strong> 优化器，这是一个在深度学习中常用的自适应学习率优化算法（参考：Loshchilov &amp; Hutter, 2019 和 Kingma &amp; Ba, 2017）。</li>
        </ul>
      </li>
      <li><strong>MLP 维度设置</strong>：
        <ul>
          <li>Transformer 中多层感知机（MLP）的隐藏层维度定义为：
\(D_{\text{ffn}} = 4 \times d_{\text{model}}\)</li>
          <li>意味着 MLP 的隐藏层通常比输入输出的维度大 4 倍，用于增强模型的非线性表达能力。</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h2 id="问题陈述">问题陈述</h2>

<p>LoRA 不涉及 loss 的设计，而是专注于语言建模任务的优化方法</p>

<h3 id="1-背景预训练模型">1. 背景：预训练模型</h3>

<p>假设我们有一个预训练的自回归语言模型（autoregressive language model）：
     \(P_\Phi (y|x)\)
     其中：
     - $x$：输入序列（例如任务的 prompt 或上下文）。
     - $y$：输出序列（例如模型生成的文本）。
     - $\Phi$：模型参数。</p>

<ul>
  <li>被用于解决多种下游任务（downstream tasks），如文本摘要（summarization）、阅读理解（MRC）、自然语言到 SQL 转换（NL 2 SQL）等。</li>
  <li><strong>任务定义</strong>：
    <ul>
      <li>每个下游任务通过一个训练数据集表示：
\(\mathcal{Z} = \{(x_i, y_i)\}_{i=1}^N\)
其中 $x_i$ 是输入序列，$y_i$ 是目标序列（输出）。
        <ul>
          <li>例如，在 NL 2 SQL 任务中：
            <ul>
              <li>$x_i$：自然语言查询。</li>
              <li>$y_i$：对应的 SQL 命令。</li>
            </ul>
          </li>
          <li>在摘要任务中：
            <ul>
              <li>$x_i$：文章内容。</li>
              <li>$y_i$：文章摘要。</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="2-完整微调full-fine-tuning的问题">2. 完整微调（Full Fine-Tuning）的问题</h3>

<ul>
  <li>在传统的完整微调方法中：
    <ul>
      <li>模型会从预训练权重 $\Phi_0$ 开始进行优化。</li>
      <li>更新后的模型参数为：
\(\Phi = \Phi_0 + \Delta \Phi\)
其中 $\Delta \Phi$ 是模型在下游任务上的更新。</li>
    </ul>
  </li>
  <li>目标是最大化条件语言建模目标：
\(\max_{\Phi} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log P_\Phi (y_t | x, y_{&lt;t})\)
这里：
    <ul>
      <li>$y_t$：序列的第 $t$ 个 token。</li>
      <li>$y_{&lt;t}$：序列的前 $t-1$ 个 token。</li>
    </ul>
  </li>
  <li><strong>主要问题</strong>：
    <ol>
      <li><strong>存储需求</strong>：
        <ul>
          <li>对于每个下游任务，需要学习不同的参数 $\Delta \Phi$，其维度和 $\Phi_0$ 一样大。</li>
          <li>如果预训练模型很大（例如 GPT-3，参数量高达 175 亿），存储多个独立的模型实例变得非常困难甚至不可行。</li>
        </ul>
      </li>
      <li><strong>部署复杂性</strong>：
        <ul>
          <li>部署和维护多个完整微调后的模型对存储和计算资源要求很高。</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<h3 id="3-lora-的方法参数高效微调">3. LoRA 的方法：参数高效微调</h3>
<ul>
  <li>为了解决上述问题，LoRA 提出了一个 <strong>更高效的微调方法</strong>，核心是：
    <ul>
      <li>将任务特定的参数增量 $\Delta \Phi$ 编码为一个 <strong>低维参数集</strong> $\Theta$，满足：
\(|\Theta| \ll |\Phi_0|\)</li>
      <li>这样，微调的任务变成了优化 $\Theta$：
\(\max_{\Theta} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log P_{\Phi_0 + \Delta \Phi (\Theta)}(y_t | x, y_{&lt;t})\)</li>
    </ul>
  </li>
  <li><strong>低秩表示的优点</strong>：
    <ul>
      <li>通过使用低秩分解（low-rank representation），可以显著减少存储和计算成本。</li>
      <li>当模型是 GPT-3（175 B 参数量）时，$\Theta$ 的规模可以小到原始参数规模的 <strong>0.01%</strong>。</li>
    </ul>
  </li>
</ul>

<h2 id="现有的方法">现有的方法</h2>

<h3 id="迁移学习的现有方法">迁移学习的现有方法</h3>

<ul>
  <li>转移学习的方法有很多，当前主流方法包括：
    <ol>
      <li><strong>适配器层（Adapter Layers）</strong>：通过在模型的 Transformer 层中插入小型适配器模块进行调整。</li>
      <li><strong>Prompt Tuning</strong>：通过优化输入提示（Prompt）来适配下游任务。</li>
    </ol>
  </li>
  <li>这些方法在参数和计算效率上取得了进展，但在 <strong>大规模生产环境</strong> 中仍存在明显缺点，例如增加推理延迟或优化难度。</li>
</ul>

<p><img src="/assets/posts_assets/Pasted%20image%2020241120195037.png" alt="" /></p>
<h3 id="适配器层的不足引入推理延迟">适配器层的不足：引入推理延迟</h3>

<h4 id="1-适配器层的设计">(1) <strong>适配器层的设计</strong>：</h4>
<ul>
  <li>原始适配器层设计（例如 Houlsby et al., 2019）在每个 Transformer 块中插入两层适配器。</li>
  <li>后续方法（例如 Lin et al., 2020）优化了设计，减少为每块一层，并引入了额外的 <strong>LayerNorm</strong>，进一步降低了延迟。</li>
</ul>

<h4 id="2-延迟来源">(2) <strong>延迟来源</strong>：</h4>
<ul>
  <li><strong>顺序处理的限制</strong>：适配器层尽管参数量很小（通常 &lt;1% 的原始模型参数），但它们需要逐层顺序执行，限制了并行处理能力。</li>
  <li><strong>延迟问题明显</strong>：在在线推理（batch size 较小，例如 batch size = 1）中，由于缺乏并行性，适配器层显著增加了推理延迟。</li>
  <li><strong>分片（Sharding）的复杂性</strong>：在模型分片场景（如 Shoeybi et al., 2020）中，适配器层会增加更多 GPU 间的同步操作（如 <strong>AllReduce</strong> 和 <strong>Broadcast</strong>），进一步提高了开销。</li>
</ul>

<h4 id="3-表格中的数据说明">(3) <strong>表格中的数据说明</strong>：</h4>
<ul>
  <li>表格展示了 GPT-2 模型在使用适配器层和其他方法时的推理延迟：
    <ul>
      <li><strong>Fine-Tune/LoRA</strong> 方法延迟最小（例如 batch size = 1 时仅 19.8 毫秒）。</li>
      <li><strong>Adapter Layers</strong> 方法的延迟明显更高：
        <ul>
          <li>Adapter$L$ 和 Adapter$H$ 分别增加了 20.7% 和 30.3% 的延迟。</li>
          <li>延迟增加在小 batch size 场景尤为明显。</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<h3 id="prompt-tuning-的不足">Prompt Tuning 的不足</h3>
<h4 id="1-优化难度">(1) <strong>优化难度</strong>：</h4>
<ul>
  <li>Prompt Tuning 的优化过程较难，表现为：
    <ul>
      <li>随着可训练参数增加，性能的变化呈现 <strong>非单调性</strong>（即增加参数量并不一定提升性能）。</li>
      <li>作者观察到这种现象在原始论文（Li &amp; Liang, 2021）中也被证实。</li>
    </ul>
  </li>
</ul>

<h4 id="2-序列长度限制">(2) <strong>序列长度限制</strong>：</h4>
<ul>
  <li>Prompt Tuning 需要保留一部分序列作为 Prompt，这会减少可用于任务处理的序列长度，进而限制模型性能。</li>
  <li>例如，原始输入序列长度可能被占用一部分用于 Prompt，从而减少了处理有效任务的空间。</li>
</ul>

<h4 id="3-性能潜力受限">(3) <strong>性能潜力受限</strong>：</h4>
<ul>
  <li>Prompt Tuning 在下游任务的表现可能不如其他方法，例如适配器层或 LoRA。</li>
</ul>

<h2 id="lora-方法">LoRA 方法</h2>

<p>LoRA原则上适用于深度学习模型中的任何稠密层，尽管在我们的实验中我们只关注 Transformer 语言模型中的某些权重作为激励使用案例。</p>

<h3 id="通过低秩分解low-rank-decomposition来高效更新权重矩阵">通过低秩分解（low-rank decomposition）来高效更新权重矩阵</h3>

<p><strong>1. 低秩参数化更新矩阵（Low-Rank-Parameterized Update Matrices）</strong></p>
<ul>
  <li>神经网络的权重矩阵通常是全秩（full-rank）的。</li>
  <li>LoRA 假设：预训练语言模型的权重更新可以被约束在一个 <strong>低秩子空间</strong> 中。</li>
  <li>核心公式：
\(W_0 + \Delta W = W_0 + BA\)
    <ul>
      <li>$W_0 \in \mathbb{R}^{d \times k}$：预训练模型中的冻结权重矩阵。</li>
      <li>$\Delta W \in \mathbb{R}^{d \times k}$：权重的更新量。</li>
      <li>$B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}$：两个可训练的低秩矩阵，其中 $r \ll \min (d, k)$。</li>
      <li><strong>低秩分解的核心思想</strong>：通过约束 $\Delta W$ 为低秩表示，显著减少了优化参数量。</li>
    </ul>
  </li>
  <li>在训练过程中：
    <ul>
      <li>$W_0$ 冻结，不会更新。</li>
      <li>$A$ 和 $B$ 是唯一需要优化的矩阵。</li>
      <li>输入向量 $x$ 的前向计算公式变为：
\(H = W_0 x + \Delta Wx = W_0 x + BAx\)</li>
    </ul>
  </li>
</ul>

<hr />

<ol>
  <li><strong>初始化和优化细节</strong>
    <ul>
      <li>初始化方式：
        <ul>
          <li>$A$ 的初始值来自随机高斯分布。</li>
          <li>$B$ 的初始值为零矩阵。</li>
          <li>因此，$\Delta W = BA = 0$ 在训练开始时为零。</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<ul>
  <li>缩放机制
    <ul>
      <li>$\Delta W$ 被缩放为：
\(\Delta W = \frac{\alpha}{r} BA\)
其中 $\alpha$ 是一个比例常数，与秩 $r$ 成反比。$\alpha$ 是缩放因子，用于调整更新幅度</li>
      <li>调整 $\alpha$ 的作用类似于调整学习率，可以减少因 $r$ 变化导致的超参数重新调整需求。</li>
    </ul>
  </li>
</ul>

<p>这段话主要说明了 <strong>LoRA（Low-Rank Adaptation）</strong> 是完整微调（full fine-tuning）的一种泛化形式，并对其表达能力及与其他方法的对比做出分析。以下是详细解释：</p>

<hr />

<p><strong>3. 适用于所有全参数微调</strong></p>

<ul>
  <li><strong>完整微调的传统方式</strong>：
    <ul>
      <li>一般来说，完整微调会训练预训练模型中的 <strong>部分参数</strong> 或者 <strong>全部参数</strong>。</li>
      <li>这些训练参数可以包括权重矩阵以及模型中的偏置（bias）。</li>
    </ul>
  </li>
  <li><strong>LoRA 的突破</strong>：
    <ul>
      <li>LoRA 提出了一种更通用的微调形式：
        <ul>
          <li>不需要直接调整完整的权重矩阵（full-rank updates），而是通过低秩分解（low-rank decomposition）来表示权重更新。</li>
          <li>在应用 LoRA 时，对所有权重矩阵应用低秩更新，同时训练模型中的所有偏置项，可以近似恢复完整微调的表达能力</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>LoRA 的效果
    <ul>
      <li>效果主要取决于 LoRA 的秩 $r$：
        <ul>
          <li>LoRA 中的权重更新是通过两个低秩矩阵 $B$ 和 $A$ 表示的，秩 $r$ 决定了更新矩阵的表达能力。</li>
          <li>当 $r$ 增加并接近于原始权重矩阵的秩时，LoRA 的表达能力接近于完整微调。</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>与其他方法的对比
    <ul>
      <li><strong>适配器层方法（adapter-based methods）</strong>：
        <ul>
          <li>适配器方法在微调时，会插入额外的层进行任务适配。</li>
          <li>缺点：
            <ul>
              <li>当增加可训练参数时，这些方法的表现最终会趋于类似于一个多层感知机（MLP），难以与完整微调匹敌。</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>基于前缀的微调方法（prefix-based methods）</strong>：
        <ul>
          <li>例如 Prompt Tuning，通过优化前缀（prompt）来适配任务。</li>
          <li>缺点：
            <ul>
              <li>受限于输入序列的长度（部分序列长度被前缀占用），在长输入序列的任务中表现不佳。</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>LoRA 的优势</strong>：
        <ul>
          <li>通过调整秩 $r$，LoRA 在模型训练参数的规模增加时，可以更接近完整微调的表现，同时不会受到序列长度或其他限制。</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<p><strong>4. 推理时的零延迟（No Additional Inference Latency）</strong></p>
<ul>
  <li><strong>推理阶段的操作</strong>：
    <ul>
      <li>在生产部署中，可以直接计算并存储：
\(W = W_0 + BA\)</li>
      <li>推理时，模型直接使用 $W$ 进行计算，与原始全权重模型完全一致，无需额外计算。</li>
    </ul>
  </li>
  <li><strong>切换任务</strong>：
    <ul>
      <li>当需要切换到其他任务时，只需移除现有的 $BA$，并替换为新的 $B’A’$，这一过程非常快且占用极少的内存。</li>
      <li>这种特性特别适合需要频繁切换任务的应用场景。</li>
    </ul>
  </li>
</ul>

<h3 id="将-lora-应用于-transformer-模型">将 LoRA 应用于 Transformer 模型</h3>

<p>好处：</p>

<ol>
  <li>内存和存储的减少
    <ul>
      <li>内存的减少：VRAM 减少了 2/3 ，因为冻结的参数不需要为这些参数存储梯度和优化器状态，显存需求降低至原来的 1/3（1.2 TB -&gt; 350 GB）</li>
      <li>存储的减少：模型的保存文件减少了 10,000 倍（350 GB -&gt; 35 MB）</li>
    </ul>
  </li>
  <li>切换任务代价更低：我们可以在部署时以更低的成本在任务之间切换，只需交换 LoRA 权重，而不是所有参数</li>
  <li>训练速度也提高了 25%：因为不需要计算绝大多数参数的梯度。</li>
</ol>

<p>缺点：</p>

<p>不同任务的批量处理不行</p>

<ul>
  <li>例如，如果选择将 A 和 B 吸收到 W 中以消除额外的推理延迟，那么在单次前向传播中，将不同任务的不同 A 和 B 批量输入并不是很简单。</li>
  <li>尽管在延迟不关键的场景中，可以不合并权重，并动态选择批次中样本使用的 LoRA 模块。</li>
</ul>

<h2 id="实验">实验</h2>

<p><img src="/assets/posts_assets/Pasted%20image%2020241121102730.png" alt="" /></p>

<p><img src="/assets/posts_assets/Pasted%20image%2020241121102718.png" alt="" />
<img src="/assets/posts_assets/Pasted%20image%2020241121102741.png" alt="" /></p>

<p><img src="/assets/posts_assets/Pasted%20image%2020241121102835.png" alt="" />GPT-3 175B 验证准确率与 WikiSQL 和 MNLI-matched 上多种适应方法的可训练参数数量。LoRA 展现了更好的可扩展性和任务性能。</p>

<h2 id="理解-lora">理解 LoRA</h2>

<p>要理解三个问题：</p>

<p>1) 在参数预算约束下，应该调整预训练 Transformer 中的哪一部分权重矩阵以最大化下游性能？
2) “最优”调整矩阵∆W 真的是秩亏的吗？如果是，实际中使用哪个秩比较好？
3) ∆W 与 W 之间有什么联系？∆W 是否与 W 高度相关？与 W 相比，∆W 有多大？</p>

<h3 id="问题一哪些权重应该调整">问题一：哪些权重应该调整？</h3>

<p><img src="/assets/posts_assets/Pasted%20image%2020241121103550.png" alt="" /></p>

<p>这部分讨论了在参数预算有限的情况下，应该对 Transformer 中的哪些权重矩阵应用 LoRA，以在下游任务中获得最佳性能。以下是详细解释：</p>

<hr />

<p><strong>1. 研究目标</strong></p>
<ul>
  <li><strong>问题</strong>：在给定的参数预算（例如 18 M 可训练参数）下，如何选择 Transformer 模型中需要 LoRA 适配的权重矩阵类型，以获得最佳的任务性能。</li>
  <li><strong>背景</strong>：Transformer 的自注意力模块包含四种主要的权重矩阵：$W_q, W_k, W_v, W_o$。LoRA 可以单独适配某一个矩阵，也可以组合适配多个矩阵。</li>
  <li><strong>目标</strong>：探索不同权重组合的适配效果，并分析对验证集准确率的影响。</li>
</ul>

<hr />

<p><strong>2. 实验设置</strong></p>
<ul>
  <li><strong>参数预算</strong>：
    <ul>
      <li>总的可训练参数设置为 <strong>18 M</strong>。</li>
      <li>在存储为 FP 16 格式时，占用约 <strong>35 MB</strong> 的存储空间。</li>
    </ul>
  </li>
  <li><strong>秩 $r$</strong>：
    <ul>
      <li>如果只适配一个矩阵，则 $r = 8$。</li>
      <li>如果适配两个矩阵，则每个矩阵的 $r = 4$。</li>
      <li>如果适配更多矩阵（例如四个矩阵），则 $r$ 会进一步降低，以满足总参数预算。</li>
    </ul>
  </li>
  <li><strong>模型</strong>：
    <ul>
      <li>使用 GPT-3 175 B 作为基础模型。</li>
      <li>在两个任务上验证效果：
        <ul>
          <li><strong>WikiSQL</strong>（±0.5% 的标准差）：结构化查询的自然语言转换任务。</li>
          <li><strong>MultiNLI</strong>（±0.1% 的标准差）：文本蕴含任务。</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>3. 结论分析</strong></p>
<ol>
  <li><strong>单独适配某一种权重矩阵</strong>：
    <ul>
      <li>适配单个矩阵时，准确率相对较低，尤其是 $W_q$ 和 $W_k$ 单独适配的表现明显低于组合适配。</li>
    </ul>
  </li>
  <li><strong>组合适配的优越性</strong>：
    <ul>
      <li>适配 $W_q$ 和 $W_v$ 的组合效果最佳。</li>
      <li>这表明，即使每种矩阵的秩较低（如 $r = 4$），组合适配多个矩阵可以捕获更多的任务相关信息，从而比单独适配一个矩阵表现更好。</li>
    </ul>
  </li>
  <li><strong>适配更多矩阵（如 $W_q, W_k, W_v, W_o$）</strong>：
    <ul>
      <li>在 MultiNLI 上，适配所有四个矩阵的表现达到最优（91.7%）。</li>
      <li>说明在更复杂的任务中，适配更多权重矩阵（即使每个矩阵的秩更低）有助于提升模型的适配能力。</li>
    </ul>
  </li>
</ol>

<hr />

<p><strong>4. 结论</strong></p>
<ul>
  <li><strong>有效利用参数预算</strong>：
    <ul>
      <li>将参数预算分配给多个权重矩阵的适配，比将预算集中于单个矩阵更有效。</li>
      <li>即使每个矩阵的秩较低，组合适配能更好地捕获任务所需的信息。</li>
    </ul>
  </li>
  <li><strong>权重选择的重要性</strong>：
    <ul>
      <li>$W_q$ 和 $W_v$ 的组合对性能提升尤为重要。</li>
      <li>表明在 Transformer 中，不同权重矩阵对任务的影响不同，选择关键权重进行适配是提高性能的关键。</li>
    </ul>
  </li>
  <li><strong>任务复杂性影响</strong>：
    <ul>
      <li>对于更复杂的任务（如 MultiNLI），适配更多权重矩阵效果更好。</li>
    </ul>
  </li>
</ul>

<h3 id="问题二最优秩是多少">问题二：最优秩是多少？</h3>

<p><img src="/assets/posts_assets/Pasted%20image%2020241121104154.png" alt="" /></p>

<p>令我们惊讶的是，在这些数据集中，像一个这样的较小排名就足以适应 Wq 和 Wv，而单独训练 Wq 需要更大的 r。</p>

<p>增加 r 并不会覆盖更有意义的子空间，这表明低秩适应矩阵是足够的。</p>

<h4 id="探讨不同秩的空间相似性">探讨不同秩的空间相似性</h4>

<p>讨论在不同秩 $r$ 的情况下，LoRA 的低秩更新矩阵 $A_{r=8}$ ​ 和 $A_{r=64}$ ​ 的子空间相似性。通过子空间分析，解释了为何较低的 $r$（如 $r=1$ 或 $r=8$）能够在任务中表现良好。
<img src="/assets/posts_assets/Pasted%20image%2020241121110556.png" alt="" /></p>

<h5 id="研究目标">研究目标</h5>
<ul>
  <li><strong>目标</strong>：
    <ul>
      <li>探索低秩分解中不同秩 $r$ 的矩阵 $A$ 是否共享重要的子空间。</li>
      <li>进一步解释为何较低的秩（如 $r=1$ 或 $r=8$）仍能有效表示权重更新。</li>
    </ul>
  </li>
  <li><strong>方法</strong>：
    <ul>
      <li>使用奇异值分解（SVD）得到适配矩阵 $A_{r=8}$ 和 $A_{r=64}$ 的右奇异向量矩阵（right-singular unitary matrices）：
\(U_{A_{r=8}}, U_{A_{r=64}}\)</li>
      <li>研究 $U_{A_{r=8}}$ 的前 $i$ 个奇异向量与 $U_{A_{r=64}}$ 的前 $j$ 个奇异向量之间的子空间重叠情况。</li>
    </ul>
  </li>
</ul>

<h5 id="子空间相似性定义">子空间相似性定义</h5>
<ul>
  <li><strong>子空间相似性度量</strong>：
    <ul>
      <li>使用基于 Grassmann 距离的归一化相似性指标来量化子空间重叠：
\(\phi (A_{r=8}, A_{r=64}, i, j) = \frac{\|\left (U_{A_{r=8}}^i\right)^T U_{A_{r=64}}^j\|_F^2}{\min (i, j)}\)
        <ul>
          <li>$U_{A_{r=8}}^i$：$U_{A_{r=8}}$ 的前 $i$ 个奇异向量列。</li>
          <li>$U_{A_{r=64}}^j$：$U_{A_{r=64}}$ 的前 $j$ 个奇异向量列。</li>
          <li>$\phi$ 的值范围为 $[0, 1]$，其中：
            <ul>
              <li>$\phi = 1$：两个子空间完全重叠。</li>
              <li>$\phi = 0$：两个子空间完全分离。</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>研究重点</strong>：
    <ul>
      <li>探索 $A_{r=8}$ 和 $A_{r=64}$ 在不同的 $i$ 和 $j$ 组合下的子空间重叠情况。</li>
      <li>仅分析第 48 层的结果，但结果适用于所有层。</li>
    </ul>
  </li>
</ul>

<hr />

<h5 id="图表解析">图表解析</h5>
<p>图 3 展示了子空间相似性 $\phi$ 的热图，分别针对权重更新矩阵 $\Delta W_q$ 和 $\Delta W_v$：</p>
<ul>
  <li><strong>第一和第二图</strong>（左侧）：
    <ul>
      <li>代表 $A_{r=8}$ 和 $A_{r=64}$ 的子空间相似性，聚焦于 $i, j \leq 8$ 的情况。</li>
      <li>左下角的部分放大，显示较低秩时的子空间重叠情况。</li>
    </ul>
  </li>
  <li><strong>第三和第四图</strong>（右侧）：
    <ul>
      <li>代表相似性较高的方向（子空间重叠显著）。</li>
    </ul>
  </li>
</ul>

<hr />

<h5 id="关键观察">关键观察</h5>

<ul>
  <li><strong>子空间重叠现象</strong>：
    <ul>
      <li>$A_{r=8}$ 和 $A_{r=64}$ 的奇异向量子空间在前几个奇异向量方向上高度重叠（相似性 $\phi &gt; 0.5$）。</li>
      <li>说明低秩矩阵 $A_{r=8}$ 的重要方向已经包含在 $A_{r=64}$ 的子空间中。</li>
    </ul>
  </li>
  <li><strong>噪声过滤作用</strong>：
    <ul>
      <li>更高秩的矩阵（如 $A_{r=64}$）包含更多奇异向量，但其中一些可能主要是噪声。</li>
      <li>低秩矩阵（如 $A_{r=8}$）通过限制秩，有效过滤掉了这些噪声向量，从而提高了更新的有效性。</li>
    </ul>
  </li>
  <li><strong>权重更新的重要方向</strong>：
    <ul>
      <li>对于 $\Delta W_v$ 和 $\Delta W_q$，前几个奇异向量的重叠性尤其显著，表明这些方向对任务适配最为重要。</li>
    </ul>
  </li>
</ul>

<hr />

<h5 id="结论与解释">结论与解释</h5>

<ol>
  <li><strong>子空间相似性说明了低秩的有效性</strong>：
    <ul>
      <li>尽管 $r=8$ 比 $r=64$ 的秩低得多，但它已经捕捉了任务适配所需的最重要方向。</li>
      <li>因此，低秩的 LoRA 参数（如 $r=8$ 或更低）仍能在任务中表现良好。</li>
    </ul>
  </li>
  <li><strong>解释低秩的表现</strong>：
    <ul>
      <li>低秩适配矩阵通过子空间共享，专注于最重要的方向，减少了对不必要方向（潜在噪声）的学习。</li>
    </ul>
  </li>
  <li><strong>为什么 $r=1$ 仍然有效</strong>：
    <ul>
      <li>当 $r=1$ 时，虽然只有一个奇异向量，但这个方向与高秩矩阵（如 $r=64$）中的重要方向高度重叠，适配效果不会显著下降。</li>
    </ul>
  </li>
</ol>

<h3 id="问题三w-与-w-之间的关系">问题三：∆W 与 W 之间的关系</h3>

<h4 id="1-研究问题"><strong>1. 研究问题</strong></h4>
<ul>
  <li><strong>核心问题</strong>：
    <ul>
      <li>$\Delta W$ 与 $W$ 的关系如何？
        <ul>
          <li>$\Delta W$ 是否与 $W$ 的重要奇异方向高度相关？</li>
          <li>$\Delta W$ 是否主要放大了 $W$ 已有的重要方向，还是引入了新方向？</li>
        </ul>
      </li>
      <li>这些问题的答案可以揭示 LoRA 如何利用预训练模型权重进行下游任务的适配。</li>
    </ul>
  </li>
</ul>

<hr />

<h4 id="2-分析方法"><strong>2. 分析方法</strong></h4>
<ol>
  <li><strong>将 $W$ 投影到 $\Delta W$ 的子空间</strong>：
    <ul>
      <li>使用 $\Delta W$ 的左右奇异向量矩阵（通过 SVD 得到的 $U$ 和 $V$）定义子空间。</li>
      <li>投影操作：计算 $U^T W V^T$，并与 Frobenius 范数 $|W|_F$ 比较，以量化 $W$ 的信息在 $\Delta W$ 子空间中的占比。</li>
    </ul>
  </li>
  <li><strong>比较不同方向的相关性</strong>：
    <ul>
      <li>替换 $U$ 和 $V$ 为：
        <ul>
          <li>$\Delta W$ 的奇异向量（适配子空间）。</li>
          <li>$W$ 的前 $r$ 个奇异向量（预训练模型的重要方向）。</li>
          <li>随机高斯矩阵的奇异向量（随机方向）。</li>
        </ul>
      </li>
      <li>比较投影结果的 Frobenius 范数，分析 $\Delta W$ 的相关性。</li>
    </ul>
  </li>
</ol>

<hr />
<p><img src="/assets/posts_assets/Pasted%20image%2020241121111027.png" alt="" /></p>
<h4 id="3-图-4-的热图分析"><strong>3. 图 4 的热图分析</strong></h4>
<ul>
  <li><strong>左图和中图（$\Delta W_q$ 和 $\Delta W_v$）</strong>：
    <ul>
      <li>显示了两个随机种子下，$\Delta W_q$ 和 $\Delta W_v$ 的列向量相似性。</li>
      <li>可以看到，相似性较高的方向集中在前几个奇异向量，说明适配的矩阵捕获了主要的变化方向。</li>
    </ul>
  </li>
  <li><strong>右图（随机高斯矩阵的相似性）</strong>：
    <ul>
      <li>显示了随机矩阵的奇异向量相似性，几乎没有任何结构（接近 0），表明 $\Delta W$ 的方向与随机噪声完全不同。</li>
    </ul>
  </li>
</ul>

<hr />

<p><img src="/assets/posts_assets/Pasted%20image%2020241121111241.png" alt="" /></p>
<h4 id="4-表格-7-的数据分析"><strong>4. 表格 7 的数据分析</strong></h4>
<p>表格展示了不同投影方向下的 Frobenius 范数对比：</p>

<ol>
  <li><strong>$|U^T W_q V^T|_F$</strong>：
    <ul>
      <li>这是将 $W_q$ 投影到不同子空间后的范数：
        <ul>
          <li>对比 $\Delta W_q$、$W_q$ 自身、随机矩阵的子空间。</li>
        </ul>
      </li>
      <li>$r=4$：
        <ul>
          <li>投影到 $\Delta W_q$ 的子空间后，范数为 $0.32$，远小于投影到 $W_q$ 的 $21.67$，表明 $\Delta W_q$ 并没有简单重复 $W_q$ 的奇异方向。</li>
          <li>投影到随机矩阵的范数为 $0.02$，进一步验证 $\Delta W_q$ 的方向并非随机。</li>
        </ul>
      </li>
      <li>$r=64$：
        <ul>
          <li>投影到 $\Delta W_q$ 的子空间后，范数为 $1.90$，仍然小于 $W_q$ 的 $37.71$。</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>$|\Delta W_q|_F$</strong>：
    <ul>
      <li>适配矩阵 $\Delta W_q$ 的范数为 $6.91$（$r=4$）和 $3.57$（$r=64$）。</li>
      <li>说明随着 $r$ 增加，适配矩阵的强度逐渐降低。</li>
    </ul>
  </li>
</ol>

<hr />

<h4 id="5-关键结论"><strong>5. 关键结论</strong></h4>
<ol>
  <li><strong>$\Delta W$ 不是简单重复 $W$</strong>：
    <ul>
      <li>投影结果表明，$\Delta W$ 的子空间与 $W$ 的重要方向部分重叠，但也包含了额外的新方向。</li>
      <li>适配矩阵的作用是放大 $W$ 中已有的部分方向，同时引入对任务有用的新方向。</li>
    </ul>
  </li>
  <li><strong>适配矩阵的放大因子较高</strong>：
    <ul>
      <li>表格中提到，对于 $r=4$，$\Delta W_q$ 的放大因子高达 $21.5 = 6.91 / 0.32$，表明 LoRA 更倾向于放大重要特征，而不是重复预训练模型已有的权重分布。</li>
    </ul>
  </li>
  <li><strong>低秩适配有效性</strong>：
    <ul>
      <li>尽管 $r$ 较低（如 $r=4$），$\Delta W$ 的低秩方向仍能捕获适配的核心特征，并显著增强下游任务的表现。</li>
    </ul>
  </li>
</ol>]]></content><author><name>Xingjie Gao</name><email>xingjie-gao@outlook.com</email></author><category term="nlp" /><category term="自然语言处理" /><category term="论文笔记" /><summary type="html"><![CDATA[LoRA LOW-RANK ADAPTATION 论文笔记]]></summary></entry><entry><title type="html">DPO论文学习</title><link href="http://localhost:4000/2024/11/18/DPO%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0.html" rel="alternate" type="text/html" title="DPO论文学习" /><published>2024-11-18T00:00:00+08:00</published><updated>2024-11-18T00:00:00+08:00</updated><id>http://localhost:4000/2024/11/18/DPO%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0</id><content type="html" xml:base="http://localhost:4000/2024/11/18/DPO%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0.html"><![CDATA[<h1 id="dpo-论文学习">DPO 论文学习</h1>

<!---more-->

<h2 id="不懂的问题">不懂的问题</h2>

<h3 id="偏好学习的单位是什么也是-token还是以-response">偏好学习的单位是什么？也是 token？还是以 response？</h3>

<p>和 SFT 一样是Response</p>

<h3 id="为什么人类偏好学习需要强化学习">为什么人类偏好学习需要强化学习</h3>

<p>对于一个 response 的偏好程度打分，不是模型的输出（模型输出的是对人类偏好的response（word tokens）），因为由于模型输出和打分不是一种类型的数据，所以我们无法将 LLM 的输出与标签对比算出 loss， 也就不能反向传播更新），所以就不能用常规的监督学习来训练模型</p>

<p><strong>DPO 的缺点是需要大量有标注单独信息，而 RLHF 只需要小部分的</strong></p>

<h3 id="什么是策略">什么是策略？</h3>

<p>在本文的环境中：策略就是模型生成概率空间，即生成的文本的概率分布。</p>

<p>策略训练（policy train）就是训练 LLM 的策略的过程，使其生成的文本更符合人类的偏好。</p>

<p>一般的策略训练的过程</p>

<ol>
  <li>
    <p><strong>确定目标</strong>：首先需要定义目标，这通常是通过一个<strong>损失函数</strong>来表示的。在传统的强化学习中，这个目标是最大化奖励。</p>
  </li>
  <li>
    <p><strong>优化过程</strong>：通过某种优化算法（如梯度下降、Proximal Policy Optimization（PPO）等），不断调整模型的参数，改变策略，使得策略能够在特定任务中表现得更好。例如，语言模型的策略可以通过优化语言生成的质量来训练。</p>
  </li>
  <li>
    <p><strong>策略调整</strong>：在训练过程中，模型会不断根据新的数据或环境反馈调整策略，以提高其执行任务的能力。</p>
  </li>
</ol>

<h3 id="什么是奖励函数">什么是奖励函数？</h3>

<p><strong>奖励函数</strong>：在强化学习和偏好学习中，奖励函数通常用于量化一个动作或响应的质量。在传统的 RLHF（从人类反馈中进行强化学习）方法中，奖励函数是由人类偏好数据训练出来的，它用来评估语言模型输出与人类偏好的一致性。</p>

<p>然而，在 DPO 方法中，作者没有显式地定义和训练一个独立的奖励函数，而是通过将偏好数据直接映射到策略（即语言模型）的优化中，从而隐含地使用奖励函数。</p>

<blockquote>
  <p>这里的奖励函数可以理解为偏好数据对某个输出的偏好程度，反映了人类更倾向于选择哪些输出</p>
</blockquote>

<h3 id="什么是-kl-散度">什么是 KL 散度？</h3>

<p><strong>KL 散度</strong>（<strong>Kullback-Leibler Divergence</strong>）是一种用于衡量两个概率分布之间差异的度量。它通常用于比较一个真实分布（或参考分布）和一个估计分布（或预测分布）之间的差异。</p>

<p>KL 散度的公式为：</p>

\[D_{KL}(P || Q) = \sum_{x} P (x) \log \frac{P (x)}{Q (x)}\]

<p>或在连续情况下：</p>

\[D_{KL}(P || Q) = \int p (x) \log \frac{p (x)}{q (x)} \, dx\]

<p>其中：</p>
<ul>
  <li>$P (x)$ 是真实的概率分布（或参考分布），表示“正确”或期望的分布。</li>
  <li>$Q (x)$ 是估计的概率分布，表示模型输出的分布。</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$D_{KL}(P</td>
          <td> </td>
          <td>Q)$ 衡量了分布 $P$ 相对于 $Q$ 的<strong>信息损失</strong>，或者说是从 $Q$ 生成 $P$ 时的效率损失。</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h4 id="kl-散度的含义">KL 散度的含义：</h4>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>非对称性</strong>：KL 散度是非对称的，即 $D_{KL}(P</td>
          <td> </td>
          <td>Q) \neq D_{KL}(Q</td>
          <td> </td>
          <td>P)$，这意味着它不是一个真正的“距离”，而是一个<strong>信息量</strong>度量。</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><strong>度量差异</strong>：KL 散度的值越大，说明两个分布之间的差异越大。KL 散度为零时，说明两个分布是完全相同的（即 $P = Q$）。</li>
  <li><strong>信息量</strong>：KL 散度表示使用分布 $Q$ 来表示分布 $P$ 时所失去的信息量。如果我们使用 $Q$ 来描述真实分布 $P$，那么我们需要多少额外的信息来弥补这个差距。</li>
</ul>

<h4 id="在机器学习中的应用">在机器学习中的应用：</h4>
<ol>
  <li>
    <p><strong>在强化学习中</strong>：KL 散度通常用于控制策略优化过程中的“平稳性”或“稳定性”。例如，在<strong>Proximal Policy Optimization (PPO)</strong> 算法中，KL 散度用作约束，防止优化过程中新的策略与旧的策略偏差过大，从而保证训练过程的稳定性。</p>
  </li>
  <li>
    <p><strong>在生成模型中</strong>：KL 散度常用于衡量生成模型（如变分自编码器 VAE）学习的分布与真实数据分布之间的差异。最小化 KL 散度有助于生成的样本更接近真实数据分布。</p>
  </li>
  <li>
    <p><strong>在模型优化中</strong>：KL 散度也常用于<strong>对抗训练</strong>等场景，作为衡量两个模型输出概率分布差异的一种方式。</p>
  </li>
</ol>

<h4 id="总结">总结：</h4>
<p>KL 散度是一个衡量概率分布之间差异的工具，它通过计算一个分布生成另一个分布时的“信息损失”来工作。在强化学习、生成模型以及其他机器学习领域，KL 散度是优化过程中的一个重要工具，尤其用于控制训练过程中的分布变化。</p>

<h3 id="dpo-的变量变换思想">DPO 的变量变换思想</h3>

<blockquote>
  <p>我理解这里变量变换思想就是：
 假设 A=B=C
 原本的过程就是使用 A—&gt;B—&gt;C
 而 DPO 的变量变换思想就是直接使用 A—&gt;C</p>
</blockquote>

<p>在<strong>DPO</strong>算法中，<strong>变量变换</strong>是一个数学技巧，它允许我们跳过显式构建奖励模型的过程，直接通过人类偏好数据优化策略。为了更好地理解这个概念，下面我将举一个简单的例子来解释“变量变换”的含义。</p>

<h4 id="举例假设我们有一个简单的分类问题">举例：假设我们有一个简单的分类问题</h4>

<p>假设我们正在训练一个模型来生成文本，并且我们收集了大量的人类偏好数据，这些数据表明在两个不同的模型输出之间，人类更喜欢哪一个。</p>

<ul>
  <li><strong>模型输出 A</strong>和<strong>模型输出 B</strong>是两个候选答案。</li>
  <li><strong>人类偏好数据</strong>告诉我们，<strong>A 更好</strong>，因此我们可以把人类偏好表示为“喜欢 A、不喜欢 B”。</li>
</ul>

<p>在传统的强化学习方法中，我们可能会这样做：</p>
<ol>
  <li><strong>构建奖励模型</strong>：首先需要定义一个奖励函数，比如给<strong>A</strong>一个高奖励，给<strong>B</strong>一个低奖励。</li>
  <li>然后使用强化学习（如 PPO）来优化策略，使得模型生成的答案更倾向于 A，因为 A 得到了更高的奖励。</li>
</ol>

<h4 id="变量变换的作用">变量变换的作用</h4>

<p>但是，<strong>DPO</strong>采用了<strong>变量变换</strong>的方式，避免了需要显式构建奖励模型和奖励值的问题，直接通过人类的偏好数据来优化模型。</p>

<p>假设我们已经知道，人类给出的是一个简单的<strong>偏好排序</strong>，也就是说，模型输出 A 被偏好于 B。DPO 的<strong>变量变换</strong>过程可以被理解为：</p>

<ol>
  <li>
    <p><strong>换一个视角</strong>：我们不再直接优化奖励值，而是通过将奖励值变换成策略本身来进行优化。具体来说，我们定义一个新的<strong>损失函数</strong>，这个损失函数表示我们希望生成更倾向于人类偏好的答案（即 A）的策略。</p>

    <ul>
      <li><strong>变量变换</strong>的步骤：通过数学变换（例如：<strong>对数变换</strong>），我们可以把偏好损失函数定义为一个关于策略的函数，而不需要显式地训练奖励模型。换句话说，我们直接用人类给出的“喜欢”或“不喜欢”作为优化目标，调整模型的生成策略，使得模型生成 A 的概率更高。</li>
    </ul>
  </li>
  <li>
    <p><strong>直接优化策略</strong>：通过这种变换，我们可以用<strong>二元交叉熵</strong>这样的简单目标函数来优化策略，即最大化模型生成 A 的概率，同时最小化生成 B 的概率。</p>
  </li>
</ol>

<h4 id="数学背景">数学背景：</h4>
<ul>
  <li>
    <p>如果我们不进行变量变换，奖励模型可能会让我们使用以下目标：
\(\text{maximize} \quad \mathbb{E}[R (\text{output})]\)
其中 $R (\text{output})$ 是奖励函数。</p>
  </li>
  <li>
    <p>但是通过<strong>变量变换</strong>，我们可以将这个目标变成：
\(\text{maximize} \quad \log \left ( \frac{P (\text{preferred response})}{P (\text{dispreferred response})} \right)\)
这实际上是优化模型策略的一个等价目标，而不需要定义一个显式的奖励函数。</p>
  </li>
</ul>

<h4 id="总结-1">总结：</h4>
<p>在<strong>DPO</strong>中，<strong>变量变换</strong>指的是将优化目标从奖励模型转变为直接优化策略的函数。通过这种变换，我们避免了构建奖励模型的复杂过程，直接通过人类偏好数据优化语言模型的策略。</p>

<h3 id="dpo-和-bradley-terry-的异同">DPO 和 Bradley-Terry 的异同</h3>

<blockquote>
  <p>Bradley-Terry 模型是一种用于衡量人类偏好数据的一致性的统计模型，它可以用来评估两个或多个选项之间的相对偏好。
它通过给每个物品分配一个质量值，预测不同物品之间的偏好概率</p>

</blockquote>

<p>DPO 和 Bradley-Terry 都是理论模型，而不是真实的训练出来模型。是可以通过数学公式计算出来的。</p>

<p>DPO 直接用人类偏好数据来优化模型，而 Bradley-Terry 模型是用来评估人类相对偏好的</p>

<h2 id="摘要">摘要</h2>

<h3 id="当前背景">当前背景</h3>

<ol>
  <li>LLM 的行为很难被精准控制，因为进行了无监督的预训练</li>
  <li>想要控制 LLM 的办法：
    <ul>
      <li>RLHF：通过收集人类标签来评估模型生成结果的相对质量，并对无监督LM进行微调以符合这些偏好，通常需要通过来自人类反馈的强化学习（RLHF）</li>
    </ul>
  </li>
</ol>

<h3 id="作者的工作">作者的工作</h3>

<ol>
  <li>DPO：我们利用<strong>奖励函数与最优策略之间的映射</strong>，表明这个受限的奖励最大化问题可以通过一次单独的策略训练精确优化，实际上解决了人类偏好数据上的分类问题（二分类：喜欢、不喜欢）。</li>
</ol>

<h3 id="实验结果">实验结果</h3>

<ol>
  <li>DPO可以将语言模型微调到与人类偏好相符，且效果与现有方法相当或更好。</li>
  <li>使用DPO进行微调超越了RLHF控制生成内容情感的能力，并在摘要和单轮对话中提高了响应质量，同时实现和训练起来大大更简单。</li>
</ol>

<h2 id="引言">引言</h2>

<h3 id="1-问题">1. 问题：</h3>
<p>从模型非常广泛的知识和能力中选择所期望的响应和行为对于构建安全、高性能和可控的人工智能系统至关重要</p>

<p>例子：</p>

<blockquote>
  <p>我们可能希望我们的语言模型意识到50%的人相信的一个常见误解，但我们肯定不希望模型声称这个误解在50%的查询中是真实的！</p>
</blockquote>

<h3 id="2-现在一般的解决方法">2. 现在一般的解决方法：</h3>

<ol>
  <li>有监督微调：直接给 LLM 高质量的数据</li>
  <li>通过 RLHF 进行微调（缺点：复杂、计算成本高）</li>
</ol>

<h3 id="3-作者的解决办法dpo">3. 作者的解决办法：DPO</h3>

<ol>
  <li><strong>DPO 算法的基本思路</strong>：
    <ul>
      <li><strong>目标</strong>：DPO 的目标与现有的 RLHF 方法类似，都是<strong>奖励最大化</strong>，并且包含了一个<strong>KL 散度约束</strong>，即优化过程中不仅要最大化奖励，还要确保生成的回答不会偏离原始模型过多。</li>
      <li><strong>细节</strong>：DPO 通过<strong>增加偏好响应的对数概率</strong>来优化模型，使得模型生成“更喜欢”的回答，同时引入了<strong>动态的权重</strong>，防止模型因过度偏向某些回答而发生退化。传统方法中的概率比值目标可能导致模型不稳定，而 DPO 通过引入这些权重来避免这种问题。</li>
    </ul>
  </li>
  <li><strong>与现有方法的区别</strong>：
    <ul>
      <li>DPO 使用一个<strong>理论偏好模型</strong>（如<strong>Bradley-Terry 模型</strong>），用来衡量给定的奖励函数与人类偏好数据的一致性。现有方法（RLHF）依赖偏好模型来定义一个“偏好损失”函数，训练奖励模型，再用这个奖励模型来优化语言模型（策略）。</li>
      <li>而 DPO 则通过<strong>变量变换</strong>，将这个“偏好损失”函数直接定义为语言模型策略的函数。也就是说，DPO 不需要显式地学习奖励函数，而是直接通过人类偏好的数据来优化模型。</li>
    </ul>
  </li>
  <li><strong>DPO 的训练过程</strong>：
    <ul>
      <li>给定一组人类对语言模型响应的偏好数据，DPO 通过一个<strong>简单的二元交叉熵目标</strong>（binary cross entropy）来优化模型，而不需要在训练过程中进行策略采样或显式学习奖励函数。</li>
    </ul>
  </li>
  <li><strong>实验结果</strong>：
    <ul>
      <li>实验表明，DPO 至少与现有方法（如基于 PPO 的 RLHF 方法）一样有效，在任务如情感调节、文本摘要和对话生成等方面，使用最多 6 B 参数的语言模型，DPO 的表现与现有方法相当，甚至在某些情况下更好。</li>
    </ul>
  </li>
</ol>

<p><strong>DPO 是一个简单且不依赖强化学习的算法，</strong> 它直接通过人类偏好数据优化语言模型，从而避免了复杂的奖励建模和强化学习过程。DPO 在多个任务中与传统方法相比同样有效，且实现简单，易于训练。</p>

<h2 id="相关工作">相关工作</h2>

<h3 id="llm-对齐的发展">LLM 对齐的发展</h3>

<ol>
  <li>无监督的 LLM 可以在 zero-shot 和 few-shot 的情况下完成一些任务，但是不如指令微调后的 LLM</li>
  <li>指令微调可以显著提高 LLM在下游任务上的表现和与用户意图的对齐，但是需要专家示范的数据</li>
  <li>收集对响应质量的相对人类判断往往更为容易，因此随后的研究使用人类偏好的数据集对大语言模型进行了微调，
 结果：提高了翻译、摘要、讲故事和遵循指令的能力。
 一般过程：
    <ol>
      <li>优化神经网络的奖励函数，以便与偏好模型下的偏好数据集兼容，如 Bradley-Terry 模型[5]，</li>
      <li>然后使用强化学习算法（通常是 REINFORCE 、近端策略优化（PPO；）或变体）微调语言模型，以最大化给定的奖励。</li>
    </ol>
  </li>
  <li>其他研究：
    <ol>
      <li>利用经过人类反馈微调的 LLMs 来生成针对安全性或无害性等特定属性的额外合成偏好数据，仅使用人类提供的文本评估作为 LLM 注释的弱监督</li>
    </ol>
  </li>
</ol>

<h3 id="总结-2">总结：</h3>

<p>总共有两个方向：</p>

<ol>
  <li>一个是关于使用强化学习训练语言模型以实现多种目标的研究工作</li>
  <li>另一个是关于从人类偏好中学习的一般方法的研究工作</li>
</ol>

<p>本文：</p>

<ol>
  <li>尽管使用相对人类偏好的方法具有吸引力，但使用强化学习对大型语言模型进行微调仍然是一个主要的实际挑战；</li>
  <li>本项工作提供了一种在没有强化学习的情况下优化相对偏好的理论上合理的方法。</li>
</ol>

<h3 id="偏好学习不在-nlp-中的">偏好学习（不在 NLP 中的）</h3>

<h4 id="情境对决赌博机-contextual-dueling-bandits">情境对决赌博机 (Contextual Dueling Bandits)</h4>

<p>首先，使用偏好或动作排名而非奖励的情境赌博机学习被称为情境对决赌博机（Contextual Dueling Bandit，CDB）。在没有绝对奖励的情况下，CDB 的理论分析用<strong>冯·诺伊曼胜者（von Neumann winner）</strong> 的概念取代了最优策略。</p>

<blockquote>
  <p>冯·诺伊曼胜者是指其对任何其他策略的预期胜率至少为 50%的策略。</p>

  <p>通常，在有明确奖励的环境中，我们可以定义一个最优策略，它能最大化预期奖励。</p>

  <p>然而，在CDB中，没有绝对的奖励信号，只有策略之间的偏好或胜率信息。因此，冯·诺伊曼胜者被定义为：对于任何其他策略，其预期胜率至少为50%的策略。也就是说，这个策略在与任何其他策略对比时，至少不会输，可能会赢。这一概念适用于无法直接量化奖励的情况下，用相对胜率来评估策略的优劣。</p>
</blockquote>

<p>然而，在 CDB 的设置中，偏好标签是在线获得的；而在从人类偏好中学习时，我们通常从一批离线的、带有偏好标注的动作对中学习。</p>

<p>类似地，<strong>基于偏好的强化学习（Preference-based RL，PbRL）</strong> 从由未知的“评分”函数生成的二元偏好中学习，而不是直接从奖励中学习。已有多种 PbRL 算法，包括可以重用离线偏好数据的方法，但通常需要先显式估计潜在的评分函数（即奖励模型），然后再对其进行优化。</p>

<p>与这些方法不同，作者提出了一种单阶段的策略学习方法，可以直接优化策略以满足偏好，而无需先估计奖励模型。</p>

<h2 id="预备知识">预备知识</h2>

<p>RLHF 的 pipeline</p>

<p>1) supervised fine-tuning (SFT); 
2) preference sampling and reward learning
3) RL optimization.</p>

<h3 id="sft">SFT</h3>

<p>RLHF 通常通过对预训练语言模型进行微调，以监督学习的方式在高质量数据上进行，针对感兴趣的下游任务（对话、摘要等），以获得模型 $\pi ^{SFT}$。</p>

<h3 id="奖励模型阶段">奖励模型阶段</h3>

<ol>
  <li><strong>奖励建模的基本任务</strong>：
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>在奖励建模阶段，模型（SFT 模型，指的是预先监督微调的模型 $\pi^{\text{SFT}}(y</td>
              <td>x)$）接收一个输入提示 $x$，并生成一组候选答案对 $(y_1, y_2)$。</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>这些候选答案对会交给人类标注者进行评估，标注者会基于偏好选择其中一个答案。例如，人类可能更喜欢答案 $y_w$ 而不是 $y_l$。</li>
      <li>$y_w$ 表示人类标注中“偏好”的答案，而 $y_l$ 表示“不偏好”的答案。</li>
    </ul>
  </li>
</ol>

<hr />

<ol>
  <li><strong>引入奖励模型</strong>：
    <ul>
      <li>假设人类偏好的分布是由一个<strong>潜在奖励模型</strong> $r^*(y, x)$ 生成的。</li>
      <li><strong>奖励模型 $r^*(y, x)$</strong>：它是一个无法直接获取的函数，用于衡量在特定输入 $x$ 下，答案 $y$ 的质量或偏好。</li>
    </ul>
  </li>
</ol>

<hr />

<ol>
  <li>如何使用奖励模型对偏好建模：
    <ul>
      <li>文中指出，Bradley-Terry 模型是用于偏好建模的常见方法之一。该模型假设：
\(P^*(y_1 \succ y_2 \mid x) = \frac{\exp (r^*(x, y_1))}{\exp (r^*(x, y_1)) + \exp (r^*(x, y_2))}.\)</li>
      <li>这个公式描述了答案 $y_1$ 相比 $y_2$ 被人类偏好的概率。概率值由各自的奖励值 $r^*(x, y)$ 决定，奖励值越高的答案更有可能被选择。</li>
    </ul>
  </li>
</ol>

<blockquote>
  <p>[!TIP] 
为什么公式是这样的：</p>
  <ol>
    <li>想要 $r(x,y)$ 打分越高越偏向于 $y$，所以使用了指数函数</li>
    <li>分母是为了归一化，使得概率和为 1，使其成为一个有效的概率分布</li>
    <li>当两数差异较大是，使用指数函数会扩大差异，使得概率更加明显</li>
    <li>也可换一种理解方式，这个公式可以从 sigmod 简化而来，</li>
    <li>而为什么使用 sigmod 呢？因为我们可以用模型推导出的偏好与不偏好的差值来判断是否偏好，如果是偏好的分数大于不偏好的分数，所以为正值，正值越大 sigmod 的函数值越接近 1，反之越接近 0，所以使用 sigmod 函数来表示偏好。</li>
  </ol>
</blockquote>

<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />由偏好公式推到这个损失函数  [priority:: highest]  [created:: 2024-11-18]  [scheduled:: 2024-11-18]  [completion:: 2024-11-19]</li>
</ul>

<hr />

<ol>
  <li>如何训练奖励模型 $D$：
    <ul>
      <li>数据集：</li>
    </ul>
    <ul>
      <li>作者假设可以访问一个静态的偏好数据集 $D = {(x^{(i)}, y_w^{(i)}, y_l^{(i)})}$，其中包含输入 $x^{(i)}$ 及其对应的偏好答案对。</li>
      <li>目标是通过这些数据学习一个<strong>参数化的奖励模型</strong> $r_\phi (x, y)$，该模型的参数 $\phi$ 可以通过最大似然估计（MLE）优化。
     - <strong>将问题建模为二分类任务</strong>：</li>
      <li>偏好学习的任务被建模为一个<strong>二元分类问题</strong>，目的是预测在一对答案中哪一个更符合人类的偏好。</li>
      <li>负对数似然损失（Negative Log-Likelihood Loss）定义如下：
\(\mathcal{L}_R (r_\phi, D) = - \mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma (r_\phi (x, y_w) - r_\phi (x, y_l)) \right].\)</li>
      <li>$\sigma$ 是 sigmod 函数，用来二分类，表示 $\sigma (z) = \frac{1}{1 + \exp (-z)}$。</li>
    </ul>
  </li>
</ol>

<blockquote>
  <p>[!NOTE] 损失函数推导过程
偏好的定义：
\(P^*(y_1 \succ y_2 \mid x) = \frac{\exp (r^*(x, y_1))}{\exp (r^*(x, y_1)) + \exp (r^*(x, y_2))}\)
然后根据最大似然的思想，我们要最大化这个概率，为了最大化这个概率方便我们对他取对数
\(\log P^*(y_1 \succ y_2 \mid x) = \log \left ( \frac{\exp (r^*(x, y_1))}{\exp (r^*(x, y_1)) + \exp (r^*(x, y_2))} \right)\)
 然后因此，我们将模型的评分函数 $r_\phi (x, y_1)$ 和 $r_\phi (x, y_2)$ 代入上式，得到：
 \(\log P^*(y_w \succ y_l \mid x) = \log \left ( \frac{1}{1 + \exp (r^*(x, y_l) - r^*(x, y_w))} \right)\)
 代入 sigmod 函数简化
 \(\sigma (z) = \frac{1}{1 + \exp (-z)}\)
 得：
 \(\log P^*(y_w \succ y_l \mid x) = \log \sigma (r_\phi (x, y_w) - r_\phi (x, y_l))\)
 我们要求的是在样本上的期望，所以损失函数就是这个
 \(\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma (r_\phi (x, y_w) - r_\phi (x, y_l)) \right]\)
 因为 loss 是最小化，所以加个负号
 \(\mathcal{L}_{R}(r_{\phi},\mathcal{D})=-\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\big[\log\sigma(r_{\phi}(x,y_{w})-r_{\phi}(x,y_{l}))\big]\)</p>
</blockquote>

<ol>
  <li><strong>奖励模型的实现</strong>：
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>在语言模型（LM）的上下文中，奖励模型 $r_\phi (x, y)$ 是在现有的 SFT 模型 $\pi^{\text{SFT}}(y</td>
              <td>x)$ 基础上构建的：</td>
            </tr>
          </tbody>
        </table>
        <ul>
          <li>在现有的 SFT 模型顶部添加一个线性层，以生成一个单一的标量预测值作为奖励值。</li>
        </ul>
      </li>
      <li>为了防止奖励模型的方差过高，通常会对奖励值进行归一化（如设置所有样本的奖励值期望为 0）。</li>
    </ul>
  </li>
</ol>

<h3 id="强化学习阶段">强化学习阶段</h3>

\[\max_{\pi_\theta}\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_\theta(y\mid x)}\begin{bmatrix}r_\phi(x,y)\end{bmatrix}-\beta\mathbb{D}_{\mathbf{KL}}\begin{bmatrix}\pi_\theta(y\mid x)\parallel\pi_{\mathbf{ref}}(y\mid x)\end{bmatrix}\]

<p>最大化奖励函数，同时最小化策略分布与参考分布的 KL 散度。</p>
<ul>
  <li><strong>第一项 $\mathbb{E}[r_\phi (x, y)]$：</strong>
    <ul>
      <li>这是奖励函数 $r_\phi (x, y)$ 的期望，表示模型生成输出 $y$ 的质量。</li>
      <li>模型的目标是生成能获得高奖励的输出。</li>
    </ul>
  </li>
  <li><strong>第二项 $\beta D_{\text{KL}}[\pi_\theta | \pi_{\text{ref}}]$：</strong>
    <ul>
      <li>这是一个 KL 散度（Kullback-Leibler divergence）项，用来衡量当前策略 $\pi_\theta$ 偏离参考策略 $\pi_{\text{ref}}$ 的程度。</li>
      <li>$\beta$ 是一个超参数，用来控制当前策略 $\pi_\theta$ 和参考策略 $\pi_{\text{ref}}$ 偏离的强度。</li>
      <li>添加这个约束的目的是防止模型在追求高奖励时生成过度偏离参考策略的内容（例如避免模式崩溃，即模型只输出单一高奖励答案）。</li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$\pi_{\text{ref}}(y</td>
          <td>x)$ 是参考策略，一般初始化为已经通过监督微调（Supervised Fine-Tuning, SFT）得到的模型 $\pi^{\text{SFT}}$。</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><strong>重要性</strong>：使用参考策略是为了确保生成的内容在奖励函数精度高的分布范围内，同时保持生成的多样性，防止模式崩塌（mode collapse）。</li>
</ul>

<p>但是，由于语言生成是离散的（输出是离散的单词序列），这个优化目标函数不可直接微分，无法用常规的梯度下降法进行优化。</p>

<p>标准的方法：
  \(R (x, y) = r_\phi (x, y) - \beta \left ( \log \pi_\theta (y|x) - \log \pi_{\text{ref}}(y|x) \right)\)
然后说使用 PPO 最大化。</p>

<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />复习一下梯度下降，思考一下这个为什么不可微就不能用梯度下降  [created:: 2024-11-19]  [due:: 2024-11-19]  [completion:: 2024-11-20]</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />学习 PPO 的论文 [created:: 2024-11-19]</li>
</ul>

<h2 id="dpo-算法">DPO 算法</h2>

<h3 id="dpo-目标函数的推导">DPO 目标函数的推导</h3>

<p><strong>思路：从原本的 RLHF 的 loss 函数推导到 DPO 的 loss 函数</strong></p>

<ol>
  <li><strong>优化目标</strong>
我们的目标是找到策略 $\pi_\theta (y|x)$，使以下目标函数最大化：
\(\mathcal{L}(\pi_\theta) = \int \pi_\theta (y|x) r (x, y) \, dy - \beta \int \pi_\theta (y|x) \log \frac{\pi_\theta (y|x)}{\pi_{\text{ref}}(y|x)} \, dy.\)
其中：
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>第一项 $\int \pi_\theta (y</td>
              <td>x) r (x, y) dy$ 表示奖励函数 $r (x, y)$ 在策略 $\pi_\theta (y</td>
              <td>x)$ 下的期望。</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>第二项 $\beta \int \pi_\theta (y</td>
              <td>x) \log \frac{\pi_\theta (y</td>
              <td>x)}{\pi_{\text{ref}}(y</td>
              <td>x)} \, dy$ 是 KL 散度，用来约束策略 $\pi_\theta (y</td>
              <td>x)$ 不偏离参考策略 $\pi_{\text{ref}}(y</td>
              <td>x)$。</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>$\beta &gt; 0$ 是权衡奖励和约束的超参数。</li>
    </ul>
  </li>
</ol>

<table>
  <tbody>
    <tr>
      <td>此外，策略 $\pi_\theta (y</td>
      <td>x)$ 需要满足概率分布的归一化条件：</td>
    </tr>
    <tr>
      <td>$$\int \pi_\theta (y</td>
      <td>x) \, dy = 1.$$</td>
    </tr>
  </tbody>
</table>

<hr />

<ol>
  <li><strong>引入拉格朗日乘子</strong>
为了在优化目标中引入概率分布的归一化约束，我们定义拉格朗日函数，加入一个归一化约束项 $\lambda (x)$：
\(\mathcal{L}(\pi_\theta, \lambda) = \int \pi_\theta (y|x) r (x, y) \, dy - \beta \int \pi_\theta (y|x) \log \frac{\pi_\theta (y|x)}{\pi_{\text{ref}}(y|x)} \, dy + \lambda (x) \left ( 1 - \int \pi_\theta (y|x) \, dy \right ).\)</li>
</ol>

<p>这里：</p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$\lambda (x)$ 是与输入 $x$ 相关的拉格朗日乘子，用来确保 $\pi_\theta (y</td>
          <td>x)$ 满足归一化条件。</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<hr />

<ol>
  <li>
    <table>
      <tbody>
        <tr>
          <td>**对 $\pi_\theta (y</td>
          <td>x)$ 求偏导**</td>
        </tr>
        <tr>
          <td>对拉格朗日函数 $\mathcal{L}(\pi_\theta, \lambda)$ 关于 $\pi_\theta (y</td>
          <td>x)$ 求导，得到优化条件。具体计算如下：</td>
        </tr>
      </tbody>
    </table>
  </li>
</ol>

<p>(1) 奖励项的偏导
奖励项是：
\(\int \pi_\theta (y|x) r (x, y) \, dy.\)
对 $\pi_\theta (y|x)$ 求偏导，得到：
\(\frac{\partial}{\partial \pi_\theta (y|x)} \left[ \pi_\theta (y|x) r (x, y) \right] = r (x, y).\)</p>

<p>(2) KL 散度项的偏导
KL 散度项是：
\(\int \pi_\theta (y|x) \log \frac{\pi_\theta (y|x)}{\pi_{\text{ref}}(y|x)} \, dy.\)
对 $\pi_\theta (y|x)$ 求偏导：</p>
<ul>
  <li>首先展开 KL 散度：
\(\pi_\theta (y|x) \log \frac{\pi_\theta (y|x)}{\pi_{\text{ref}}(y|x)} = \pi_\theta (y|x) \log \pi_\theta (y|x) - \pi_\theta (y|x) \log \pi_{\text{ref}}(y|x).\)</li>
  <li>求导时：
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>对 $\pi_\theta (y</td>
              <td>x) \log \pi_\theta (y</td>
              <td>x)$ 求导，得到：</td>
            </tr>
            <tr>
              <td>$$\log \pi_\theta (y</td>
              <td>x) + 1.$$</td>
              <td> </td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>对 $\pi_\theta (y</td>
              <td>x) \log \pi_{\text{ref}}(y</td>
              <td>x)$ 求导，得到：</td>
            </tr>
            <tr>
              <td>$$\log \pi_{\text{ref}}(y</td>
              <td>x).$$</td>
              <td> </td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
</ul>

<p>因此，KL 散度项的偏导为：
\(\frac{\partial}{\partial \pi_\theta (y|x)} \left[ -\beta \int \pi_\theta (y|x) \log \frac{\pi_\theta (y|x)}{\pi_{\text{ref}}(y|x)} \, dy \right] = -\beta \left[ \log \pi_\theta (y|x) - \log \pi_{\text{ref}}(y|x) + 1 \right].\)</p>

<p>(3) 归一化约束项的偏导
归一化约束项是：
\(\lambda (x) \left ( 1 - \int \pi_\theta (y|x) \, dy \right).\)
对 $\pi_\theta (y|x)$ 求导，得到：
\(\frac{\partial}{\partial \pi_\theta (y|x)} \left[ \lambda (x) \left ( 1 - \int \pi_\theta (y|x) \, dy \right) \right] = -\lambda (x).\)</p>

<hr />

<ol>
  <li><strong>合并偏导并设为 0</strong>
将三部分的结果合并，得到：
\(\frac{\partial \mathcal{L}}{\partial \pi_\theta (y|x)} = r (x, y) - \beta \left[ \log \pi_\theta (y|x) - \log \pi_{\text{ref}}(y|x) + 1 \right] - \lambda (x).\)
设偏导为 0：
\(R (x, y) - \beta \left[ \log \pi_\theta (y|x) - \log \pi_{\text{ref}}(y|x) + 1 \right] - \lambda (x) = 0.\)</li>
</ol>

<p>整理得到：
\(\beta \log \pi_\theta (y|x) = \beta \log \pi_{\text{ref}}(y|x) + r (x, y) - \beta - \lambda (x).\)</p>

<hr />

<ol>
  <li><strong>解出 $\pi_\theta (y|x)$</strong>
两边除以 $\beta$，得到：
\(\log \pi_\theta (y|x) = \log \pi_{\text{ref}}(y|x) + \frac{1}{\beta} r (x, y) - \frac{1}{\beta} (\beta + \lambda (x)).\)
对两边取指数，得到：
\(\pi_\theta (y|x) = \pi_{\text{ref}}(y|x) \cdot \exp\left (\frac{1}{\beta} r (x, y)\right) \cdot \exp\left (-\frac{1}{\beta} (\beta + \lambda (x))\right).\)</li>
</ol>

<p>定义 $Z (x) = \exp\left (\frac{\lambda (x) + \beta}{\beta}\right)$ 为归一化常数，最终得到：
\(\pi_\theta (y|x) = \frac{1}{Z (x)} \pi_{\text{ref}}(y|x) \cdot \exp\left (\frac{1}{\beta} r (x, y)\right).\)</p>

<hr />

<ol>
  <li>
    <p><strong>分区函数 $Z (x)$ 的确定</strong>
为了保证 $\pi_\theta (y|x)$ 是一个合法的概率分布，满足：
\(\int \pi_\theta (y|x) \, dy = 1.\)
将 $\pi_\theta (y|x)$ 的表达式代入：
\(\int \frac{1}{Z (x)} \pi_{\text{ref}}(y|x) \cdot \exp\left (\frac{1}{\beta} r (x, y)\right) \, dy = 1.\)
因此：
\(Z (x) = \int \pi_{\text{ref}}(y|x) \cdot \exp\left (\frac{1}{\beta} r (x, y)\right) \, dy.\)
—</p>
  </li>
  <li>
    <p>解出 $r(x,y)$
由上述 $\pi_\theta (y|x)$ 的表达式，可以解出 $r(x,y)$：</p>
  </li>
</ol>

\[r(x,y)=\beta\log\frac{\pi_r(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}+\beta\log Z(x).\]

<hr />

<ol>
  <li>代入偏好模型</li>
</ol>

\[p^*(y_1\succ y_2\mid x)=\frac1{1+\exp\left(\beta\log\frac{\pi^*(y_2|x)}{\pi_{\mathrm{ref}}(y_2|x)}-\beta\log\frac{\pi^*(y_1|x)}{\pi_{\mathrm{ref}}(y_1|x)}\right)}\text{(6)}\]

<hr />

<ol>
  <li>求得 Loss
根据最大似然估计，我们可以得到损失函数：</li>
</ol>

\[\mathcal{L}_{\mathrm{DPO}}(\pi_\theta;\pi_{\mathrm{ref}})=-\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}\left[\log\sigma\left(\beta\log\frac{\pi_\theta(y_w\mid x)}{\pi_{\mathrm{ref}}(y_w\mid x)}-\beta\log\frac{\pi_\theta(y_l\mid x)}{\pi_{\mathrm{ref}}(y_l\mid x)}\right)\right].\]

<p>DPO相当于拟合一个重新参数化的 Bradley-Terry 模型</p>

\[r(y)=\beta\log\frac{\pi_\theta(y|x)}{\pi_\mathrm{ref}(y|x)},\]

<table>
  <tbody>
    <tr>
      <td>作者不直接定义一个显式的 r(y)，而是通过语言模型策略 $\pi_\theta(y</td>
      <td>x)$ 和参考策略 $\pi_\mathrm{ref}(y</td>
      <td>x)$ 的比率间接定义。</td>
    </tr>
  </tbody>
</table>

<h3 id="dpo-梯度更新过程">DPO 梯度更新过程</h3>

<p>我们从 DPO 损失函数的定义出发，通过逐步推导梯度公式，来说明公式的来源。以下是具体推导过程。</p>

<hr />

<ol>
  <li><strong>DPO 的损失函数</strong>
DPO 的目标是最小化以下损失函数：
\(\mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[\log \sigma \left (\beta \log \frac{\pi_\theta (y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta (y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right],\)</li>
</ol>

<hr />

<ol>
  <li><strong>对损失函数的梯度求导</strong>
我们希望计算损失函数对策略参数 $\theta$ 的梯度：
\(\nabla_\theta \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}).\)
2.1 <strong>将梯度作用到期望外</strong>
损失函数是一个期望形式，因此可以将梯度直接作用到期望内部：
\(\nabla_\theta \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \nabla_\theta \log \sigma \left (\beta \log \frac{\pi_\theta (y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta (y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right].\)</li>
</ol>

<p>2.2 <strong>对 sigmoid 内部项求导</strong>
对 $\log \sigma (z)$ 的梯度为：
\(\nabla_\theta \log \sigma (z) = \sigma (-z) \cdot \nabla_\theta z.\)
其中 $\sigma (-z) = 1 - \sigma (z)$。这一步的作用是将梯度转移到 $z$ 上。</p>

<p>令：
\(Z = \beta \log \frac{\pi_\theta (y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta (y_l|x)}{\pi_{\text{ref}}(y_l|x)}.\)</p>

<p>则：
\(\nabla_\theta \log \sigma (z) = \sigma (-z) \cdot \nabla_\theta z.\)</p>

<p>2.3 <strong>计算 $z$ 对 $\theta$ 的梯度</strong>
将 $z$ 展开为：
\(Z = \beta \log \frac{\pi_\theta (y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta (y_l|x)}{\pi_{\text{ref}}(y_l|x)}.\)</p>

<p>计算 $z$ 对 $\theta$ 的梯度：
\(\nabla_\theta z = \beta \left ( \nabla_\theta \log \pi_\theta (y_w|x) - \nabla_\theta \log \pi_\theta (y_l|x) \right).\)</p>

<p>将 $\nabla_\theta z$ 和 $\sigma (-z)$ 带入梯度公式：
\(\nabla_\theta \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \sigma (-z) \cdot \beta \left ( \nabla_\theta \log \pi_\theta (y_w|x) - \nabla_\theta \log \pi_\theta (y_l|x) \right) \right].\)</p>

<p>2.4 <strong>简化权重项</strong>
注意到 $\sigma (-z) = 1 - \sigma (z)$。对于 DPO 的优化，权重用 $\sigma (z)$ 即可，因此最终梯度为：
\(\nabla_\theta \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\beta \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \sigma \left ( \beta \log \frac{\pi_\theta (y_l|x)}{\pi_{\text{ref}}(y_l|x)} - \beta \log \frac{\pi_\theta (y_w|x)}{\pi_{\text{ref}}(y_w|x)} \right) \cdot \left ( \nabla_\theta \log \pi_\theta (y_w|x) - \nabla_\theta \log \pi_\theta (y_l|x) \right) \right].\)</p>

<hr />

<ol>
  <li><strong>最终公式</strong>
令隐式奖励函数为：
\(\hat{r}_\theta (x, y) = \beta \log \frac{\pi_\theta (y|x)}{\pi_{\text{ref}}(y|x)}.\)
则公式进一步简化为：
\(\nabla_\theta \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\beta \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \sigma \left ( \hat{r}_\theta (x, y_l) - \hat{r}_\theta (x, y_w) \right) \cdot \left ( \nabla_\theta \log \pi_\theta (y_w|x) - \nabla_\theta \log \pi_\theta (y_l|x) \right) \right].\)</li>
</ol>

<h4 id="梯度更新的解释">梯度更新的解释</h4>

<ol>
  <li><strong>调整偏好答案和非偏好答案的概率</strong>：</li>
</ol>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>梯度的第一项 $\nabla_\theta \log \pi_\theta(y_w</td>
          <td>x)$ 增加偏好答案 $y_w$ 的生成概率。</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>第二项 $\nabla_\theta \log \pi_\theta(y_l</td>
          <td>x)$ 减少非偏好答案 $y_l$ 的生成概率。</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>更新的大小由 sigmoid 加权，重点修正模型排序错误较大的样本。</li>
</ul>

<ol>
  <li><strong>权重反映偏好评分的偏差</strong>：</li>
</ol>

<ul>
  <li>
    <p>权重 $\sigma(\hat{r}<em>\theta(x, y_l) - \hat{r}</em>\theta(x, y_w))$ 直接取决于偏好排序的正确性。</p>
  </li>
  <li>
    <p>如果模型对偏好和非偏好的排序与人类一致，则权重较小；否则，权重较大，修正力度更强。</p>
  </li>
</ul>

<ol>
  <li><strong>综合目标</strong>：</li>
</ol>

<ul>
  <li>DPO 的更新机制本质上是在策略 $\pi_\theta$ 和人类偏好之间不断调整，使 $\pi_\theta$ 生成的结果逐渐接近人类期望。</li>
</ul>

<h4 id="dpo-的流程">DPO 的流程</h4>

<p>这段话描述了 DPO 方法的具体工作流程，包括如何构建数据集、初始化模型以及优化策略的过程。以下是详细解析：</p>

<hr />

<ol>
  <li><strong>DPO 的工作流程</strong>
(1) <strong>数据集构建</strong>
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td><strong>样本生成</strong>：首先，对每个提示 $x$（例如一个问题或任务）生成两个候选答案 $y_1$ 和 $y_2$，它们是从参考策略 $\pi_{\text{ref}}(\cdot</td>
              <td>x)$ 中采样得到的。</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li><strong>人类偏好标注</strong>：然后，通过人类标注的方式，根据偏好选择 $y_1$ 和 $y_2$ 中的优先答案（例如，哪一个更符合人类预期）。标注后的数据集可以表示为：
\(D = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}^N,\)
其中 $y_w$ 是偏好答案（”winner”），$y_l$ 是不偏好答案（”loser”）。</li>
    </ul>
  </li>
</ol>

<p>(2) <strong>优化语言模型</strong></p>
<ul>
  <li>在构造好的偏好数据集 $D$ 和给定的参考策略 $\pi_{\text{ref}}$ 下，优化目标是最小化 DPO 损失函数：
\(\mathcal{L}_{\text{DPO}} = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[\log \sigma\left (\beta \log \frac{\pi_\theta (y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta (y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)\right].\)</li>
  <li>通过优化 $\pi_\theta$，使得模型逐渐调整为生成更符合人类偏好的答案。</li>
</ul>

<hr />

<ol>
  <li><strong>实际应用中的简化</strong>
    <ul>
      <li>在实际操作中，可以直接复用现有的公开偏好数据集，而无需重复生成候选答案并进行人类标注。这种做法大大减少了数据构建的成本。</li>
    </ul>
  </li>
</ol>

<hr />

<ol>
  <li><strong>参考策略 $\pi_{\text{ref}}$ 的初始化</strong></li>
</ol>

<p>(1) 如果 $\pi^{\text{SFT}}$ 可用：</p>
<ul>
  <li>通常，参考策略 $\pi_{\text{ref}}$ 是从监督微调模型 $\pi^{\text{SFT}}$ 初始化的。
    <ul>
      <li><strong>$\pi^{\text{SFT}}$：</strong> 通过监督学习方法训练的模型，它是一个能够生成合理答案的基础模型。</li>
      <li>当 $\pi^{\text{SFT}}$ 可用时，直接设置 $\pi_{\text{ref}} = \pi^{\text{SFT}}$。</li>
    </ul>
  </li>
</ul>

<p>(2) 如果 $\pi^{\text{SFT}}$ 不可用：</p>

<ul>
  <li>如果没有现成的监督微调模型 $\pi^{\text{SFT}}$，可以通过最大化偏好答案的似然初始化参考策略：
\(\pi_{\text{ref}} = \arg\max_{\pi} \mathbb{E}_{(x, y_w) \sim D} [\log \pi (y_w|x)].\)</li>
  <li>这意味着 $\pi_{\text{ref}}$ 的训练目标是提升偏好答案 $y_w$ 的生成概率，从而构造一个合理的参考策略。</li>
  <li>通过 $\pi^{\text{SFT}}$ 或最大化偏好答案似然的方法，可以缓解模型分布与真实参考分布（即人类偏好生成的答案分布不可用）之间的偏移问题。</li>
</ul>

<h2 id="dpo-理论分析">DPO 理论分析</h2>

<h3 id="证明-1你的模型包含奖励模型">证明 1：你的模型包含奖励模型</h3>

<p>证明，DPO 的训出来的模型</p>

<p>\(r^*(x,y)=\beta\log\frac{\pi_\theta^*(y|x)}{\pi_{\mathrm{ref}}(y|x)}\)
等价于一般的奖励模型（Bradley-Terry 模型）</p>

<p>下面为了证明证明 1 ，给出了一些定义和引理。</p>

<h4 id="定义-1偏好奖励模型等价">定义 1：偏好奖励模型等价</h4>

<p>两个偏好奖励模型等价当且仅当
\(r(x,y)-r^{\prime}(x,y)=f(x)\)</p>

<p>证明：</p>

<p>在定义中，$f (x)$ 是<strong>与 $x$ 有关</strong>的函数，而不是一个常数。这是等价性定义中的关键点。</p>

<p>定义中提到，两个奖励函数 $r (x, y)$ 和 $r’ (x, y)$ 被认为是等价的，如果它们的差异可以表示为：
\(R (x, y) - r' (x, y) = f (x),\)
其中 $f (x)$ 是仅与输入 $x$ 相关的函数，和候选答案 $y$ 无关。</p>

<h4 id="为什么-f-x-与-x-相关">为什么 $f (x)$ 与 $x$ 相关？</h4>
<ul>
  <li>$f (x)$ 可能表示不同输入 $x$ 的整体偏移量。这个偏移量对候选项 $y$ 是一致的，但会因 $x$ 而变化。</li>
  <li>$f (x)$ 的存在表明，不同输入 $x$ 下，奖励函数的绝对值可以有不同的基线或偏移，但这些偏移不影响候选项 $y$ 之间的<strong>相对评分</strong>。</li>
</ul>

<hr />

<h4 id="等价性定义">等价性定义</h4>
<p>两个奖励函数 $r (x, y)$ 和 $r’ (x, y)$ 被认为是等价的，若它们的差异为：
\(R (x, y) - r' (x, y) = f (x),\)</p>
<ul>
  <li>这里 $f (x)$ 是<strong>仅与输入 $x$ 有关</strong>的函数，而不是一个常数。</li>
  <li>换句话说，$r (x, y)$ 和 $r’ (x, y)$ 对任意给定的输入 $x$，它们的评分差异在候选项 $y$ 上是恒定的。</li>
</ul>

<h4 id="为什么允许-f-x-的存在">为什么允许 $f (x)$ 的存在？</h4>
<p>在偏好分布（如 Bradley-Terry 或 Plackett-Luce 模型）中，偏好仅取决于奖励函数的相对差异，而不是奖励函数的绝对值。因此：</p>
<ul>
  <li>$f (x)$ 的存在相当于为输入 $x$ 下的所有候选项 $y$ 添加了一个全局偏移。</li>
  <li>这个偏移不影响候选项之间的相对评分，因此不会改变偏好分布或最优策略。</li>
</ul>

<hr />

<h4 id="具体解释f-x-的作用">具体解释：$f (x)$ 的作用</h4>

<ol>
  <li><strong>举例</strong>
假设对于输入 $x$，两个奖励函数是：
\(R (x, y) = \text{quality}(y) + 10 x, \quad r' (x, y) = \text{quality}(y),\)
其中 $\text{quality}(y)$ 表示候选项 $y$ 的质量，而 $10 x$ 是一个仅与 $x$ 相关的偏移。
    <ul>
      <li>这里 $r (x, y) - r’ (x, y) = 10 x = f (x)$。</li>
      <li>虽然 $f (x) = 10 x$ 会影响奖励函数的绝对值，但它对 $y$ 的相对排序没有影响，因此 $r (x, y)$ 和 $r’ (x, y)$ 是等价的。</li>
    </ul>
  </li>
  <li><strong>对偏好分布的影响</strong>
在 Bradley-Terry 模型中，偏好分布是由相对评分决定的，例如：
\(P (y_1 \succ y_2 \mid x) = \frac{\exp (r (x, y_1))}{\exp (r (x, y_1)) + \exp (r (x, y_2))}.\)
如果 $r (x, y)$ 和 $r’ (x, y)$ 相差 $f (x)$，则：
\(P (y_1 \succ y_2 \mid x) = \frac{\exp (r' (x, y_1) + f (x))}{\exp (r' (x, y_1) + f (x)) + \exp (r' (x, y_2) + f (x))}.\)
因为 $f (x)$ 是对所有候选项的全局偏移，它在分布计算中被约掉，不影响最终的概率值。</li>
</ol>

<h4 id="-结论-等价类和最优策略">### 结论 ：等价类和最优策略</h4>

<h4 id="引理-2">引理 2</h4>

<p><strong>同一等价类中的奖励函数会导致相同的最优策略</strong>。</p>

<h4 id="为什么"><strong>为什么？</strong></h4>

<ul>
  <li>在强化学习（RL）中，最优策略由奖励函数的相对差异决定，而非绝对值。</li>
  <li>由于同一等价类中的奖励函数在候选答案之间的相对排序相同，因此它们会诱导相同的最优策略。</li>
</ul>

<h4 id="结论与-plackett-luce-或-bradley-terry-模型一致的奖励函数等价类都可以通过重新参数化的形式来表示">结论：与 Plackett-Luce 或 Bradley-Terry 模型一致的奖励函数等价类，都可以通过重新参数化的形式来表示。</h4>

<p>证明略</p>

<h3 id="传统强化学习方法的不稳定性">传统强化学习方法的不稳定性</h3>

<p>这段内容从理论和实验两个角度解释了 <strong>DPO（Direct Preference Optimization）</strong> 方法的稳定性优势，并将其与基于强化学习（如 <strong>PPO</strong>）的方法进行了对比，指出了 DPO 的理论优势以及在实际应用中的鲁棒性表现。</p>

<p>以下是逐步解析：</p>

<h4 id="1-强化学习中的优化目标">(1) 强化学习中的优化目标</h4>
<p>文中提到的优化目标是：
\(\max_{\pi_\theta} \mathbb{E}_{\pi_\theta} \left[ r_\phi (x, y) - \beta \log \sum_y \pi_{\text{ref}}(y|x) \exp \left ( \frac{1}{\beta} r_\phi (x, y) \right) - \beta \log \frac{\pi_\theta (y|x)}{\pi_{\text{ref}}(y|x)} \right].\)</p>

<ul>
  <li><strong>主要内容</strong>：
    <ul>
      <li>$r_\phi (x, y)$：由奖励函数定义的分数。</li>
      <li>第二项是一个<strong>归一化项</strong>，对应分区函数的对数值。</li>
      <li>最后一项是策略 $\pi_\theta$ 和参考策略 $\pi_{\text{ref}}$ 之间的 KL 散度，用于限制策略偏离参考策略的程度。</li>
    </ul>
  </li>
</ul>

<h4 id="2-强化学习的不稳定性来源">(2) 强化学习的不稳定性来源</h4>
<ul>
  <li>传统强化学习（例如 PPO）使用<strong>Actor-Critic 框架</strong>优化类似的目标，然而：
    <ul>
      <li><strong>归一化项（normalization term）</strong>：
        <ul>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>分区函数的计算（即 $\sum_y \pi_{\text{ref}}(y</td>
                  <td>x) \exp \left ( \frac{1}{\beta} r_\phi (x, y) \right)$）是高维的积分，可能导致优化过程的不稳定。</td>
                </tr>
              </tbody>
            </table>
          </li>
          <li>如果不精确估计归一化项，梯度的方差会增大，优化变得困难。</li>
        </ul>
      </li>
      <li><strong>高方差的梯度</strong>：
        <ul>
          <li>在传统 RL 中，归一化项通常通过 Monte Carlo 采样估计，这可能导致梯度估计的高方差，进而影响收敛。</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="3-dpo-的解决方案">(3) DPO 的解决方案</h4>
<p>DPO 的重新参数化解决了上述问题：</p>
<ul>
  <li><strong>不需要显式估计归一化项</strong>：
    <ul>
      <li>DPO 的重新参数化将归一化项隐含地包含在优化过程中，不需要像 PPO 那样使用 Monte Carlo 或复杂的基线函数来估计分区函数。</li>
    </ul>
  </li>
  <li><strong>更稳定的梯度更新</strong>：
    <ul>
      <li>通过直接优化策略（而非依赖价值函数或采样基线），DPO 避免了梯度的高方差问题，使得训练过程更加稳定。</li>
    </ul>
  </li>
</ul>

<h2 id="实验">实验</h2>

<p>略</p>]]></content><author><name>Xingjie Gao</name><email>xingjie-gao@outlook.com</email></author><category term="nlp" /><category term="自然语言处理" /><category term="论文笔记" /><summary type="html"><![CDATA[DPO 论文学习]]></summary></entry><entry><title type="html">Tool Learning with Foundation Models 论文笔记</title><link href="http://localhost:4000/2024/11/18/Tool-Learning-with-Foundation-Models-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.html" rel="alternate" type="text/html" title="Tool Learning with Foundation Models 论文笔记" /><published>2024-11-18T00:00:00+08:00</published><updated>2024-11-18T00:00:00+08:00</updated><id>http://localhost:4000/2024/11/18/Tool%20Learning%20with%20Foundation%20Models%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0</id><content type="html" xml:base="http://localhost:4000/2024/11/18/Tool-Learning-with-Foundation-Models-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.html"><![CDATA[<h1 id="tool-learning-with-foundation-models-论文笔记">Tool Learning with Foundation Models 论文笔记</h1>

<!---more-->

<h2 id="不懂的知识">不懂的知识</h2>

<h3 id="概念工具">概念工具</h3>

<p><strong>概念工具</strong>（Conceptual Tools）是指帮助人类理解、分析、解决问题或推动学习与创新的一种抽象工具。这些工具通常以思想、模型、框架或方法论的形式存在，旨在扩展人类的认知能力，而不是直接作用于物理世界。它们为我们提供结构化的方式来处理复杂的信息和概念问题，支持逻辑推理、知识构建和创造性思维。</p>

<hr />

<h4 id="概念工具的核心特点">概念工具的核心特点</h4>

<ol>
  <li><strong>抽象性</strong>：概念工具以非物理形式存在，更多是思想和方法的体现，例如理论模型或数学公式。</li>
  <li><strong>广泛适用性</strong>：它们可以跨学科、跨领域应用，用于多种场景的问题解决和知识深化。</li>
  <li><strong>认知扩展</strong>：通过系统化的思维方式或结构化框架，帮助人类简化复杂问题或生成新的见解。</li>
  <li><strong>非依赖性</strong>：与具体的物理工具不同，概念工具存在于人类的思维过程中，可以通过学习和训练掌握。</li>
</ol>

<hr />

<h4 id="常见的概念工具类型">常见的概念工具类型</h4>

<ol>
  <li>
    <p><strong>框架和模型（Frameworks and Models）</strong></p>

    <ul>
      <li>描述现实世界的一种简化或抽象。</li>
      <li>示例：SWOT分析（用于战略规划），供需模型（经济学）。</li>
    </ul>
  </li>
  <li>
    <p><strong>逻辑和推理工具（Logical and Reasoning Tools）</strong></p>

    <ul>
      <li>提供系统化的思维方法，用于推理和决策。</li>
      <li>示例：归纳推理、演绎推理、决策树。</li>
    </ul>
  </li>
  <li>
    <p><strong>符号和语言系统（Symbolic and Language Systems）</strong></p>

    <ul>
      <li>帮助描述和传递复杂的抽象概念。</li>
      <li>示例：数学符号、编程语言、逻辑表达式。</li>
    </ul>
  </li>
  <li>
    <p><strong>类比与隐喻（Analogies and Metaphors）</strong></p>

    <ul>
      <li>通过将新概念与已知概念关联，促进理解。</li>
      <li>示例：用水流比作电流，用“DNA是生命的代码”形容遗传信息。</li>
    </ul>
  </li>
  <li>
    <p><strong>思维与学习方法（Cognitive and Learning Strategies）</strong></p>

    <ul>
      <li>支持信息组织、问题解决和知识构建。</li>
      <li>示例：思维导图、问题解决的建构主义方法。</li>
    </ul>
  </li>
</ol>

<hr />

<h4 id="概念工具的功能">概念工具的功能</h4>

<ol>
  <li>
    <p><strong>简化复杂性</strong>：通过提供结构化框架或模型，帮助人类分解复杂问题，使其更易于分析和理解。</p>

    <ul>
      <li>例如，用供应链模型简化全球物流的运作分析。</li>
    </ul>
  </li>
  <li>
    <p><strong>支持逻辑推理与决策</strong>：概念工具为推理提供指导或验证逻辑链条的工具。</p>

    <ul>
      <li>例如，布尔逻辑真值表用于计算复杂的逻辑表达式。</li>
    </ul>
  </li>
  <li>
    <p><strong>促进创新与创造性思维</strong>：通过新颖的视角或框架激发创造力。</p>

    <ul>
      <li>例如，类比思维促使我们从生物学现象中获取技术灵感（仿生学）。</li>
    </ul>
  </li>
  <li>
    <p><strong>提升学习与认知效率</strong>：通过认知工具（如学习软件）减少负担，将注意力集中在高级认知任务上。</p>

    <ul>
      <li>例如，在线学习工具让学生专注于知识应用，而非信息收集。</li>
    </ul>
  </li>
  <li>
    <p><strong>跨学科连接</strong>：帮助整合不同领域的知识，形成综合性解决方案。</p>

    <ul>
      <li>例如，生态系统模型结合了生物学、物理学和经济学的知识。</li>
    </ul>
  </li>
</ol>

<hr />

<h4 id="概念工具与物理工具的对比">概念工具与物理工具的对比</h4>

<table>
  <thead>
    <tr>
      <th>特性</th>
      <th>概念工具</th>
      <th>物理工具</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>存在形式</strong></td>
      <td>抽象、基于思想和逻辑</td>
      <td>具体、作用于物理世界</td>
    </tr>
    <tr>
      <td><strong>作用范围</strong></td>
      <td>支持认知活动，帮助分析和解决问题</td>
      <td>辅助完成实际操作任务</td>
    </tr>
    <tr>
      <td><strong>例子</strong></td>
      <td>数学模型、逻辑推理、思维框架</td>
      <td>锤子、剪刀、计算机硬件</td>
    </tr>
    <tr>
      <td><strong>应用场景</strong></td>
      <td>思维活动、学习、创新</td>
      <td>建筑、制造、日常物理操作</td>
    </tr>
  </tbody>
</table>

<hr />

<h4 id="总结">总结</h4>

<p>概念工具是人类认知能力的延伸，帮助我们以系统化、逻辑化的方式应对复杂性。这些工具存在于思想中，通过模型、框架或推理方式引导学习和解决问题的过程。它们广泛应用于教育、科学研究、商业战略、技术开发等领域，是推动知识增长和创新的重要资源。</p>

<h3 id="认知工具">认知工具</h3>

<p><strong>认知工具</strong>（Cognitive Tools）是用于辅助人类思维、学习和解决问题的一类工具。它们的主要功能是增强人类的认知能力，而不是替代人的思维。这些工具可以是抽象的概念（如思维模型）或具体的技术（如软件），能够帮助人们更高效地组织信息、推理、分析和创造。</p>

<hr />

<h4 id="认知工具的特点">认知工具的特点</h4>

<ol>
  <li><strong>辅助作用</strong>：认知工具是人类认知活动的扩展器，帮助提高效率，而不是直接完成任务。</li>
  <li><strong>增强学习与推理</strong>：它们特别适用于复杂问题的学习、分析和解决，促进深度思考。</li>
  <li><strong>动态性</strong>：许多认知工具可以随着任务需求而灵活适配，帮助人们应对多样化的认知挑战。</li>
  <li><strong>个性化</strong>：认知工具常被用于个性化学习或工作场景，为用户提供定制化的支持。</li>
</ol>

<hr />

<h4 id="认知工具的功能">认知工具的功能</h4>

<p>认知工具的功能可以大致归纳为以下几个方面：</p>

<ol>
  <li>
    <p><strong>支持认知过程</strong>：</p>

    <ul>
      <li>帮助用户记录思考过程或中间结果。</li>
      <li>提供直观的组织和分析复杂信息的方式。</li>
      <li>例如：笔记软件、思维导图工具。</li>
    </ul>
  </li>
  <li>
    <p><strong>减轻认知负担</strong>：</p>

    <ul>
      <li>自动完成某些低级别的认知任务（如记忆或简单运算），从而让用户专注于更高层次的思维任务。</li>
      <li>例如：计算器、语法检查工具。</li>
    </ul>
  </li>
  <li>
    <p><strong>激发创造性思维</strong>：</p>

    <ul>
      <li>提供新颖的视角或灵感，帮助用户生成新的想法。</li>
      <li>例如：头脑风暴软件、AI辅助设计工具。</li>
    </ul>
  </li>
  <li>
    <p><strong>模拟和测试</strong>：</p>

    <ul>
      <li>创建虚拟环境，让用户实验和测试假设。</li>
      <li>例如：虚拟实验室、医学模拟工具。</li>
    </ul>
  </li>
  <li>
    <p><strong>支持学习和技能发展</strong>：</p>

    <ul>
      <li>帮助用户学习复杂知识或技能，通过交互性工具提高理解能力。</li>
      <li>例如：在线学习平台、游戏化学习工具。</li>
    </ul>
  </li>
</ol>

<hr />

<h4 id="认知工具的类型与示例">认知工具的类型与示例</h4>

<ol>
  <li>
    <p><strong>组织和结构化工具</strong></p>

    <ul>
      <li>用于整理和结构化思维的工具。</li>
      <li>示例：思维导图工具（MindMeister）、Kanban任务板（Trello）。</li>
    </ul>
  </li>
  <li>
    <p><strong>学习和教育工具</strong></p>

    <ul>
      <li>支持学习过程的工具，通常提供个性化的学习体验。</li>
      <li>示例：语言学习应用（如 Duolingo）、在线课程平台（如 Coursera）。</li>
    </ul>
  </li>
  <li>
    <p><strong>数据分析和可视化工具</strong></p>

    <ul>
      <li>帮助用户分析数据并以图形化方式呈现结果。</li>
      <li>示例：Excel、Tableau。</li>
    </ul>
  </li>
  <li>
    <p><strong>逻辑与推理工具</strong></p>

    <ul>
      <li>用于支持用户进行逻辑推理和批判性思考。</li>
      <li>示例：决策树软件、布尔逻辑验证工具。</li>
    </ul>
  </li>
  <li>
    <p><strong>模拟和交互工具</strong></p>

    <ul>
      <li>提供虚拟环境让用户实验和测试。</li>
      <li>示例：医学诊断模拟工具、建筑设计模拟（如 SketchUp）。</li>
    </ul>
  </li>
  <li>
    <p><strong>AI驱动的认知工具</strong></p>

    <ul>
      <li>使用人工智能技术支持更复杂的任务，如生成内容或回答问题。</li>
      <li>示例：ChatGPT、图像生成工具。</li>
    </ul>
  </li>
</ol>

<hr />

<h4 id="认知工具的应用场景">认知工具的应用场景</h4>

<ol>
  <li>
    <p><strong>教育与培训</strong>：</p>

    <ul>
      <li>学生使用认知工具来增强理解能力和记忆力，例如通过在线学习工具学习新知识。</li>
    </ul>
  </li>
  <li>
    <p><strong>问题解决</strong>：</p>

    <ul>
      <li>专业人员使用认知工具进行问题分析和解决，例如使用数据分析工具挖掘商业趋势。</li>
    </ul>
  </li>
  <li>
    <p><strong>创造性活动</strong>：</p>

    <ul>
      <li>艺术家和设计师使用AI辅助工具进行创意生成，例如通过图像处理软件制作视觉内容。</li>
    </ul>
  </li>
  <li>
    <p><strong>研究与开发</strong>：</p>

    <ul>
      <li>科学家利用认知工具进行建模、仿真和数据处理，从而探索新现象或开发新技术。</li>
    </ul>
  </li>
</ol>

<hr />

<h4 id="认知工具与物理工具的对比">认知工具与物理工具的对比</h4>

<table>
  <thead>
    <tr>
      <th>特性</th>
      <th>认知工具</th>
      <th>物理工具</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>作用范围</strong></td>
      <td>扩展思维、分析、学习能力</td>
      <td>改变物理世界或完成物理任务</td>
    </tr>
    <tr>
      <td><strong>存在形式</strong></td>
      <td>抽象的概念或数字化形式</td>
      <td>具体的物体（如锤子、剪刀）</td>
    </tr>
    <tr>
      <td><strong>例子</strong></td>
      <td>思维导图、学习应用、AI工具</td>
      <td>锤子、螺丝刀、打印机</td>
    </tr>
  </tbody>
</table>

<hr />

<h4 id="总结-1">总结</h4>

<p>认知工具是人类思维能力的延伸，帮助我们更高效地完成复杂认知任务。它们在教育、科研、商业、创造性领域中都有广泛的应用，并随着技术进步（如人工智能的兴起）变得更加智能化和个性化。</p>

<h2 id="摘要">摘要</h2>

<h3 id="背景">背景：</h3>

<ol>
  <li>现状：LLM 有使用工具的潜力：这一范式被称为用基础模型进行工具学习，结合了专用工具和基础模型的优势，以实现问题解决中更高的准确性、效率和自动化。</li>
  <li>困难：缺乏对 LLM 使用工具的困难和挑战的全面理解</li>
</ol>

<h3 id="工作">工作：</h3>

<ol>
  <li><strong>介绍背景</strong>：我们首先介绍工具学习的背景，包括其认知起源、基础模型的范式转变以及工具与模型的互补角色。</li>
  <li><strong>指定框架</strong>：回顾现有的工具学习研究，并制定了一个通用的工具学习框架：
    <ol>
      <li>从理解用户指令开始，模型应学习将复杂任务分解为几个子任务，</li>
      <li>通过推理动态调整其计划，</li>
      <li>并通过选择适当的工具有效地征服每个子任务。</li>
    </ol>
  </li>
  <li><strong>讨论</strong>：我们还讨论了如何训练模型以提高工具使用能力，并促进工具学习中的泛化。</li>
  <li><strong>实验</strong>：对 18 种具有代表性的工具进行了实验，并展示了当前基础模型在熟练使用工具方面的潜力</li>
  <li><strong>讨论开发问题</strong>：讨论了一些需要进一步研究的工具学习开放问题，如确保安全和可信的工具使用、利用基础模型实现工具创建以及解决个性化挑战</li>
</ol>

<h2 id="引言">引言</h2>

<h3 id="工具学习的发展">工具学习的发展</h3>

<ol>
  <li>工具对人类有极大的帮助</li>
  <li>早期简单模型能力不足，使用工具学习比较困难</li>
  <li>近期更具能力的基础模型的出现，标志着能力大幅提升，使得工具学习变得可行</li>
  <li>基础模型+工具的范式出现，但是主要集中在一些特殊任务或领域上，对工具学习的理解仍然不够全面，无法估计其特征和未来发展。</li>
  <li>所以：审查和总结基于基础模型的工具学习的当前进展对于探索其潜力和挑战，并为未来的技术进步铺平道路至关重要。</li>
</ol>

<h3 id="本文工作大纲目录">本文工作（大纲&amp;目录）</h3>

<p>全面的调查工具学习的现状，理解工具学习的挑战、机遇和方向</p>

<ol>
  <li>==§2==  介绍背景
    <ol>
      <li>==§2.1== 人类历史上工具使用的认知起源及其对人工智能系统中工具使用的潜在影响</li>
      <li>==§2.2== 随后从用户界面的角度对工具进行了分类</li>
      <li>==§2.3== 回顾了基础模型带来的人工智能范式转变，强调了工具学习的出现及其重要性</li>
      <li>==§2.4== 讨论了工具和基础模型的互补作用，并认为将两者整合可以带来多种优势，</li>
    </ol>
  </li>
  <li>==§3== 现有工具学习探索的综合文献综述
    <ol>
      <li>==§3.1== 制定了一个通用的学习框架</li>
      <li>==§3.2==制定了整个工具学习的过程</li>
      <li>==§3.3==训练策略：从示范中学习和从反馈中学习</li>
      <li>为了促进将学习到的工具使用技能转移到新工具和新情况，即可推广的工具学习，设计一个统一接口，使模型能够以标准化的方式与不同工具互动是非常重要的。</li>
    </ol>
  </li>
  <li>==§4== 实验：我们基于我们的框架对 18 种代表性工具进行了实验（第 4 节），</li>
  <li>==§5== 讨论开放问题：讨论了我们的通用框架应用于现实世界场景所需关注的其他重要研究主题
    <ol>
      <li>==§5.1== 安全和可信的工具使用</li>
      <li>==§5.2== 针对大型复杂系统的工具学习</li>
      <li>==§5.3== 工具创造</li>
      <li>==§5.4== 个性化工具学习</li>
      <li>==§5.5== 具身学习与工具学习</li>
      <li>==§5.6== 工具学习中的知识冲突</li>
      <li>==§5.7== 其他未解决的问题</li>
    </ol>
  </li>
</ol>

<h2 id="背景-1">背景</h2>

<ol>
  <li>在本节中，我们首先讨论人类工具使用的认知起源（§ 2.1），</li>
  <li>接着通过用户界面的视角进行工具分类（§ 2.2）。</li>
  <li>然后，我们回顾基础模型带来的近期人工智能范式转变（§ 2.3）及其在工具学习中的重要性。</li>
  <li>之后，我们考察专业工具和基础模型在问题解决中的各自角色，并讨论它们整合的优势和挑战（§ 2.4）。</li>
</ol>

<h3 id="21-工具使用的认知起源">2.1 工具使用的认知起源</h3>

<ol>
  <li><strong>工具在数千年的人类进化历史中扮演了至关重要的角色</strong>。
    <ul>
      <li>人类与动物使用工具不同，人类能够制造出比其他动物更复杂的工具，这种能力可能归因于我们对因果关系的深刻理解，这使我们能进行技术推理</li>
    </ul>
  </li>
  <li><strong>人类使用工具的神经基础</strong>
    <ul>
      <li>顶叶系统：在工具使用研究中，顶叶系统被认为是处理工具操作和观察的核心区域：它协助我们理解物体的用途和与周围环境的关系。
  例如，当观察他人使用工具时，顶叶系统会被激活，帮助我们推测工具的功能以及如何使用。</li>
      <li>前上颞回：前上颞回会被激活，帮助我们理解工具的功能和使用方法（猕猴并没有表现出这一点）</li>
      <li>使用工具：认知科学的整体趋势是将认知理解为一种强调与外部世界互动的活动过程（Engel et al., 2013），而观察、沟通和动手实践的反馈对于掌握工具使用非常重要。</li>
    </ul>
  </li>
  <li><strong>工具使用的三种智力水平</strong>
    <ul>
      <li>Assistive tool : 使用通常是被动和无意识的 (例如，在雨棚走廊中行走）)</li>
      <li>Arbitrary tool : 任意工具使用需要主动互动（例如，驾驶、使用智能手机）</li>
      <li>Free tool : 自由工具使用进一步需要理解和选择适合场景的工具（例如，烹饪新的菜肴）</li>
      <li>在这个框架中，这三种工具使用模式呈现出递进关系，作者假设实现自由工具使用的关键认知过程是技术推理，这使得某人能够通过观察他人使用、选择或制作工具，而不是通过大量实践来学习新的动作。</li>
    </ul>
  </li>
  <li><strong>从物理工具到概念工具的过渡</strong>
    <ul>
      <li>认知工具：它指的是一种辅助工具，促进更高阶的思维（例如，多步批判性分析、创造性解决问题方案的产生）。</li>
      <li>认知工具可以根据其提供的功能进行分类（Lajoie &amp; Derry, 2013）。这些功能包括
        <ul>
          <li>(1) 支持认知过程（例如，记录中间推理结果），</li>
          <li>(2) 减轻低级认知负荷，以释放资源用于高级思维，</li>
          <li>(3) 使学习者能够参与超出他们能力范围的活动，</li>
          <li>(4) 允许学习者生成和测试假设（例如，为医学学生模拟诊断）。</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>弥合人类使用工具与机器使用工具之间的差距</strong>
    <ul>
      <li>首先，操控工具的能力深深植根于我们的认知和感知系统中，并在数百万年的进化中形成。相比之下，基础模型主要依赖于预训练数据的统计模式，基础模型的工具使用能力与人类相对而言仍存在显著差距。人类能够感知工具的属性，理解其功能，并识别适合每个任务的工具。（==§3.2.1§==讲解 LLM 该如何学习该过程）</li>
      <li>其次，人类擅长将复杂任务分解为更小的可管理子任务，并灵活地操控工具以完成每个子任务。然而，基础模型缺乏充分理解和利用工具所需的物理具身和感官体验。因此，这些模型在需要更高阶推理和适应性任务时常常难以应对，且无法有效整合多个知识和工具来源。（==§3.2.2§==讲如何更好地利用大模型的推理能力制定可执行计划）</li>
      <li>此外，当前将基础模型适应于学习特定工具的算法通常需要大量的监督数据（Nakano 等，2021；Reed 等，2022），这限制了其对更广泛工具或新情况的普适性和迁移能力。（==§ 3.3.1== 和 ==§ 3.3.2==总结了工具学习的训练策略  ==§ 3.3.3== 讨论如何促进工具学习的普适性和迁移性)</li>
    </ul>
  </li>
</ol>

<h3 id="22-工具分类用户界面视角">2.2 工具分类：用户界面视角</h3>

<p>本文的重点特别是那些可以通过指令与基础模型结合操作的工具。我们介绍了一种根据工具的表达和交互方式进行分类的系统。</p>

<p><img src="/assets/posts_assets/Pasted%20image%2020241125160243.png" alt="" /></p>

<ol>
  <li>物理工具：这类工具涉及与物理世界的直接交互，如机器人、传感器和可穿戴设备等，能够对环境产生实质影响。</li>
  <li>GUI 工具：一些工具允许用户通过交互界面进行操作，即工具的视觉表示，以及预定义的操作。这些工具被定义为基于 GUI 的工具，它们对物理世界没有直接影响。如像浏览器、Microsoft Office、Adobe PhotoShop 等经过良好开发的软件</li>
  <li>基于编程的工具：用户可以访问的最内层工具是源代码，为这些基于程序的工具的输入和输出提供了高度的灵活性。基于程序的工具是主要通过编程接口而非可视化接口设计的软件工具。它们可以有多种形式，包括声明性语言、编程库、软件开发工具包（SDK）甚至基于神经网络的工具。</li>
</ol>

<p>它们并不是严格互斥的，而是相互交织的倾向。人类有能力通过灵活执行不同类型的工具来处理复杂任务。本文认为，无论工具类型如何，基本上都可以通过建立中介接口来利用基础模型来执行它们。我们将在§3.3.3 中介绍统一不同工具接口的方法。</p>

<h3 id="23-基础模型的范式转变">2.3 基础模型的范式转变</h3>

<ol>
  <li>PLM 的作用：
    <ul>
      <li>PLM 出现，利用 PLM 作为基础设施，自然语言作为媒介统一执行各种任务，所有自然语言理解和生成过程均通过对话交互完成。</li>
      <li>PLM 的强大泛化能力使我们能够使用自然语言作为媒介，通过操控工具来完成这些任务。</li>
    </ul>
  </li>
  <li>工具学习可以被 PLM 支持的原因：
    <ul>
      <li><strong>本质上，工具学习的关键在于将复杂任务分解为子动作，以自然语言的形式对动作进行分词，并将其转换为特定工具能够理解的可执行指令。语言模型充当“翻译者”，使复杂任务对没有专业技术知识的个体更加可及。</strong></li>
    </ul>
  </li>
  <li>然而，仍然存在许多超出纯自然语言范围的任务。
    <ul>
      <li>例如：生成演示文稿、通过 CAD 应用程序构建 3 D 模型，以及通过分析团队成员日历安排会议，这些都是传统人工智能尚未定义的复杂任务。</li>
    </ul>
  </li>
  <li>工具学习未来展望
    <ul>
      <li>因此，虽然自然语言界面在语言领域内实现了统一（Hao et al., 2022），但非语言任务带来的挑战需要一种更先进的方法来利用自然语言和工具学习。通过利用自然语言的力量，我们可以创建能够理解和适应我们周围复杂和动态世界的系统，从而开启创新和发现的新途径。</li>
    </ul>
  </li>
</ol>

<h3 id="24-工具与基础模型的互补角色">2.4 工具与基础模型的互补角色</h3>

<p>专业工具与基础模型的整合代表了一种有前景的方法，以利用两者的独特优势。通过将基础模型的理解和推理能力融入专业工具中，我们可以创建能够执行比专业工具或基础模型单独更复杂任务的智能工具。具体而言，两者的结合带来了以下诸多好处。</p>

<h4 id="使用工具的好处">使用工具的好处</h4>

<ol>
  <li>减轻记忆负担：
    <ul>
      <li>尽管基础模型在记忆方面表现出色（Carlini et al., 2021, 2022, 2023），但它们并不能记住每一条训练数据。</li>
      <li>单靠记忆并不能支持实时更新知识</li>
      <li>基础模型还被批评为会幻想知识</li>
    </ul>
  </li>
  <li>提升专业性</li>
  <li>更好的可解释性。</li>
  <li>提升鲁棒性：LLM 的输入的轻微修改可以改变模型预测。这是因为这些模型在很大程度上依赖于训练数据中的统计模式。相反，工具是专门为其预期用途而设计的，这可能与输入扰动无关。</li>
</ol>

<h4 id="使用-llm-的好处">使用 LLM 的好处</h4>

<ol>
  <li>改善决策和推理能力</li>
  <li>更好的用户体验
    <ul>
      <li>得益于基础模型强大的意图理解能力，工具学习可能会彻底改变我们与机器的互动方式，并减轻用户的认知负担，使他们能够参与更高阶的思维和决策过程。这反过来又促进了一种无缝且更自然的基于语言的交互范式，彻底改变了传统的图形用户界面（GUI）。<strong>用户只需提供高层次的指导和方向，模型就能无缝理解用户的意图，从而提供更个性化和准确的响应。</strong></li>
      <li>降低了新用户的入门障碍，还为创新和创造力开启了无尽的可能性。</li>
    </ul>
  </li>
</ol>

<h2 id="工具学习">工具学习</h2>

<h3 id="一般工具学习的框架">一般工具学习的框架</h3>

<p><img src="/assets/posts_assets/Pasted%20image%2020241125193731.png" alt="" /></p>

<h4 id="tool-set">Tool Set</h4>

\[T=\{T_1,T_2,\dots \}\]

<ul>
  <li>工具集 T 包含一系列具有不同功能的工具。</li>
  <li>在接下来的章节中，我们主要以应用程序编程接口（API）作为例子来说明如何与工具互动。</li>
</ul>

<blockquote>
  <p>在这里，我们将 API 定义为任何可以将基础模型的输出作为输入的函数。例如，对于一个天气 API，API 的输入可能是位置和时间，而输出可能包含温度或风速。</p>
</blockquote>

<h4 id="environment">Environment</h4>
<p>\(\mathcal{E}\)</p>

<p>环境 $\mathcal{E}$ 是工具运行的世界</p>

<ol>
  <li>它向感知者提供工具的执行结果。</li>
  <li>它提供了工具执行所需的基础设施，这可以是虚拟的也可以是真实的。
    <ul>
      <li>前者指的是一个模拟环境，允许模型与工具的数字表示进行交互，而真实环境则涉及与物理工具的实际互动。</li>
      <li>虚拟环境的优势在于易于访问和复制，使得模型的培训更具成本效益。然而，虚拟环境可能无法完全复制真实世界环境的复杂性，导致过拟合和较差的泛化能力（Hansen 等，2021）。</li>
      <li>真实环境提供了更真实的背景，但可能更难以接触，并且涉及更高的成本。</li>
    </ul>
  </li>
</ol>

<h4 id="controller">Controller</h4>

\[\mathcal{C}\]

<p>控制器 $\mathcal{C}$ 作为工具学习框架的“大脑”，通常使用基础模型进行建模。</p>

<ul>
  <li>控制器 C 的目的是提供一个可行且精确的计划，以使用工具满足用户的请求</li>
  <li>制定计划：为此，C 应该理解用户意图，以及意图与可用工具之间的关系，然后制定一个计划，以选择适当的工具来处理任务.</li>
  <li>分解子任务：在查询复杂且针对高层次任务的情况下，C 可能需要将任务分解为多个子任务，这需要基础模型具备强大的规划和推理能力</li>
</ul>

<h4 id="perceiver">Perceiver</h4>
<p>\(\mathcal{P}\)
感知者 P 负责处理用户和环境的反馈，并生成一个摘要供控制器使用。</p>

<ul>
  <li>简单的反馈处理形式包括将用户和环境反馈进行连接，或使用预定义模板格式化反馈。</li>
  <li>然后，汇总的反馈被传递给控制器，以协助其决策。通过观察这些反馈，控制器可以确定生成的计划是否有效，以及在执行过程中是否存在需要解决的异常情况。</li>
  <li>在更复杂的情境下，感知者应能够支持多种模态，如文本、视觉和音频，以捕捉用户和环境反馈的多样性。</li>
</ul>

<h4 id="连接组件">连接组件</h4>

<h5 id="工作过程">工作过程</h5>

<p>假设有一个工具集 $\mathcal{T}$, 在时间步 $\mathcal{t}$ 的时候，执行以下步骤</p>

<ol>
  <li>环境 $\mathcal{E}$ 提供了工具执行的反馈 $e_t$</li>
  <li>感知者接受环境反馈 $e_t$ 和用户反馈 $f_t$ 并生成一个反馈总结 $x_t$
    <ul>
      <li>通常感知者可以通过预定义的规则（将 $e_t$ 与 $f_t$ 连接起来），或者使用神经网络建模</li>
    </ul>
  </li>
  <li>控制器 $\mathcal{C}$ 生成计划 $a_t$ 从工具集 $\mathcal{T}$ 中选择一个工具来执行</li>
</ol>

<p>公式：</p>

<p>\(p_{C}(a_{t})=p_{\theta_{C}}(a_{t}\mid x_{t},{\mathcal{H}}_{t},q)\)</p>
<ul>
  <li>$\theta_C$ 表示 C 的参数，q 表示用户的查询或指令，$\mathcal{H}<em>t = { (x_s,a_x)}^{t-1}</em>{s=0}$ 表示历史反馈和计划</li>
  <li>$\mathcal{C}$ 还可以将其推理过程与行动预测协同作用
    <ul>
      <li>子任务 1：选择工具 $T_i$ ​：从工具集合 T 中选择一个适合当前任务的工具。</li>
      <li>子任务 2：制定具体计划 $a_t$ ​：确定如何使用选择的工具执行动作。</li>
    </ul>
  </li>
</ul>

\[p_{\theta_{C}}(a_{t}\mid x_{t},{\mathcal{H}}_{t},q)=\sum__{i}\in{\mathcal{T}}}p_{\theta_{C}}(a_{t}\mid{\mathcal{T}}_{i},x_{t},{\mathcal{H}}_{t},q)\times p_{\theta_{C}}({\mathcal{T}}_{i}\mid x_{t},{\mathcal{H}}_{t},q),\]

<ol>
  <li>在生成计划 at 后，它将在 E 中执行，来自 E 的反馈 et+1 将被传递给感知者。上述过程会重复多轮，直到控制器完成任务。</li>
</ol>

<p>总体目标是找到一个动作序列{at}，最终实现用户指令 q 所指定的任务。请注意，在工具执行后，控制器可能还会将执行结果整合成一个合理的回应给用户</p>

<h5 id="例子">例子</h5>

<p>例如，给定“我想预定下周去北京的航班”这样的指令，</p>

<ol>
  <li>控制器 C 首先推断出用户的目标是预定航班，北京是目的地，下周是旅行时间。</li>
  <li>然后，模型选择航空公司预订系统作为工具。</li>
  <li>最后，它将时间和目的地输入作为初步计划。</li>
  <li>在进行预订的过程中，我们可能会面临意外情况，比如下周前往北京的航班不可用。为应对这些异常情况，我们可以进一步赋予 C 推理当前上下文的能力，并生成替代计划。</li>
</ol>

<h3 id="一般流程从意图到计划">一般流程：从意图到计划</h3>

<p>工具学习的总体过程需要不同组件之间的复杂交互。在本节中，我们将进一步阐述这一过程中的关键问题。</p>

<h4 id="理解用户的意图和工具">理解用户的意图和工具</h4>

<p>为了准确履行用户查询 q 指定的任务，控制器需要理解两个方面：</p>

<ul>
  <li>用户的根本意图，这涉及识别和形式化自然语言 q 为高层次任务（即意图理解）；</li>
  <li>工具集 T，这意味着理解其中每个工具的功能和目标（即工具理解）。</li>
</ul>

<h5 id="理解用户的意图">理解用户的意图</h5>

<ol>
  <li>指令微调：在用人类指令模板化的数据集上对大型语言模型进行微调，可以使模型甚至能够对未见任务的指令进行泛化</li>
  <li>挑战：
    <ol>
      <li>理解模糊指令。第一个挑战是处理用户查询中固有的模糊性和歧义。许多用户查询天生不精确，甚至可能是多义的，这要求控制器依赖于上下文线索和背景知识来推断用户的真实含义。
        <ul>
          <li>一种可能的解决方案是主动与用户互动，以澄清任何歧义，例如询问用户对先前查询的进一步说明。</li>
        </ul>
      </li>
      <li>对多样化指令的泛化。另一个挑战是使模型能够对更具多样性的用户指令进行泛化。由于意图空间在理论上是无限的，基础模型在训练期间几乎无法接触到每种现实世界的意图。
        <ul>
          <li>一种解决方案是引入更为多样化的训练数据，这些数据涵盖了广泛的真实世界场景，从而使模型能够学习不同指令的细微差别。</li>
          <li>另一种解决方案是利用用户反馈，主动地使模型适应个别用户，即个性化工具学习</li>
        </ul>
      </li>
    </ol>
  </li>
</ol>

<h5 id="理解工具">理解工具</h5>
<p>一般方法：prompt-learning</p>

<ul>
  <li>零样本提示（zero-shot prompting）：描述 API 的功能、输入/输出格式、可能的参数等。此方法允许模型理解每个 API 可以完成的任务。</li>
  <li>少样本提示（few-shot prompting）：为模型提供具体的工具使用示例，通过这些示例模仿人类行为，模型可以学习如何使用工具。</li>
</ul>

<p>挑战：</p>

<ul>
  <li>提示的效果很大程度上取决于模型本身的能力，能力较弱的模型可能无法很好地理解提示。</li>
  <li>提示受输入上下文长度的限制。当工具数量庞大且描述较长时，在一个提示中包含所有可能的工具信息变得不可行。</li>
</ul>

<p>解决方法：</p>

<ul>
  <li>一种潜在的解决方案是增加一个中间阶段的工具选择，首先检索一小部分最适合当前任务的工具。</li>
  <li>另一种解决方案是进行模型微调（Fine-tuning），优化模型以通过具体的工具使用案例来理解工具。</li>
</ul>

<h4 id="计划与推理">计划与推理</h4>

<blockquote>
  <p>如何通过推理和规划在工具学习中提升基础模型的能力，以及在复杂任务场景中的实现方法和挑战</p>

</blockquote>

<ol>
  <li>现有的 LLM 有涌现出来的推理能力</li>
  <li>定义推理：在基础模型中，推理能力通常被描述为“将复杂问题分解为子问题并逐步解决这些子问题的能力。</li>
  <li>一般方法：few-shot prompt learning、CoT</li>
</ol>

<ul>
  <li>推理研究可以分为两类：
    <ul>
      <li>内省式推理（Introspective Reasoning）：生成静态计划，不依赖环境交互。</li>
      <li>外向式推理（Extrospective Reasoning）：通过与环境交互迭代生成计划，并利用反馈调整计划。</li>
    </ul>
  </li>
</ul>

<h5 id="内省推理和外向式推理">内省推理和外向式推理</h5>

<ul>
  <li>
    <p><strong>内省式推理</strong>：</p>

    <ul>
      <li>在执行任务之前，生成一个静态的完整计划。</li>
      <li>不需要与环境或用户进行交互。</li>
      <li>优点：规划过程明确，适用于环境反馈较少的场景。</li>
      <li>局限性：如果任务中出现意外或环境改变，静态计划难以应对。</li>
    </ul>
  </li>
  <li>
    <p><strong>外向式推理</strong>：</p>

    <ul>
      <li>通过与环境交互逐步生成计划，每一步根据上一步的反馈进行调整。</li>
      <li>优点：能更好地处理动态变化的环境或任务中的意外情况。</li>
      <li>应用案例：
        <ul>
          <li>Self-Ask 和 ReAct（Yao et al., 2022b）：将任务分解为多个子问题，通过 API 或工具逐步回答。</li>
          <li>Auto-GPT：能够自动选择工具，分步完成复杂任务，同时优化计划。</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h5 id="多工具多子任务场景中的挑战">多工具、多子任务场景中的挑战</h5>

<ul>
  <li>
    <p><strong>任务分解复杂性</strong>：</p>

    <ul>
      <li>多工具、多子任务场景通常涉及多个步骤，这些步骤之间可能存在依赖关系，需要模型能够正确排序和选择工具。</li>
    </ul>
  </li>
  <li>
    <p><strong>模型要求</strong>：</p>

    <ul>
      <li>模型需要具备高水平的推理和环境适应能力。</li>
      <li>工具使用的切换成本较高，因此需要优化工具选择和执行效率。</li>
    </ul>
  </li>
  <li>
    <p><strong>当前研究的限制</strong>：</p>

    <ul>
      <li>现有研究更多关注单工具或单子任务的场景，对多工具、多步骤任务的探索较少。</li>
    </ul>
  </li>
</ul>

<h5 id="解决多工具任务的未来方向">解决多工具任务的未来方向</h5>

<ul>
  <li><strong>理解工具间依赖关系</strong>：模型需要识别任务中工具的优先级和依赖关系，以便合理安排工具调用顺序。</li>
  <li><strong>任务优化和效率提升</strong>： 优化模型在多工具任务中的执行效率，例如并行执行独立的子任务。</li>
  <li><strong>工具升级与动态适应</strong>：工具可能会更新或发生变化，模型需要具有适应能力，能够实时调整任务规划。</li>
</ul>

<h3 id="工具学习的训练策略">工具学习的训练策略</h3>

<ul>
  <li><strong>示范学习</strong>：通过观察标注数据掌握工具使用。</li>
  <li><strong>反馈学习</strong>：通过与环境和人类交互优化任务执行。</li>
</ul>

<h4 id="示范学习">示范学习</h4>

<p>数据集：$D = { (q_i, a_i^*) }_{i=0}^{N-1}$</p>

<ul>
  <li>用户查询 $q_i$：用户的输入或任务需求。</li>
  <li>人类标注 $a_i^*$：人类对如何处理该查询的示范。</li>
</ul>

<p>优化目标：</p>

<ul>
  <li>通过最大化以下公式来优化控制器参数 $\theta_C$：
\(\theta_C^* = \arg\max_{\theta_C} \mathbb{E}_{(q_i, a_i^*) \in D} \prod_{t=0}^{T_i} p_{\theta_C}(a_{i,t}^* \mid x_{i,t}, \mathcal{H}_{i,t}, q_i)\)</li>
  <li><strong>解释</strong>：
    <ol>
      <li><strong>目标</strong>：使控制器在处理用户查询 qiq_i 时，生成的人类标注 $a_i^*$的概率最大化。</li>
      <li><strong>公式中的变量</strong>：
        <ul>
          <li>$T_i$：处理 $q_i$ 的总迭代次数（任务需要的步骤数）。</li>
          <li>$a_{i, t}^*$：第 $t$ 次迭代中的人类标注（示范）。</li>
          <li>$x_{i, t}$：第 $t$ 次的总结反馈（来自环境的结果和用户的输入）。</li>
          <li>$\mathcal{H}_{i, t}$：历史记录，包含此前的反馈和动作。</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<h5 id="监督学习">监督学习</h5>

<p>传统上，行为克隆在学习用于自主车辆和机器人应用的端到端或模块化感知-控制模型方面得到了广泛探索</p>

<p>例子：</p>

<ul>
  <li>WebGPT
    <ul>
      <li>作者首先构建了一个由 Bing 支持的搜索界面，然后微调 GPT-3（Brown 等，2020）以克隆人类的网页搜索行为。</li>
      <li>作为一个在通用领域上预训练的语言模型，原始的 GPT-3 在本质上并不依赖于有效的浏览器命令。因此，首先收集人类与浏览器交互的示例，然后学习状态到行动的映射是至关重要的。</li>
      <li>在微调后，该模型在操作搜索引擎进行信息检索方面表现出卓越的能力，甚至超过了人类专家。</li>
    </ul>
  </li>
  <li>WebShop:
    <ul>
      <li>提供了一个基于 web 的互动环境，代理可以浏览和购买产品。通过行为克隆，训练后的代理在根据人类指示购买正确产品方面表现出非平凡的性能。</li>
    </ul>
  </li>
</ul>

<h5 id="半监督学习">半监督学习</h5>

<ul>
  <li>有标签的数据比较难获得</li>
  <li>使用一个能力较弱的模型来为未标记数据标注伪标签，并将其转化为弱监督的工具使用示范。
    <ul>
      <li>例如，Baker 等人（2022）使用少量的种子标记数据训练模型，以预测在 Minecraft 视频游戏中每个时间步采取的行动的伪标签。通过学习这些伪标签，可以训练出一个更强大的模型，而无需在目标环境中推理模型或进行大规模的黄金标准人类行为标注。</li>
    </ul>
  </li>
</ul>

<h5 id="自监督学习">自监督学习</h5>

<ul>
  <li>自监督学习。尽管减少了对人类行为标注的严格要求，半监督学习仍然需要一个种子标记数据集来获取伪标签。</li>
  <li>
    <p>此外，种子数据集中的偏差在训练过程中可能会被放大，从而导致较差的泛化性能。</p>
  </li>
  <li>为此，研究人员最近表明，通过少量示例，基础模型可以自我学习如何以自监督的方式使用工具（Parisi et al., 2022；Schick et al., 2023）。
    <ul>
      <li>例如，Toolformer（Schick et al., 2023）利用基础模型的上下文学习能力，基于少量人工编写的示例迭代生成工具使用示例。这些自动生成的示例经过进一步过滤以减少噪声。最终的工具使用数据集包含足够的监督，从而显著提高了 GPT-J（Wang &amp; Komatsuzaki, 2021）的工具使用性能，突显了自监督学习在增强工具使用能力方面的潜力。</li>
    </ul>
  </li>
</ul>

<h4 id="反馈学习">反馈学习</h4>

<ul>
  <li>手动收集和标注工具使用的示例（可能包括完整的人类行为轨迹和最终答案）既耗时又劳动强度高。</li>
  <li>此外，所学的模型可能由于遵循记录下的人类行为而无法有效适应新环境。</li>
  <li>除此之外，明确标注环境条件和代理行为的每一种可能场景也是不切实际的</li>
  <li>相比之下，人类通过试错学习来纠正和修正他们的工具使用行为（Allen et al., 2019）。同样，来自环境和人类的反馈能够让模型理解其行为的后果，并适应其行为。</li>
</ul>

\[\theta_C^* = \arg\max_{\theta_C} \mathbb{E}_{q_i \in Q} \mathbb{E}_{\{a_{i,t}\}_{t=0}^{T_i} \sim p_{\theta_C}} \left[ R(\{a_{i,t}\}_{t=0}^{T_i}) \right],\]

<p>强化学习反馈来源：环境反馈和人类反馈，这可以被视为工具学习中的奖励信号来源。这两种反馈是互补的，可以相互结合。</p>

<h5 id="环境反馈">环境反馈</h5>

<p>控制器与环境互动，并接收有关其行为后果的反馈。模型随后根据这些反馈更新其策略，以改善工具使用行为。</p>

<p><strong>环境反馈的形式</strong>：</p>

<ul>
  <li>结果反馈：指示模型的行动是否成功完成了任务</li>
  <li>中间反馈：指的是由一个动作触发的环境状态变化。通过观察状态变化，基础模型可以学习每个动作是否有效和适当，从而更好地调整其行为</li>
</ul>

<h5 id="人类反馈">人类反馈</h5>

<p>人类可以根据模型生成的计划给予模型奖励和惩罚，以调节其行为。</p>

<p>显式：在 1 到 5 的评分标准上对模型生成的动作质量进行评分</p>

<p>隐式：用户的比较（Ouyang 等，2022），响应时间，以及在接收到模型输出后采取的行动（例如，点击推荐链接）</p>

<p>缺点：</p>

<ol>
  <li>是标签密集型的，</li>
  <li>并且具有较高的延迟</li>
</ol>

<p>解决方法：RLHF</p>

<p>挑战：</p>

<ol>
  <li>任务特定性：特定任务的相应评估标准需要预先定义，<strong>为一个任务标注的偏好数据难以转移到其他设置，这限制了 RLHF 在更广泛任务中的适用性</strong>。为此，开发一个普适的奖励模型以适应各种任务至关重要；</li>
  <li>偏见：RL 代理优化朝向伪人类奖励模型，因此可能受到人类偏好的上限和偏见。此外，社会偏见或个人经验可能在 RLHF 过程中被放大，因此必须仔细评估学习到的奖励模型是否存在任何偏见，并采取措施进行缓解。</li>
</ol>

<h4 id="工具学习泛化">工具学习泛化</h4>

<p>泛化也是工具学习的一个重要方面，尤其考虑到存在大量迅速扩展的工具。</p>

<p><strong>虽然对大量工具使用数据进行监督微调可以成为促进泛化的潜在解决方案，但收集足够的监督工具使用数据并确保其质量和多样性既耗时又在实践中不可行。</strong></p>

<p>抽象能力：抽象是识别工具的基本特征的过程。抽象涉及识别工具的共性和模式，以便模型能够合成和转移其知识和技能，使其能够轻松使用新工具。</p>

<h5 id="泛化的基础api-统一">泛化的基础：API 统一</h5>

<ul>
  <li>该接口使模型能够以一致和标准化的方式操作各种工具</li>
  <li>通过统一的接口，模型可以更容易地在统一的工具协议中识别和抽象工具的基本特征，而不是费力地理解各种工具接口。</li>
</ul>

<p>API 统一的三种方式：语义接口、图形用户界面（GUI）接口和编程接口。</p>

<h6 id="语义接口">语义接口</h6>

<p>构建自然语言与具体行动的映射</p>

<p>例子：比如，<strong>ReAct</strong>（Yao等人，2022年）使用“Action:Search”作为触发器，表示执行“搜索相关段落”这一功能。</p>

<p>优点：简便，直观，自然</p>

<p>缺点：</p>

<ul>
  <li>生成的文本和具体工具动作之间的映射需要事先定义，这一过程非常繁琐，尤其是当工具或系统的功能快速扩展时。</li>
  <li>模型可能无法准确生成触发特定动作的准确文本，这可能导致误触发不相关的动作。</li>
</ul>

<h6 id="gui-接口">GUI 接口</h6>

<p>GUI 已经被广泛优化了</p>

<p>但是有缺点：</p>

<ul>
  <li>必须建立一个虚拟环境，以便将预测的标记映射到类人鼠标移动和键盘输入上。</li>
  <li>这些环境将模型限制在一组有限的预定义鼠标选项和常见的键盘操作中。</li>
</ul>

<p>发展：</p>

<ul>
  <li>通过利用基础模型，可以引入关于常见关键字和鼠标操作组合的先验知识，从而扩大模型能够执行的潜在操作。</li>
</ul>

<h6 id="编程接口">编程接口</h6>

<p>这种接口允许模型超越纯自然语言，使用程序指定其操作。这种统一要求模型熟悉函数调用的语法</p>

<p>当前的工作：</p>

<ul>
  <li>最近的代码生成语言模型（CLM），如 Incoder（Fried 等，2022）和 CodeX（Chen 等，2021），提供了这种统一的可能性。</li>
  <li>编程接口得到了广泛应用。例如，Code-as-Policies（Liang 等，2022 a）发现，使用 CLM 作为机器人控制的基础，机器人可以利用代码语法执行复杂动作，推广到新指令，并以准确的参数值对函数进行精确控制。</li>
</ul>

<p>优点：</p>

<ul>
  <li>（1）复杂的工具学习逻辑可以使用编程语言的控制流进行建模；</li>
  <li>（2）外部 API 的显式调用可以通过执行程序自然实现。</li>
</ul>

<h6 id="挑战">挑战</h6>

<p><strong>接口选择应与基础模型的能力和局限性相一致</strong></p>

<ul>
  <li>语言基础模型训练用于生成文本，因此可能更适合语义接口。</li>
  <li>结合视觉和文本信息的多模态基础模型可能更适合图形用户界面，因为它可以理解和生成类人鼠标移动和键盘输入。</li>
  <li>代码基础模型可能更适合编程接口，因为它训练用于理解代码语法和函数调用</li>
</ul>

<p>工具的输出与模型输入格式不一致的挑战</p>

<p>解决办法：</p>

<ol>
  <li>一个常见的做法是将模型和工具的功能组合在同一模态中
    <ul>
      <li>例如，Zeng 等人（2022）通过将它们的输出转换为自然语言，将各种模态的基础模型链在一起。这种简单的方法利用提示组合新的多模态能力，而无需微调。</li>
    </ul>
  </li>
  <li>构建能够感知一般模态的多模态基础模型
    <ul>
      <li>Gato（Reed 等，2022）是一个典型的通用多体现代理，经过庞大代理经验数据集的训练。Gato 可以通过不同的体现进行感知和行动，例如玩雅达利游戏、为图像添加标题、聊天等。</li>
      <li>PaLM-E（Driess 等，2023）将不同模态的连续输入纳入预训练语言模型。通过对多个体现任务的联合训练，PaLM-E 能够在现实世界中做出基于事实的决策。</li>
    </ul>
  </li>
</ol>

<h5 id="工具学习泛化的策略">工具学习泛化的策略</h5>

<ul>
  <li>仅仅统一 API 的是不够的。</li>
  <li>可泛化工具学习要求模型进一步适应、细化和专业化其学习的知识，以满足特定任务或领域的需求</li>
</ul>

<h6 id="元工具学习">元工具学习</h6>

<p><strong>元认知（Metacognition）</strong> 是指个体反思自己思维过程的能力，在面对陌生情境时能够调整自己的行为。</p>

<p>在工具学习中，元认知指的是模型能够反思自己的学习过程，并在需要时调整或改进工具使用策略。</p>

<ul>
  <li><strong>元工具学习的核心思想</strong>是让模型能够根据之前的经验调整其工具使用策略，从而迁移到新任务或新领域。
    <ul>
      <li>例如，假设一个模型已经在<strong>Bing搜索引擎</strong>上进行过训练，当该模型迁移到<strong>Google搜索引擎</strong>时，它可以通过元认知能力，识别出在搜索引擎使用中常见的策略模式（如有效的搜索查询、相关的结果以及用户反馈），并基于这些经验，调整自己的搜索策略，以适应新的搜索引擎的算法和用户界面。</li>
    </ul>
  </li>
  <li>优点：这种能力使得模型能够在新的环境中更加智能地调整自己，从而提高其在不同工具和任务之间的迁移能力。</li>
</ul>

<h6 id="课程工具学习">课程工具学习</h6>

<p><strong>课程学习（Curriculum Learning）</strong> 是一种从简单到复杂的学习方法。它从基础工具入手，逐步引导模型学习更复杂的工具，从而使模型能够在先前知识的基础上逐渐建立对工具的更深理解。</p>

<ul>
  <li><strong>课程工具学习的核心思想</strong>是从最简单的工具或基本概念开始，逐步引入更复杂的工具或操作。这种方法确保模型先学习工具的基本特性，再逐渐深入到更复杂的应用。
    <ul>
      <li>例如，在教授一个模型使用<strong>Mathematica</strong>时，首先可以从基础的加法和减法开始，逐步引导模型学习更复杂的数学概念，比如微积分和线性代数。通过这种方法，模型能够理解复杂工具是如何建立在简单工具的基础上的，并能够把简单工具的知识迁移到更复杂的工具上</li>
    </ul>
  </li>
  <li>优点：
    <ul>
      <li><strong>理解工具之间的关系</strong>：课程工具学习不仅帮助模型掌握个别工具的使用，还帮助模型理解复杂工具是如何通过多个基本工具组合而成的。这种理解可以帮助模型更好地识别工具之间的相似性和差异性，从而在面对新工具时，能够更有效地调整自己的策略。</li>
      <li><strong>渐进式学习</strong>：通过从简单到复杂的逐步引导，课程学习确保了模型在面对更高阶的工具时不会感到过于复杂或困惑。模型可以基于以前学过的内容逐步建立对新任务的理解。</li>
    </ul>
  </li>
</ul>

<h2 id="应用与实验">应用与实验</h2>

<p>我们旨在探索工具学习的应用，并调查最先进基础模型在使用工具方面的有效性和局限性。</p>

<p><img src="/assets/posts_assets/Pasted%20image%2020241127132435.png" alt="" /></p>

<p>“# APIs”表示每个工具对应的 API 数量。测试集是指我们在进行实验时使用的数据集。我们展示了三种设置的结果，即无工具、零-shot、少-shot。Text-davinci-003 的结果以白色背景显示，而 ChatGPT 的结果则以青色背景显示。</p>

<p>结论：</p>

<ol>
  <li>在大多数情况下，模型可以通过简单的提示有效地学习如何使用工具，并提高其任务表现。</li>
  <li>次优会产生负影响：对于模型可以利用其内部知识解决的任务（例如计算器和搜索引擎的案例），使用零-shot 提示的工具有时会导致更差的性能，这意味着非最佳的工具使用可能会对表现产生负面影响。</li>
  <li>工具有用：结合少量提示的工具仍然始终能带来优于未使用工具的表现。这强调了工具在问题解决中所能带来的具体好处，前提是它们被有效地使用。</li>
  <li>比较 ChatGPT 和 text-davinci-003 的表现，我们观察到，尽管 ChatGPT 经过了 RLHF 的微调，但其结果并不优于 text-davinci-003。
    <ul>
      <li>原因猜测：首先，即 Ouyang 等人（2022）提到的 alignment tax 问题，即在 RLHF 训练过程中，特定任务技能和上下文学习能力受到损害；</li>
      <li>猜测Chatgpt 规模比 text-davinci-003 小</li>
    </ul>
  </li>
  <li>工具使用效果：检查 API 调用的成功率
    <ol>
      <li>我们观察到在少量提示设置下，某些工具如地图、天气、幻灯片、表格、烹饪助手和 AI 绘画显示出令人满意的完成率。这些工具被认为比其他工具更容易。实际上，我们通过实证发现，无论是 ChatGPT 还是 text-davinci-003 都能熟练地使用这些工具，尽管没有直接针对这些工具进行微调。</li>
      <li>对于一些工具如知识图谱、维基百科、在线购物和 3 D 模型构建，即使在少量提示下，模型的表现仍然远未令人满意。其原因可能是这些工具的使用不能通过少量示例轻易学习。例如，要求生成可执行代码作为 API 参数的工具，如知识图谱工具中的 search_by_query API（详见附录A.9），被发现明显更为困难。这意味着有必要训练基础模型使用工具。</li>
    </ol>
  </li>
</ol>

<h2 id="讨论">讨论</h2>

<ol>
  <li>安全问题</li>
  <li>复杂系统中工具学习</li>
  <li>自主工具创造</li>
  <li>从一般智能到个性化智能</li>
  <li>工具学习和具身智能</li>
  <li>整合工具后的知识冲突（类似 RAG 的知识冲突）</li>
</ol>

<h3 id="知识冲突">知识冲突</h3>

<h4 id="1-模型知识与增强知识之间的冲突">1. <strong>模型知识与增强知识之间的冲突</strong></h4>

<p>这种冲突是指模型本身的知识和通过外部工具增强的知识之间出现的不一致，主要由以下三种原因导致：</p>

<ul>
  <li>
    <p><strong>模型知识过时</strong>：基础模型（如语言模型）通常在训练过程中使用的是某个时间点的数据，而这些数据在模型训练后并不常更新。因此，模型所拥有的知识可能已经过时。而大多数增强工具（如搜索引擎或实时数据工具）能够提供最新的知识，这种新知识可能与模型中的旧知识发生冲突。</p>
  </li>
  <li>
    <p><strong>训练数据质量不高</strong>：模型的训练数据可能并没有经过严格筛选，甚至可能包含错误的知识，比如人类的误解或错误的信仰。增强工具（如维基百科等可靠的来源）提供的知识可能会将这些错误信息放大，导致模型和工具之间的知识冲突。</p>
  </li>
  <li>
    <p><strong>工具执行结果的误导性和偏差</strong>：增强工具的执行结果可能存在偏差或误导性，特别是在处理信息时，工具可能根据某些算法或数据的局限性提供不完全或不准确的答案。因此，用户必须谨慎判断知识来源的可信度，以避免错误信息的影响。</p>
  </li>
</ul>

<h4 id="2-不同工具之间的增强知识冲突">2. <strong>不同工具之间的增强知识冲突</strong></h4>

<p>在实际应用中，控制器（例如一个智能系统）可能会使用多个工具来获取更多全面和精准的知识。然而，不同工具返回的信息可能会因为以下几个原因而产生冲突：</p>

<ul>
  <li>
    <p><strong>工具的可信度差异</strong>：不同工具的可信度存在显著差异，某些工具可能在特定领域具有更高的可靠性。例如，<strong>Google Scholar</strong>在科学研究领域的准确性远高于某些不那么权威的资源。因此，如果使用多个工具，可能会得到不同质量的信息，造成知识冲突。</p>
  </li>
  <li>
    <p><strong>工具的偏见</strong>：不同工具可能包含不同的偏见，这会影响它们提供的信息。例如，一个新闻聚合器可能为了吸引读者点击而优先展示耸人听闻的头条新闻，而忽略了更准确的报道，导致对某些事件的片面解读。</p>
  </li>
  <li>
    <p><strong>相同功能工具的不同表现</strong>：即使是同类工具，因其内部算法和实现方式的不同，可能会得出不同的结论。例如，<strong>Bing Translator</strong>和<strong>Google Translator</strong>对于同一文本的翻译结果可能有所不同，因为它们的翻译模型和数据处理方法不同。这种差异可能导致相互矛盾的信息出现。</p>
  </li>
</ul>]]></content><author><name>Xingjie Gao</name><email>xingjie-gao@outlook.com</email></author><category term="nlp" /><category term="自然语言处理" /><category term="论文笔记" /><summary type="html"><![CDATA[Tool Learning with Foundation Models 论文笔记]]></summary></entry><entry><title type="html">RAG-DDR论文笔记</title><link href="http://localhost:4000/2024/11/12/RAG-DDR%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%98%85%E8%AF%BB.html" rel="alternate" type="text/html" title="RAG-DDR论文笔记" /><published>2024-11-12T00:00:00+08:00</published><updated>2024-11-12T00:00:00+08:00</updated><id>http://localhost:4000/2024/11/12/RAG-DDR%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E2%80%94%E9%98%85%E8%AF%BB</id><content type="html" xml:base="http://localhost:4000/2024/11/12/RAG-DDR%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%98%85%E8%AF%BB.html"><![CDATA[<h1 id="rag-ddr-optimizing-retrieval-augmented--generation-using-differentiable-data-rewards-论文笔记">RAG-DDR: OPTIMIZING RETRIEVAL-AUGMENTED  GENERATION USING DIFFERENTIABLE DATA REWARDS 论文笔记</h1>

<!---more-->

<blockquote>
  <p>论文来源: 师兄给的</p>

</blockquote>

<h2 id="不懂的问题">不懂的问题</h2>

<h3 id="什么是-agent">什么是 agent？</h3>

<p>文中的“agent”指的是 RAG 系统中负责特定任务的独立模块或组件。在 RAG-DDR 框架中，agent 通常指的是系统中用于检索和生成的两个模块，分别为“检索代理（Retriever Agent）”和“生成代理（Generator Agent）”。</p>

<ul>
  <li>
    <p><strong>检索代理（Retriever Agent）</strong>：负责根据用户查询从外部知识库或文档集合中检索相关信息。这个模块的任务是找到能够为生成模块提供有用背景知识的文档或数据。</p>
  </li>
  <li>
    <p><strong>生成代理（Generator Agent）</strong>：接收检索到的信息，并基于这些内容生成最终的输出（如回答或响应）。生成代理的主要目标是综合这些信息生成符合用户需求的文本。</p>
  </li>
</ul>

<p>在 DDR 方法中，两个 agent 之间相互配合，通过数据偏好的对齐来提升整体性能。DDR 通过奖励机制，让每个 agent 在决策时不仅考虑自身的需求，还要确保生成的输出在系统层面上能为另一个 agent（即对方模块）提供更有利的信息支持。这种协同优化使得系统的检索和生成流程更加协调，提升了生成内容的准确性和一致性。</p>

<h3 id="什么是agent的数据偏好">什么是agent的数据偏好？</h3>

<p>文中提到的“数据偏好”指的是 RAG 系统中不同模块（如检索模块和生成模块）对数据需求的特定倾向。简单来说，不同模块在处理任务时所需要的信息可能存在差异，因此它们在数据选择和使用上具有不同的“偏好”。</p>

<p>在检索增强生成系统中，检索模块的主要任务是从外部资源中找到与用户查询相关的文档或信息，但这些检索到的内容并不一定都适合生成模块。例如，生成模块通常需要的是能够直接回答用户问题的核心信息，而不是一堆冗余或含糊的信息。因此，如果检索模块提供的信息不符合生成模块的需求，可能会导致生成质量下降，出现内容不准确、混淆或所谓的“幻觉”现象。</p>

<p>DDR 方法通过奖励机制，使得系统能够在训练过程中调整检索和生成模块的数据偏好，逐渐对齐它们的需求。通过这种数据偏好对齐，检索模块可以更有针对性地提供生成模块需要的核心信息，而生成模块也能够更有效地利用这些检索内容，从而提升系统整体的生成效果。</p>

<h3 id="dpo-和-sft-有什么本质的区别">DPO 和 SFT 有什么本质的区别?</h3>

<p>使用的数据不一样</p>

<h3 id="对比学习是什么这里的对比学习和经典的有什么区别与联系">对比学习是什么？这里的对比学习和经典的有什么区别与联系</h3>

<h4 id="you">You:</h4>

<p>对比学习不是要最大化正对（来自同一样本的实例）之间的一致性，并最小化负对（来自不同样本的实例）之间的一致性吗？这里不是吗？</p>

<h4 id="chatgpt">ChatGPT:</h4>

<p>你提到的对比学习方法确实是在许多情况下的典型用法，即最大化“正对”（positive pairs）的相似性（通常来自同一样本的不同实例）并最小化“负对”（negative pairs）的相似性（来自不同样本的实例）。这种方式常用于无监督或自监督学习，例如图像表示学习，目标是让来自同一个实例的变体（如不同的图像增强）更相似，而来自不同实例的变体更不相似。</p>

<p>然而，在DDR（Differentiable Data Reward）方法中，所采用的对比学习方式稍有不同。这里的目标不是简单地比较不同样本的相似性，而是对比<strong>同一输入下不同生成输出的质量</strong>。这种“对比”方式更符合排序学习或“对比奖励学习”（contrastive reward learning）的框架，而不完全是传统的对比学习范式。具体来说：</p>

<ol>
  <li>
    <p><strong>同一输入的不同输出</strong>：在DDR中，对于给定的输入样本 x ，生成模型 V_t 会产生多个输出。这些输出中包括质量较高的输出（正样本， $\tilde{y}_t^+$ ）和质量较低的输出（负样本， $\tilde{y}_t^-$ ）。目标是让生成模型更倾向于生成质量更高的输出，而不是简单地增加同一输入样本的不同实例的一致性。</p>
  </li>
  <li>
    <p><strong>奖励差值作为对比标准</strong>：在这里，正样本和负样本的定义是基于奖励值，而不是基于是否属于同一实例。正样本 $\tilde{y}_t^+$ 是指生成质量较高的输出，而负样本 $\tilde{y}_t^-$ 是指生成质量较低的输出。因此，DDR中的对比学习本质上是对比<strong>同一输入的不同生成输出的奖励</strong>，而不是对比不同样本的实例。</p>
  </li>
  <li>
    <p><strong>目标是优化输出质量而非实例一致性</strong>：传统对比学习的目的是让模型学会在实例级别上的一致性，而DDR的目标是让生成系统学会优先选择高质量的输出。因此，这里的对比学习实际上是将同一输入的输出按质量排序，确保高奖励的输出更有可能被生成。这个过程可以被理解为一种奖励驱动的对比学习，但目标是优化输出质量而不是实例相似性。</p>
  </li>
</ol>

<h5 id="总结">总结</h5>

<p>虽然DDR的损失函数也有“对比”结构，但它与传统对比学习不同，主要关注的是在同一输入下的<strong>输出质量对比</strong>。DDR使用的对比损失是为了优化生成模型，使其更倾向于生成高奖励的输出，而不是仅仅在实例层面上对不同样本进行区分。</p>

<h4 id="you-1">You:</h4>

<p>这里对比学习的核心思想是什么？</p>

<h4 id="chatgpt-1">ChatGPT:</h4>

<p>在DDR（Differentiable Data Reward）方法中，对比学习的核心思想是<strong>基于奖励信号对同一输入的不同输出进行排序，使得模型更倾向于生成高质量（高奖励）的输出</strong>。具体来说，这种对比学习的核心思想包括以下几点：</p>

<ol>
  <li>
    <p><strong>区分高质量输出和低质量输出</strong>：对于每一个输入样本 x ，生成模型 V_t 会生成多个候选输出，这些输出的质量可能不同。通过奖励机制，DDR对每个输出的质量进行评分，其中高质量输出（正样本， $\tilde{y}_t^+$ )和低质量输出（负样本， $\tilde{y}_t^-$ )被区分开来。DDR的目标是让模型能够识别并倾向于生成质量更高的输出。</p>
  </li>
  <li>
    <p><strong>通过对比损失优化生成策略</strong>：DDR利用对比损失来优化生成模型的输出选择策略。具体地，通过比较正样本和负样本的奖励差值，DDR最大化生成模型在生成高奖励输出时的概率。这种对比损失使得模型更倾向于生成正样本，并减少生成负样本的概率。</p>
  </li>
  <li>
    <p><strong>以相对质量为优化目标，而非绝对奖励</strong>：DDR不直接优化绝对的奖励值，而是通过奖励差值来进行对比优化。这种方式的好处是，模型学习的是生成高质量输出的相对偏好，而不依赖于奖励的绝对值大小。只要正样本的奖励高于负样本，模型就会倾向于生成正样本。这种相对质量的优化目标增强了模型的鲁棒性和泛化性。</p>
  </li>
  <li>
    <p><strong>对比学习的排序机制</strong>：DDR通过排序机制来指导模型的学习。具体来说，对于同一输入样本，模型生成的输出被分为高奖励和低奖励的类别，模型的目标是“排序”这些输出，使得高奖励的输出排在前面。这种排序学习的机制可以帮助模型学习到哪些输出更符合优质标准，从而更好地满足生成任务的需求。</p>
  </li>
</ol>

<h5 id="总结-1">总结</h5>

<p>DDR中的对比学习核心思想是<strong>利用奖励信号对输出进行排序优化</strong>，使得模型能够根据输出质量的差异更好地进行选择。通过最大化高奖励输出的概率，DDR能够逐步提高模型生成高质量输出的能力，从而优化整个生成系统的表现。这种基于奖励差异的对比学习方式，使得DDR不仅关注输出的质量排序，还可以动态调整生成策略，以实现更优的生成效果。</p>

<h4 id="you-2">You:</h4>

<p>和之前的对比学习有什么共通之处</p>

<h4 id="chatgpt-2">ChatGPT:</h4>

<p>DDR中的对比学习虽然在应用方式上不同，但它与传统的对比学习方法仍然具有一些共通之处，主要体现在<strong>相对差异的优化</strong>和<strong>结构化的学习信号</strong>两个方面：</p>

<h5 id="1-相对差异的优化">1. <strong>相对差异的优化</strong></h5>

<ul>
  <li>
    <p><strong>共通之处</strong>：传统对比学习和DDR的核心思想都围绕“相对差异”展开。传统对比学习的目标是拉近相似样本（正对）并拉远不相似样本（负对），即最大化正对的相似性、最小化负对的相似性。DDR中的对比学习也是如此：它希望最大化正样本（高质量输出）的概率、最小化负样本（低质量输出）的概率。这种优化方式在两个任务中都依赖于“对比”来定义学习目标，而不是直接依赖单独的数据点。</p>
  </li>
  <li>
    <p><strong>重要性</strong>：这种基于差异的优化让模型能够在无明确标签的情况下学会区分样本的优劣，适合无监督和弱监督任务，也更有助于泛化，因为模型学到的是相对偏好，而不是绝对标准。</p>
  </li>
</ul>

<h5 id="2-结构化的学习信号">2. <strong>结构化的学习信号</strong></h5>

<ul>
  <li>
    <p><strong>共通之处</strong>：无论是传统对比学习还是DDR，模型都是通过结构化的对比信号进行学习的。传统对比学习使用“正对”和“负对”来提供学习信号，使模型学习到哪些样本更相似，哪些更不相似。DDR同样提供了一个结构化的学习信号，即高质量输出（正样本）和低质量输出（负样本）的奖励差异。模型通过这种信号学会偏向于生成高质量的输出，而抑制低质量输出的概率。</p>
  </li>
  <li>
    <p><strong>重要性</strong>：结构化的信号提供了一个明确的优化目标，使模型在训练过程中可以更有效地获得反馈。这种反馈不仅帮助模型学会区分正负样本，还能在优化过程中不断提升表现。</p>
  </li>
</ul>

<h5 id="3-对比损失的使用">3. <strong>对比损失的使用</strong></h5>

<ul>
  <li>
    <p><strong>共通之处</strong>：两者都使用了对比损失函数来实现优化。传统对比学习通常采用基于相似度的对比损失（如对比损失函数或Triplet Loss），而DDR中使用了基于奖励差异的对比损失。DDR通过奖励差异构建对比损失，确保高质量输出的生成概率更高。这种对比损失的使用让模型在每个训练步骤中有了明确的优化方向。</p>
  </li>
  <li>
    <p><strong>重要性</strong>：对比损失让模型可以学习到更细粒度的区分能力，而不仅仅是简单地提高生成概率或回归奖励值。这使得模型在不同质量的输出之间具备排序能力，从而提高生成任务的质量。</p>
  </li>
</ul>

<h5 id="4-梯度驱动的优化">4. <strong>梯度驱动的优化</strong></h5>

<ul>
  <li>
    <p><strong>共通之处</strong>：两种方法的优化都是基于梯度驱动的。传统对比学习通过反向传播更新模型参数，使正对的距离更近、负对的距离更远。DDR也是通过反向传播，让高质量输出的生成概率在优化过程中得到提升，而低质量输出的概率降低。梯度驱动的优化可以在训练过程中逐步调整模型参数，直至收敛。</p>
  </li>
  <li>
    <p><strong>重要性</strong>：梯度驱动的优化方式让模型能够在大规模数据上高效地进行学习，并在训练结束时收敛到一个稳定的生成策略，保证输出的高质量。</p>
  </li>
</ul>

<h5 id="总结-2">总结</h5>

<p>传统对比学习与DDR中的对比学习的共通之处在于<strong>对相对差异的强调</strong>、<strong>结构化学习信号的利用</strong>、<strong>对比损失的使用</strong>和<strong>梯度驱动的优化</strong>。两者都通过对比正负样本的不同，以实现特定任务的学习目标，尽管应用场景不同，但它们都旨在通过对比来提升模型对目标的区分能力。这些共通之处是对比学习方法的核心思想，无论在传统应用中还是在奖励驱动的生成优化中，都能为模型提供有效的学习信号和优化路径。</p>

<h3 id="zero-shot-model-是什么">Zero-shot Model 是什么</h3>

<h2 id="摘要">摘要</h2>

<blockquote>
  <p>什么是agent？
什么是agent的数据偏好？</p>
</blockquote>

<h3 id="问题">问题</h3>

<p>SFT使得LLMs处理不同指导下的各种RAG任务。然而，它训练RAG模块过度拟合训练信号，并忽略了RAG系统内各个代理之间不同的数据偏好。</p>
<h3 id="解决办法">解决办法</h3>

<p>Differentiable Data Reward method (DDR) 通过调整不同RAG模块之间的数据偏好，从而端到端地训练RAG系统。DDR通过使用回滚方法收集奖励来优化每个代理。</p>

<ol>
  <li>DDR 采用一种回滚方法，让每个代理对响应进行采样并加入一些细微变化（扰动），然后通过这些响应对整体系统性能的影响来评估其质量。该奖励信号促使代理生成更有助于提升系统最终输出质量的响应。</li>
</ol>

<h3 id="实验结果">实验结果</h3>

<ol>
  <li>DDR方法在性能上明显优于SFT方法，特别是对于更依赖于检索知识的参数较小规模的LLMs。</li>
  <li>DDR方法还展现出更强的能力来调整RAG模块之间的数据偏好。</li>
  <li>DDR方法使得生成模块更有效地从文档中提取关键信息，并缓解了参数化记忆与外部知识之间的冲突</li>
</ol>

<h2 id="引言">引言</h2>

<h3 id="llm现状">LLM现状</h3>

<ol>
  <li>LLM 的广泛应用</li>
  <li>优势：LLM 由于幻觉问题，通常会产生错误的响应，因此采用了检索增强生成（RAG）来增强 LLMs 的能力</li>
  <li>问题：<strong>检索知识和参数化存储之间的冲突通常会误导 LLMs，挑战 RAG 系统的有效性</strong></li>
</ol>

<h3 id="rag-现状">RAG 现状</h3>

<p>研究方向一：</p>

<ol>
  <li>研究方向：训练各种 agent 提高检索的准确性</li>
  <li>例子：查询重构，重新排序候选文档、总结检索文档、执行额外的检索步骤</li>
  <li>训练方法：EM、构建指令微调数据集然后 SFT</li>
  <li>缺点：这些基于 SFT 的方法通常会训练 LLM 过度拟合训练信号，并面临灾难性遗忘问题</li>
</ol>

<p>研究方向二：</p>

<ol>
  <li>研究方向：优化这些 RAG 模块，调整它们的数据偏好（检索器+生成器）</li>
  <li>例子：这些系统只训练检索器，以提供更准确的文件以满足生成器的数据偏好，通过对检索到的知识进行优化，调整检索器和生成器之间的数据偏好是改进 RAG 模型效果的一种直接方法。</li>
  <li>缺点：生成器仍然面临知识冲突，使得 LLMs 在生成过程中没有有效地利用检索到的知识（Xie 等，2024）。</li>
  <li>解决：因此，在训练过程中利用整个系统的奖励来优化每个 RAG 模块对于构建更加定制化的 RAG 系统是至关重要的。</li>
</ol>

<h3 id="本文工作ddr-方法">本文工作：DDR 方法</h3>

<ol>
  <li>作用：使 DPO 方法在 RAG 系统中可以端到端地训练</li>
  <li>内容：DDR 使用一个滚动方法（Kocsis 和 Szepesv 2006 年）来收集每个代理的整个系统的奖励，并根据奖励优化代理。</li>
  <li>具体流程：
    <ol>
      <li>构建了一个典型的 RAG 系统：
        <ol>
          <li>知识细化模块，用于选择检索到的文档，</li>
          <li>基于查询和精炼知识生成响应的模块。</li>
        </ol>
      </li>
      <li>使用 DDR 优化 RAG
        <ul>
          <li>训练中：使用整个 RAG 系统的奖励，优化生成和知识细化模块，以使数据偏好在两个代理之间对齐。</li>
        </ul>
      </li>
    </ol>
  </li>
</ol>

<h3 id="实验结果-1">实验结果</h3>

<ul>
  <li>可微分数据奖励（DDR）胜过所有基线模型，在一系列知识密集型任务中实现了对先前方法（Lin 等人，2023 年）的显著改进。</li>
  <li>RAG-DDR 模型的有效性主要源自生成模块，该模块受到了来自 RAG 系统的奖励的优化。</li>
  <li>减轻了知识冲突：DDR 优化生成模块更有效地捕获了从检索的文档中获得的重要信息，并减轻了外部知识和参数式记忆之间的知识冲突。</li>
  <li>鲁棒性：即使在响应生成过程中加入了额外的嘈杂文档，DDR 优化的 RAG 系统的有效性也可以得到推广。</li>
</ul>

<h2 id="相关工作">相关工作</h2>

<ul>
  <li>RAG 缺点是会检索到噪声</li>
  <li>原因：LLMs的参数知识与外部知识之间的知识冲突</li>
  <li>当前的研究方向：
    <ul>
      <li>专注于构建模块化的RAG管道，以提高检索文档的质量</li>
      <li>训练LLMs学习如何按需检索和利用知识，通过自我反思</li>
    </ul>
  </li>
</ul>

<h3 id="optimizing-rag-方法">Optimizing RAG 方法</h3>

<ul>
  <li>并采用端到端的训练方法</li>
  <li>将检索决策视为潜变量，然后迭代优化检索器和生成器以适应黄金答案。</li>
</ul>

<h3 id="优化-llm-的方法">优化 LLM 的方法</h3>

<ul>
  <li>通过设计无监督预训练方法，使 LLM 具备上下文去噪能力，以教会 LLM 从检索的上下文中提炼信息。</li>
  <li>具体：构建了一个监督训练数据集，然后通过指令调整优化检索器和 LLM。</li>
  <li>缺点：然而，这些训练方法专注于训练 LLM 以适应训练信号，并在指令调整期间面临灾难性遗忘的问题（模型在适应新任务的过程中会遗忘之前学到的信息）</li>
</ul>

<h3 id="强化学习的方法">强化学习的方法</h3>

<ul>
  <li>Agent Q 集成了蒙特卡罗树搜索（MCTS）和 DPO，使得代理可以从成功和失败的轨迹中学习，从而提高其在复杂推理任务中的表现（Putta et al., 2024）。</li>
  <li>STEP-DPO 进一步考虑将复杂任务的每个推理步骤优化作为偏好学习的基本单元，从而增强 LLM 的长链推理能力（Lai et al., 2024）。</li>
</ul>

<p>缺点：</p>

<ul>
  <li>虽然这些模型主要针对优化单个代理以提高每一步的响应准确性，但它们并没有关注多代理系统中数据对齐的有效性。</li>
</ul>

<p>本文的方法：</p>

<p>与使用 SFT 方法进行 RAG 优化（Lin et al., 2023）不同，本文侧重于使用 DPO 方法，以避免训练信号过拟合，并在不同代理之间对齐数据偏好，这与上述基于 RL 的优化方法有所不同。</p>

<h2 id="ddr-方法--使用-ddr-的-rag-系统">DDR 方法 &amp; 使用 DDR 的 RAG 系统</h2>

<h3 id="ddr-方法介绍">DDR 方法介绍</h3>

<p>在 RAG 系统中，$V = {V_1, \dots , V_t, \dots, V_T}$，代理之间交换和传递数据。</p>

<p>为了优化这个系统:</p>

<ol>
  <li>首先在代理之间前向传播数据，然后评估 RAG 系统的性能。</li>
  <li>接着，我们向后传播奖励，以细化每个代理的数据偏好。</li>
</ol>

<h4 id="前向传播">前向传播</h4>

\[x\leadsto V_{1}\ldots V_{t}\xrightarrow{y_{t}}V_{t+1}\ldots V_{T-1}\xrightarrow{y_{T-1}}V_{T}\leadsto y_{T},\]

<ul>
  <li>代理 $V_t$ 从代理 $V_{t−1}$ 接收数据，同时将数据传递给代理 $V_{t+1}$</li>
  <li>代理系统 V 的性能可以通过计算代理系统 V 的最终输出 $V_T$ 的质量评分 $S_{(yT)}$ 来评估。</li>
</ul>

<h4 id="反向传播">反向传播</h4>

\[\tilde{y}_{t}\leadsto V_{t+1}\xrightarrow{y_{t+1}}V_{t+2}\cdots V_{T-1}\xrightarrow{y_{T-1}}V_{T}\leadsto y_{T},\quad r(x,\tilde{y}_{t})=S(y_{T}).\]

<ul>
  <li>思路：我们首先指示 $V_t$ 采样多个输出 $\tilde{y}<em>{t}$，这些输出对代理系统进行了某些扰动。然后我们通过一次回滚过程计算奖励  $r(x,\tilde{y}</em>{t})$</li>
  <li>具体：我们将代理 $V_{t+1:T}$ 视为评估模型，将 $\tilde{y}<em>t$ 输入到该子系统 $V</em>{t+1:T}$ 中，并计算生成输出 $y_T$ 的评估得分 S (yT)</li>
</ul>

<p>然后定义正样本胜出的概率为两个样本奖励的差值：</p>

\[P(\tilde{y}_{t}^{+}&gt;\tilde{y}_{t}^{-}|x)=\sigma(r(x,\tilde{y}_{t}^{+})-r(x,\tilde{y}_{t}^{-}))\]

<p>然后用 DPO 训 $V_t$ 损失函数定义：</p>

<p>\(\huge 
\mathcal{L}(V_{t};V_{t}^{\mathrm{ref}})=-\mathbb{E}_{(x,\bar{y}_{t}^{+},\bar{y}_{t}^{-})\sim\mathcal{D}}[\log\sigma(\beta\log\frac{V_{t}(\tilde{y}_{t}^{+}\mid x)}{V_{t}^{\mathrm{ref}}(\tilde{y}_{t}^{+}\mid x)}-\beta\log\frac{V_{t}(\tilde{y}_{t}^{-}\mid x)}{V_{t}^{\mathrm{ref}}(\tilde{y}_{t}^{-}\mid x)})]\)</p>
<h4 id="为什么引入参考模型呢">为什么引入参考模型呢？</h4>

<p>在 DDR 的优化过程中，引入参考模型 $V_t^{\text{ref}}$ 是一种有效的策略，目的是为当前的生成模型 $V_t$ 提供一个<strong>稳定的基准</strong>。参考模型的作用可以从以下几个方面来理解：</p>

<ol>
  <li>
    <p><strong>稳定性和优化参考</strong></p>

    <ul>
      <li>引入参考模型可以提供一种<strong>相对稳定的优化基准</strong>，帮助生成模型 $V_t$ 不断改进而不偏离目标。由于生成模型 $V_t$ 在训练过程中会不断更新参数，如果没有一个固定的基准，模型可能会在优化过程中变得不稳定或出现“漂移”现象（即逐渐偏离优化目标）。</li>
      <li>通过与参考模型的对比，生成模型可以有一个“对比对象”，从而获得清晰的改进方向。这种相对优化（relative optimization）可以避免模型因参数调整而产生的震荡，让训练更加平滑。</li>
    </ul>
  </li>
  <li>
    <p><strong>鼓励生成更优质的输出</strong></p>

    <ul>
      <li>参考模型 $V_t^{\text{ref}}$ 通常是训练初期的模型版本或之前训练好的模型版本。相比于完全随机的初始模型，参考模型通常已经有一定的生成能力。通过对比当前模型与参考模型的输出概率，DDR 的损失函数可以确保当前模型始终朝着“比参考模型更优质”的方向优化。</li>
      <li>换句话说，参考模型的引入让生成模型必须“超越”参考模型，以便获得更高的奖励，从而逐渐提升生成质量。这种机制鼓励模型在每一轮迭代中生成更高质量的输出，而不是简单地维持现有的生成水平。</li>
    </ul>
  </li>
  <li>
    <p><strong>降低奖励信号的噪声影响</strong></p>

    <ul>
      <li>直接基于生成输出的奖励信号进行优化可能会引入较大的噪声，特别是在生成任务中的奖励信号波动较大的情况下。通过将生成模型的输出与参考模型的输出进行对比，DDR 的损失函数实际上引入了<strong>相对奖励</strong>，即两者之间的奖励差值。</li>
      <li>这种相对奖励（relative reward）减少了绝对奖励的波动带来的影响。因为即使绝对奖励值有波动，只要参考模型与生成模型的输出之间有相对差异，DDR 的损失函数仍能指导模型朝正确的方向优化。</li>
    </ul>
  </li>
  <li>
    <p><strong>实现对比学习的对比目标</strong></p>

    <ul>
      <li>在 DDR 的对比损失中，参考模型 $V_t^{\text{ref}}$ 是对比学习的“负对比对象”。通过将生成模型与参考模型的输出进行对比，损失函数可以计算两者之间的奖励差值，并优化生成模型，使其更倾向于生成高奖励的输出。这种对比机制帮助生成模型更好地学习“高质量”输出的特征。</li>
      <li>参考模型提供了一个固定的负样本，使得对比学习有了稳定的参照点。生成模型通过不断与参考模型对比，学习如何生成比参考模型更好的输出，进而实现对比学习的目标。</li>
    </ul>
  </li>
  <li>
    <p><strong>防止模型退化</strong></p>

    <ul>
      <li>在生成任务中，如果模型只是单纯地优化奖励信号，有时可能会发生“退化”现象，例如模型可能会陷入生成简单或低质量输出的局部最优。引入参考模型可以起到一个“门槛”作用，确保生成模型的输出质量不会退化到参考模型之下，避免训练过程中的质量下降。</li>
      <li>通过对比生成模型和参考模型的输出概率，损失函数可以确保生成模型始终保持甚至超越参考模型的性能，从而防止模型性能的退化。</li>
    </ul>
  </li>
</ol>

<p>总结</p>

<p>参考模型 $V_t^{\text{ref}}$ 的引入为 DDR 中的生成模型提供了一个稳定的优化基准，使得生成模型在训练过程中更加稳定，并且可以持续改进。参考模型的对比目标确保生成模型的输出质量逐步提升，同时减少了奖励信号噪声对优化的影响，防止模型退化。</p>

<h3 id="使用-ddr-优化的-rag-系统">使用 DDR 优化的 RAG 系统</h3>

<p><img src="/assets/posts_assets/Pasted%20image%2020241115145914.png" alt="" /></p>

<h4 id="模型结构">模型结构</h4>

<ol>
  <li>给定查询 q 和一组检索到的文档 $D = {d_1, . . . , d_n}$，</li>
  <li>通过使用知识精炼模块 ($V_{KR}$) 来过滤不相关的文档</li>
  <li>生成模块 ($V_{Gen}$) 来产生响应</li>
</ol>

<p>用数学表示：</p>

\[\{q,D\}\leadsto V_{\mathrm{KR}}\xrightarrow{\{q,\tilde{D}\}}V_{\mathrm{Gen}}\leadsto y_{\mathrm{Gen}},\]

<ul>
  <li>其中 D ̃ ⊆ D，VKR 生成选择操作以过滤出 D 中的噪声文档，以构建 D ̃。</li>
  <li>VGen 根据查询 q 和过滤后的文档 D ̃ 生成答案。如所示。</li>
</ul>

<h4 id="使用-ddr-训练-v_kr">使用 DDR 训练 $V_{KR}$</h4>

<p>模块的功能：将查询 q 和文档 $d_i$ 输入到 $V_{KR}$，并请求它生成动作 $y^i_{KR} \in {“YES”, “NO”}$，这表明文档 $d_i$ 是否被保留（$y^i_{KR} = “YES”$）或被丢弃（$y^i_{KR} = “NO”$）</p>

<p>数学表示：</p>

\[y_{\mathrm{KR}}^{i}=\mathrm{LLM}(\mathrm{Instruct}_{\mathrm{KR}},d_{i}\oplus q),\]

<p>模块的目标：</p>

<p>当文档 $d_i$ 与 query 无关时，最大化无关的概率</p>

\[P(y_{\mathrm{KR}}^{j} =\mathrm{“NO”}&gt;y_{\mathrm{KR}}^{j}=\mathrm{“YES”}|q,d_{j})\]

<p>当文档 $d_i$ 与 query 有关时，最大化有关的概率</p>

\[P(y_{\mathrm{KR}}^{j} =\mathrm{“YES”}&gt;y_{\mathrm{KR}}^{j}=\mathrm{“NO”}|q,d_{j})\]

<h4 id="用-ddr-训练-v_gen">用 DDR 训练 $V_{Gen}$</h4>

<p>生成模块可以通过两种不同的模式来生成回答：</p>

<h5 id="模式-1基于精炼文档集合和查询生成回答">模式 1：基于精炼文档集合和查询生成回答</h5>

<p>在这个模式下，生成模块会使用精炼后的文档集合和查询的拼接作为输入，通过一个大语言模型（LLM）和生成指令 $\text{Instruct}_{Gen}$ 来生成回答。公式如下：</p>

\[\tilde{y}_{Gen} \sim \text{LLM}(\text{Instruct}_{Gen}, d_1 \oplus d_2 \oplus \cdots \oplus d_k \oplus q)\]

<p>其中，符号 $\oplus$ 表示拼接操作。拼接后的输入包含所有精炼后的文档和查询，这样模型可以在生成回答时综合考虑所有相关信息。</p>

<ul>
  <li><strong>优点</strong>：这种方式可以充分利用精炼后的文档集合，为回答提供更丰富的上下文，有助于模型生成更详细和准确的回答。</li>
  <li><strong>潜在缺点</strong>：如果精炼文档集合中包含一些误导性的信息（即使经过筛选可能仍然存在少量不相关信息），则生成的回答可能会受到影响。</li>
</ul>

<h5 id="模式-2仅基于查询生成回答">模式 2：仅基于查询生成回答</h5>

<p>在这个模式下，生成模块只使用查询 $q$ 作为输入，而不使用精炼文档集合。这种方式有时被用来避免精炼文档集合中的噪声或误导性信息。公式如下：</p>

\[\tilde{y}_{Gen} \sim \text{LLM}(\text{Instruct}_{Gen}, q)\]

<p>通过只使用查询生成回答，模型可以减少来自精炼文档集合的潜在干扰，直接基于自身的知识库或语言模型的生成能力来回答问题。</p>

<ul>
  <li><strong>优点</strong>：这种方式避免了文档集合中可能存在的噪声影响，对于简单或模型已有知识库覆盖的问题，可能产生更直接的回答。</li>
  <li><strong>潜在缺点</strong>：若查询涉及复杂问题或需要更详细的上下文信息，单纯使用查询生成的回答可能不够准确或不够全面。</li>
</ul>

<h5 id="模块优化目标">模块优化目标</h5>

<p>生成模块的优化目标是<strong>生成高质量的回答</strong>，即在给定的输入（精炼文档集合和/或查询）下，生成的回答能够获得最高的质量得分 $S (\tilde{y}_{Gen})$。</p>

<ul>
  <li><strong>正样本和负样本</strong>：在训练过程中，生成的回答会根据评价得分被标记为正样本（高质量回答）或负样本（低质量回答）。高质量回答（得分较高）被标记为正样本 $\tilde{y}<em>{Gen}^+$，低质量回答（得分较低）被标记为负样本 $\tilde{y}</em>{Gen}^-$。</li>
  <li><strong>优化策略</strong>：生成模块的优化目标是<strong>最大化生成正样本的概率</strong>，从而使模型更倾向于生成高质量的回答。这可以通过对比学习的方式来实现，即提高正样本的生成概率、降低负样本的生成概率。具体来说，生成模块通过最大化以下条件概率来优化生成过程：
\(P (\tilde{y}_{Gen}^+ | q, d_1, \ldots, d_k) &gt; P (\tilde{y}_{Gen}^- | q, d_1, \ldots, d_k)\)</li>
</ul>

<p>这种优化策略确保了模型在生成回答时，能够充分利用上下文信息和相关文档，从而生成更优质的答案。</p>

<h2 id="实验">实验</h2>

<h3 id="数据集">数据集</h3>

<ul>
  <li>统一使用 BGE-large 检索文档</li>
  <li>训练：我们收集了覆盖两个任务的十个数据集，开放领域问答和推理。具体而言，我们在实验中随机抽取了 32,805 个样本用于训练集，2,000 个样本用于开发集。</li>
  <li>评估：知识密集型任务进行评估，包括开放领域问答、多跳问答、槽填充和对话任务
    <ul>
      <li>开放领域问答：NQ、MARCO QA、TriviaQA</li>
      <li>多跳任务：HotpotQA、Wikipedia of Wizard (WoW)</li>
      <li>此外，我们还采用 T-REx (Elsahar et al., 2018) 来测量模型的一跳事实查找能力。</li>
    </ul>
  </li>
</ul>

<h3 id="评估标准">评估标准</h3>

<ul>
  <li>我们分别使用 Rouge-L 和 F 1 作为 MARCO QA 任务和 WoW 任务的评估指标</li>
  <li>对于其他任务，我们使用准确率。</li>
</ul>

<h3 id="baseline">Baseline</h3>

<p>在我们的实验中，我们将 DDR 与五种基线模型进行比较，包括零样本模型和监督微调模型。</p>

<p>这段文字和表格展示了在不同 RAG（Retrieval-Augmented Generation）模型上进行的一系列实验结果，评估了不同模型在多项任务上的性能，包括开放域问答（Open-Domain QA）、多跳问答（Multi-Hop QA）、填槽任务（Slot Filling）和对话任务（Dialogue）。</p>

<h4 id="实验设置">实验设置</h4>

<ol>
  <li><strong>模型与模块的选择</strong>：
    <ul>
      <li>实验中使用了不同规模的大语言模型（LLM）来执行知识精炼和生成任务，具体包括 Llama 3-8 B 和 MiniCPM-2.4 B。</li>
      <li>Llama 3-8 B 作为知识精炼模块使用，MiniCPM-2.4 B 则作为生成模块。</li>
    </ul>
  </li>
  <li><strong>对比方法</strong>：
    <ul>
      <li>实验分为几种不同的方法来对比评估：
        <ul>
          <li><strong>LLM w/o RAG</strong>：直接使用大语言模型（LLM）回答问题，而不使用任何检索增强（RAG）。</li>
          <li><strong>Vanilla RAG (2023)</strong>：标准的 RAG 方法，利用检索到的上下文信息并结合 LLM 生成答案。</li>
          <li><strong>REPLUG (2023)</strong>：一种替代的 RAG 方法，利用来自不同段落的概率加权输出进行集成。</li>
          <li><strong>RA-DIT (2023)</strong>：一种结合了上下文学习和 RAG 的方法，使用检索到的段落作为生成上下文。</li>
          <li><strong>RAG-DDR (w/ 1-Round)</strong>：使用 DDR 方法训练的 RAG 模型，进行了一轮优化。</li>
          <li><strong>RAG-DDR (w/ 2-Round)</strong>：使用 DDR 方法训练的 RAG 模型，进行了两轮优化。</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h4 id="实验结果与分析">实验结果与分析</h4>

<ol>
  <li><strong>表格数据解读</strong>：
    <ul>
      <li>表格展示了不同方法在各项任务上的表现，按任务类型进行组织，包括：
        <ul>
          <li><strong>开放域问答（NQ, TriviaQA, MARCO QA）</strong></li>
          <li><strong>多跳问答（HotpotQA）</strong></li>
          <li><strong>填槽任务（T-REx）</strong></li>
          <li><strong>对话任务（WoW）</strong></li>
        </ul>
      </li>
      <li>每个任务下给出了不同模型的得分，得分越高表示模型在该任务上的表现越好。</li>
      <li>表格中以粗体突出显示了每个任务上表现最好的方法，第二好的方法用斜体显示。</li>
    </ul>
  </li>
  <li><strong>结果观察</strong>：
    <ul>
      <li>使用 DDR 优化的 RAG 模型（RAG-DDR）在大多数任务中都达到了最佳或次佳的结果，特别是在进行了两轮优化的 RAG-DDR (w/ 2-Round) 上表现尤为出色。</li>
      <li>例如，在 Llama 3-8 B 模型的设置下，RAG-DDR (w/ 2-Round) 在几乎所有任务（包括 NQ、TriviaQA、MARCO QA、HotpotQA、T-REx 等）上都表现最优，显示了 DDR 在提高生成质量上的显著优势。</li>
      <li>RAG-DDR (w/ 2-Round) 的性能优于其他对比方法，如 Vanilla RAG、REPLUG 和 RA-DIT，说明多轮优化有助于进一步提升模型的回答质量。</li>
    </ul>
  </li>
  <li><strong>实验方法细节</strong>：
    <ul>
      <li>研究者将大语言模型视为黑箱，设计了三种基线方法（LLM w/o RAG、Vanilla RAG、REPLUG）进行对比。</li>
      <li><strong>LLM w/o RAG</strong>：直接将查询输入 LLM，以便模型依靠其记忆的知识生成回答。</li>
      <li><strong>Vanilla RAG</strong>：基于上下文学习的方法，利用检索的段落作为上下文，引导生成回答。</li>
      <li><strong>REPLUG</strong>：结合来自不同段落的概率输出进行集成。</li>
      <li><strong>RA-DIT</strong>：不微调生成模型，而是使用检索段落作为上下文。</li>
    </ul>
  </li>
</ol>

<h3 id="实验细节">实验细节</h3>

<p>这段文字提供了实验的<strong>实现细节</strong>，说明了用于构建生成模块和知识精炼模块的模型、参数设置以及训练方法。以下是详细的解释：</p>

<h4 id="模型选择">模型选择</h4>

<ol>
  <li><strong>生成模块</strong>：
    <ul>
      <li>生成模块使用了 MiniCPM-2.4 B-sft（Hu et al., 2024）和 Llama 3-8 B-Instruct（Touvron et al., 2023）作为基础模型。这些模型用于从输入（包括查询和相关文档）生成最终的回答。</li>
    </ul>
  </li>
  <li><strong>知识精炼模块</strong>：
    <ul>
      <li>知识精炼模块则使用了 Llama 3-8 B-Instruct（Touvron et al., 2023）来筛选和精炼检索到的文档，以确保生成模块得到的是相关且高质量的信息。</li>
    </ul>
  </li>
</ol>

<h4 id="训练过程与参数设置">训练过程与参数设置</h4>

<ol>
  <li><strong>奖励计算与优化</strong>：
    <ul>
      <li>在使用 DDR 训练 RAG 模型时，研究者使用了<strong>自动化评价指标</strong>（如 Rouge-L 和 Accuracy）来计算奖励值。这些指标帮助模型评估生成回答的质量，以便指导优化。</li>
      <li>参数 <strong>$\beta$</strong> 设置为 0.1。这个参数在损失函数中用于平衡正样本与负样本的对比关系。</li>
    </ul>
  </li>
  <li><strong>学习率与训练轮数</strong>：
    <ul>
      <li>学习率被设置为 <strong>5 e-5</strong>，即 0.00005。这是一个较小的学习率，有助于模型稳定训练，避免参数更新过快导致不稳定。</li>
      <li>每个模型训练 <strong>一个 epoch</strong>，即遍历一次整个训练数据集。这表明实验使用了相对较少的训练轮次。</li>
    </ul>
  </li>
  <li><strong>生成模块中的外部知识</strong>：
    <ul>
      <li>在生成模块中，模型会使用 5 个检索到的文档作为外部知识，用于增强生成过程。这些文档由知识精炼模块筛选出来，帮助生成模块更准确地回答问题。</li>
    </ul>
  </li>
  <li><strong>高效训练的优化方法</strong>：
    <ul>
      <li>为了提升训练效率，研究者采用了 <strong>LoRA（Low-Rank Adaptation）</strong> 方法（Hu et al., 2022）。LoRA 是一种轻量化的训练方法，通过对模型参数的低秩分解来降低计算成本，从而加速训练。LoRA 在这里用于优化知识精炼模块和生成模块，使得训练过程更加高效。</li>
    </ul>
  </li>
</ol>

<h4 id="总结-3">总结</h4>

<ul>
  <li>实验中使用了 MiniCPM-2.4 B 和 Llama 3-8 B 作为生成和知识精炼模块的模型基础。</li>
  <li>使用了 Rouge-L 和 Accuracy 等指标来计算奖励，学习率为 5 e-5，每个模型训练 1 个 epoch。</li>
  <li>生成模块通过 5 个检索到的文档来增强生成质量，使用 LoRA 方法来提升训练效率。</li>
  <li>这些设置和方法共同作用，使得模型可以在计算资源可控的情况下高效完成训练，并生成高质量的回答。</li>
</ul>

<h2 id="结果分析">结果分析</h2>

<p>第 5 部分<strong>EVALUATION RESULTS（评估结果）</strong>详细讨论了使用 DDR 训练的 RAG 模型在不同任务中的表现，并分析了其优缺点。以下是该部分内容的详细解释：</p>

<h3 id="51-overall-performance整体性能">5.1 Overall Performance（整体性能）</h3>

<p>在这一小节中，研究者比较了多种 RAG 模型在开放域问答、多跳问答、填槽和对话任务上的整体表现。通过对比不同方法的实验结果，得出以下结论：</p>

<ul>
  <li><strong>性能比较</strong>：DDR 优化的 RAG 模型（特别是多轮优化的版本）在多个任务上表现优于传统 RAG 方法、REPLUG 和 RA-DIT 方法。</li>
  <li><strong>结果分析</strong>：在大部分任务中，RAG-DDR 显著提高了生成的回答质量，表明 DDR 可以有效提升 RAG 模型的性能，使其更好地回答复杂的知识密集型问题。</li>
</ul>

<p>这表明通过 DDR 优化的模型可以更准确地理解和生成回答，有助于提升任务完成质量。</p>

<h3 id="52-ablation-study消融研究">5.2 Ablation Study（消融研究）</h3>

<p>消融研究用于分析 DDR 模型的各个组成部分对模型整体性能的贡献。在本实验中，研究者逐一去除或改变 DDR 模型的不同组件，观察对性能的影响：</p>

<ul>
  <li><strong>主要组件的影响</strong>：研究者通过逐步去除知识精炼模块、生成模块等组件，观察性能的变化，以验证这些模块对整体效果的贡献。</li>
  <li><strong>实验结果</strong>：结果表明，知识精炼模块和生成模块的加入显著提升了模型的整体性能，尤其是在需要多步推理和上下文理解的复杂任务上。</li>
</ul>

<p>消融研究表明，DDR 的每个组件在模型性能提升中都发挥了关键作用，缺少任何一个模块都会导致性能的显著下降。</p>

<h3 id="53-analysis-on-the-generation-module-in-rag-ddr生成模块分析">5.3 Analysis on the Generation Module in RAG-DDR（生成模块分析）</h3>

<p>这一部分对生成模块进行了深入分析，重点关注生成模块在不同任务中的表现，以及多轮优化对生成质量的影响：</p>

<ul>
  <li><strong>不同轮次的生成质量</strong>：对比一轮优化和两轮优化的生成模块的表现，发现两轮优化后的生成模块在生成质量上有显著提升，尤其是在复杂任务中，第二轮优化进一步增强了模型的回答准确性。</li>
  <li><strong>生成模块的任务适应性</strong>：分析表明，生成模块在复杂的知识密集型任务上表现优异，而在简单任务上与其他方法的差距相对较小。</li>
</ul>

<p>这说明在复杂任务中，多轮优化能够让生成模块更好地适应和利用精炼后的知识，从而提升回答质量。</p>

<h3 id="54-effectiveness-of-rag-ddr-to-use-external-knowledgerag-ddr-对外部知识的使用效果">5.4 Effectiveness of RAG-DDR to Use External Knowledge（RAG-DDR 对外部知识的使用效果）</h3>

<p>这一部分探讨了 RAG-DDR 模型利用外部知识的能力，通过实验分析其在知识检索和回答生成中的表现：</p>

<ul>
  <li><strong>知识检索与整合能力</strong>：RAG-DDR 在外部知识的检索和整合方面表现优于基线方法，特别是在需要跨多个知识点或需要从多个文档中提取信息的任务上，DDR 显著提高了模型对外部知识的利用效率。</li>
  <li><strong>与基线方法的对比</strong>：RAG-DDR 在使用外部知识生成回答时，比直接使用 LLM 或传统 RAG 方法更为准确。通过知识精炼模块的筛选，模型得以聚焦于与问题更相关的信息，从而提升回答质量。</li>
</ul>

<p>这一分析表明，DDR 优化的 RAG 模型能够更有效地利用外部知识，提升模型的知识覆盖范围和回答准确性。</p>

<h3 id="55-case-studies案例研究">5.5 Case Studies（案例研究）</h3>

<p>案例研究展示了 RAG-DDR 模型在实际应用中的具体表现，通过示例分析模型在不同任务中的生成结果：</p>

<ul>
  <li><strong>示例分析</strong>：研究者选取了多个任务中的生成结果进行分析，包括开放域问答、多跳问答、对话等。通过对比不同方法的生成结果，展示了 DDR 在复杂任务中的优势。</li>
  <li><strong>错误分析</strong>：研究者还分析了 RAG-DDR 模型在某些情况下生成不准确回答的原因，例如可能因为知识不完整或检索到的信息不够准确。这些错误为模型的进一步改进提供了方向。</li>
</ul>

<p>案例研究表明，RAG-DDR 模型在大多数情况下生成了准确且详细的回答，但在少数情况下可能仍有改进空间。</p>

<h3 id="总结-4">总结</h3>

<ul>
  <li><strong>DDR 显著提升 RAG 模型性能</strong>：多项实验结果表明，DDR 在多个任务上显著提升了 RAG 模型的生成质量，尤其是在复杂的知识密集型任务上。</li>
  <li><strong>模块和优化的有效性</strong>：知识精炼模块和多轮优化的生成模块是 DDR 成功的关键因素。</li>
  <li><strong>外部知识的高效利用</strong>：DDR 能够更有效地利用外部知识，提高了模型在知识检索和回答生成中的表现。</li>
</ul>]]></content><author><name>Xingjie Gao</name><email>xingjie-gao@outlook.com</email></author><category term="nlp" /><category term="自然语言处理" /><category term="论文笔记" /><summary type="html"><![CDATA[RAG-DDR: OPTIMIZING RETRIEVAL-AUGMENTED GENERATION USING DIFFERENTIABLE DATA REWARDS 论文笔记]]></summary></entry><entry><title type="html">MiniMind训练记录（第一次完整训LLM记录）</title><link href="http://localhost:4000/2024/11/12/MiniMind%E8%AE%AD%E7%BB%83%E8%AE%B0%E5%BD%95-%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%8C%E6%95%B4%E8%AE%ADLLM%E8%AE%B0%E5%BD%95-%E8%AE%AD%E7%BB%83%E4%B8%AD.html" rel="alternate" type="text/html" title="MiniMind训练记录（第一次完整训LLM记录）" /><published>2024-11-12T00:00:00+08:00</published><updated>2024-11-12T00:00:00+08:00</updated><id>http://localhost:4000/2024/11/12/MiniMind%E8%AE%AD%E7%BB%83%E8%AE%B0%E5%BD%95%EF%BC%88%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%8C%E6%95%B4%E8%AE%ADLLM%E8%AE%B0%E5%BD%95%EF%BC%89%E2%80%94%20%E8%AE%AD%E7%BB%83%E4%B8%AD</id><content type="html" xml:base="http://localhost:4000/2024/11/12/MiniMind%E8%AE%AD%E7%BB%83%E8%AE%B0%E5%BD%95-%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%8C%E6%95%B4%E8%AE%ADLLM%E8%AE%B0%E5%BD%95-%E8%AE%AD%E7%BB%83%E4%B8%AD.html"><![CDATA[<h1 id="minimind训练记录">MiniMind训练记录</h1>

<p>记录一下第一次训LLM，包括pretrain、SFT……，并把遇到的问题记录下来</p>

<!---more-->

<blockquote>
  <p>SFT和instruction fine-tuing的关系？</p>
</blockquote>

<p><img src="/assets/posts_assets/Pasted%20image%2020241112190607.png" alt="" /></p>

<h2 id="训练结果">训练结果</h2>

<h3 id="pre-train">Pre-Train</h3>

<p><img src="/assets/posts_assets/image-20241111185411264.png" alt="" /></p>

<h3 id="sft">SFT</h3>

<p><img src="/assets/posts_assets/W&amp;B%20Chart%202024_11_14%2009_41_28.png" alt="" /></p>

<p><img src="assets/posts_assets/Pasted%20image%2020241114200359.png" alt="" /></p>

<h2 id="tokenizer">Tokenizer</h2>

<ul>
  <li>
    <p>算法：BPE</p>
  </li>
  <li>
    <p>训练方式：无监督训练</p>
  </li>
  <li>
    <p>训练流程：</p>

    <ol>
      <li>
        <p>读取数据</p>
      </li>
      <li>
        <p>定义special token</p>
      </li>
      <li>
        <p>训练</p>
      </li>
      <li>
        <p>保存模型</p>
      </li>
    </ol>
  </li>
  <li>
    <p>数据：(unicode)</p>
  </li>
</ul>

<p><img src="/assets/posts_assets/截屏2024-11-11%2016.03.56.png" alt="" /></p>

<ul>
  <li>核心代码：</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="err">    </span><span class="c1"># 初始化tokenizer  
</span><span class="err">  </span> <span class="err"> </span><span class="n">tokenizer</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="nc">Tokenizer</span><span class="p">(</span><span class="n">models</span><span class="p">.</span><span class="nc">BPE</span><span class="p">())</span>  
<span class="err">  </span> <span class="err"> </span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pre_tokenizer</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">pre_tokenizers</span><span class="p">.</span><span class="nc">ByteLevel</span><span class="p">(</span><span class="n">add_prefix_space</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  
<span class="err"> ​</span>  
<span class="err">  </span> <span class="err"> </span><span class="c1"># 定义特殊token  
</span><span class="err">  </span> <span class="err"> </span><span class="n">special_tokens</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="p">[</span><span class="sh">"</span><span class="s">&lt;unk&gt;</span><span class="sh">"</span><span class="p">,</span><span class="err"> </span><span class="sh">"</span><span class="s">&lt;s&gt;</span><span class="sh">"</span><span class="p">,</span><span class="err"> </span><span class="sh">"</span><span class="s">&lt;/s&gt;</span><span class="sh">"</span><span class="p">]</span>  
<span class="err"> ​</span>  
<span class="err">  </span> <span class="err"> </span><span class="c1"># 设置训练器并添加特殊token  
</span><span class="err">  </span> <span class="err"> </span><span class="n">trainer</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">trainers</span><span class="p">.</span><span class="nc">BpeTrainer</span><span class="p">(</span>  
<span class="err">  </span> <span class="err"> </span> <span class="err"> </span> <span class="err"> </span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">6400</span><span class="p">,</span>  
<span class="err">  </span> <span class="err"> </span> <span class="err"> </span> <span class="err"> </span><span class="n">special_tokens</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">,</span> <span class="err"> </span><span class="c1"># 确保这三个token被包含  
</span><span class="err">  </span> <span class="err"> </span> <span class="err"> </span> <span class="err"> </span><span class="n">show_progress</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  
<span class="err">  </span> <span class="err"> </span> <span class="err"> </span> <span class="err"> </span><span class="n">initial_alphabet</span><span class="o">=</span><span class="n">pre_tokenizers</span><span class="p">.</span><span class="n">ByteLevel</span><span class="p">.</span><span class="nf">alphabet</span><span class="p">()</span>  
<span class="err">  </span> <span class="p">)</span>  
<span class="err"> ​</span>  
<span class="err">  </span> <span class="err"> </span><span class="c1"># 读取文本数据  
</span><span class="err">  </span> <span class="err"> </span><span class="n">texts</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="nf">read_texts_from_jsonl</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>  
<span class="err"> ​</span>  
<span class="err">  </span> <span class="err"> </span><span class="c1"># 训练tokenizer  
</span><span class="err">  </span> <span class="err"> </span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">train_from_iterator</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span><span class="err"> </span><span class="n">trainer</span><span class="o">=</span><span class="n">trainer</span><span class="p">)</span>  
<span class="err"> ​</span>  
<span class="err">  </span> <span class="err"> </span><span class="c1"># 设置解码器  
</span><span class="err">  </span> <span class="err"> </span><span class="n">tokenizer</span><span class="p">.</span><span class="n">decoder</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">decoders</span><span class="p">.</span><span class="nc">ByteLevel</span><span class="p">()</span>  
<span class="err"> ​</span>  
<span class="err">  </span> <span class="err"> </span><span class="c1"># 检查特殊token的索引  
</span><span class="err">  </span> <span class="err"> </span><span class="k">assert</span><span class="err"> </span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">token_to_id</span><span class="p">(</span><span class="sh">"</span><span class="s">&lt;unk&gt;</span><span class="sh">"</span><span class="p">)</span><span class="err"> </span><span class="o">==</span><span class="err"> </span><span class="mi">0</span>  
<span class="err">  </span> <span class="err"> </span><span class="k">assert</span><span class="err"> </span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">token_to_id</span><span class="p">(</span><span class="sh">"</span><span class="s">&lt;s&gt;</span><span class="sh">"</span><span class="p">)</span><span class="err"> </span><span class="o">==</span><span class="err"> </span><span class="mi">1</span>  
<span class="err">  </span> <span class="err"> </span><span class="k">assert</span><span class="err"> </span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">token_to_id</span><span class="p">(</span><span class="sh">"</span><span class="s">&lt;/s&gt;</span><span class="sh">"</span><span class="p">)</span><span class="err"> </span><span class="o">==</span><span class="err"> </span><span class="mi">2</span>  
<span class="err">  </span> <span class="err"> </span>  
<span class="err">  </span> <span class="err"> </span><span class="c1"># 保存tokenizer  
</span><span class="err">  </span> <span class="err"> </span><span class="n">tokenizer_dir</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="sh">"</span><span class="s">./model/minimind_tokenizer</span><span class="sh">"</span>  
<span class="err">  </span> <span class="err"> </span><span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">tokenizer_dir</span><span class="p">,</span><span class="err"> </span><span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  
<span class="err">  </span> <span class="err"> </span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">tokenizer_dir</span><span class="p">,</span><span class="err"> </span><span class="sh">"</span><span class="s">tokenizer.json</span><span class="sh">"</span><span class="p">))</span>  
<span class="err">  </span> <span class="err"> </span><span class="n">tokenizer</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="sh">"</span><span class="s">./model/minimind_tokenizer</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="bep算法的原理">BEP算法的原理</h3>

<p>字节对编码（Byte-Pair Encoding, BPE）是一种常用的分词算法，广泛用于自然语言处理任务中。BPE最初是一种数据压缩算法，但它在语言模型中被重新应用，特别是在处理不定长词汇的分词任务时具有很好的效果。BPE的主要思想是通过合并频率较高的字符对或词对，逐步构建词汇表，使得模型能够处理词汇中的不同词素或子词单元。以下是BPE算法的工作原理和步骤：</p>

<h4 id="bpe算法的核心思想">BPE算法的核心思想</h4>

<p>BPE的核心思想是通过迭代地合并频率最高的字符或子词对，将文本分割成词根或子词单元，这样可以在一个较小的词汇表中实现更好的覆盖率，尤其在低频词或新词的处理上表现优异。这对于自然语言模型（如GPT、BERT）非常重要，因为这类模型往往需要一个有限的词汇表来处理大量的文本数据。</p>

<h4 id="bpe算法的步骤">BPE算法的步骤</h4>

<ol>
  <li>
    <p><strong>初始化：字符分解</strong> 将所有词汇分解成单独的字符。比如，”apple”会被分解成<code class="language-plaintext highlighter-rouge">a p p l e</code>。在这个初始阶段，每个字符都被视为一个独立的词素。</p>
  </li>
  <li>
    <p><strong>统计字符对频率</strong> 在分解后的词汇表中统计每一对连续字符的出现频率。例如，如果 “ap” 和 “pp” 在词汇表中出现频率最高，那么它们的频率就会被记录下来。</p>
  </li>
  <li>
    <p><strong>合并最高频率的字符对</strong> 找出频率最高的字符对，并将其合并成一个新的符号。例如，如果 “p p” 出现最多，则将其替换为一个新的子词 “pp”，使得 “apple” 变成 <code class="language-plaintext highlighter-rouge">a pp l e</code>。</p>
  </li>
  <li>
    <p><strong>更新词汇表</strong> 在整个词汇表中替换合并的字符对，同时更新词汇表，记录新的子词。然后，重复统计频率、合并字符对、更新词汇表的过程。</p>
  </li>
  <li>
    <p><strong>重复合并过程</strong> 上述过程会不断重复，直到词汇表的大小达到预先定义的上限或没有高频字符对可以合并。经过多次迭代，词汇表会逐渐从单字符组成的子词单元扩展为更长的词或词根单元。</p>
  </li>
</ol>

<h4 id="bpe算法的优点">BPE算法的优点</h4>

<ul>
  <li>
    <p><strong>减少词汇表大小</strong>：通过将词汇分解成子词，BPE可以大幅缩减模型词汇表的大小，从而减少存储空间并提升计算效率。</p>
  </li>
  <li>
    <p><strong>处理未登录词</strong>：BPE的子词分解方式可以有效处理未登录词，即不在训练词汇表中的词。即便是全新词汇，也可以通过子词单元组合被模型理解。</p>
  </li>
  <li>
    <p><strong>通用性强</strong>：BPE适用于多种语言，在处理词形变化丰富的语言（如德语、芬兰语等）时尤其有效，因为其分解后的子词能涵盖词根、前缀和后缀等不同形式。</p>
  </li>
</ul>

<h4 id="示例">示例</h4>

<p>假设我们有一个简单的词汇表，包含了以下词汇：<code class="language-plaintext highlighter-rouge">low</code>, <code class="language-plaintext highlighter-rouge">lowest</code>, <code class="language-plaintext highlighter-rouge">newer</code>, <code class="language-plaintext highlighter-rouge">wider</code>。通过BPE分词过程，这些词可以被逐步拆分和合并。例如：</p>

<ol>
  <li>
    <p>初始状态下，所有词被拆解为单个字符：<code class="language-plaintext highlighter-rouge">l o w</code>, <code class="language-plaintext highlighter-rouge">l o w e s t</code>, <code class="language-plaintext highlighter-rouge">n e w e r</code>, <code class="language-plaintext highlighter-rouge">w i d e r</code>。</p>
  </li>
  <li>
    <p>假设在此状态下，<code class="language-plaintext highlighter-rouge">l o</code>和<code class="language-plaintext highlighter-rouge">w i</code>出现频率较高，那么BPE会先将<code class="language-plaintext highlighter-rouge">l o</code>和<code class="language-plaintext highlighter-rouge">w i</code>合并。</p>
  </li>
  <li>
    <p>经过多次合并，词汇表最终可能包括较长的子词，如<code class="language-plaintext highlighter-rouge">low</code>，<code class="language-plaintext highlighter-rouge">wid</code>和<code class="language-plaintext highlighter-rouge">est</code>等。</p>
  </li>
</ol>

<h2 id="pre-train-1">Pre-train</h2>

<ul>
  <li>训练方式：无监督</li>
  <li>数据：<img src="/assets/posts_assets/截屏2024-11-11%2016.05.31.png" alt="" /></li>
  <li>训练过程：
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">for</span><span class="err"> </span><span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="err"> </span><span class="n">Y</span><span class="p">,</span><span class="err"> </span><span class="n">loss_mask</span><span class="p">)</span><span class="err"> </span><span class="ow">in</span><span class="err"> </span><span class="nf">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>  
      <span class="n">X</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">X</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  
      <span class="n">Y</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">Y</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  
      <span class="n">loss_mask</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">loss_mask</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>  
 <span class="err">​</span>  
      <span class="n">lr</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="nf">get_lr</span><span class="p">(</span><span class="n">epoch</span><span class="err"> </span><span class="o">*</span><span class="err"> </span><span class="n">iter_per_epoch</span><span class="err"> </span><span class="o">+</span><span class="err"> </span><span class="n">step</span><span class="p">,</span><span class="err"> </span><span class="n">args</span><span class="p">.</span><span class="n">epochs</span><span class="err"> </span><span class="o">*</span><span class="err"> </span><span class="n">iter_per_epoch</span><span class="p">)</span>  
      <span class="k">for</span><span class="err"> </span><span class="n">param_group</span><span class="err"> </span><span class="ow">in</span><span class="err"> </span><span class="n">optimizer</span><span class="p">.</span><span class="n">param_groups</span><span class="p">:</span>  
          <span class="n">param_group</span><span class="p">[</span><span class="sh">"</span><span class="s">lr</span><span class="sh">"</span><span class="p">]</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">lr</span>  
 <span class="err">​</span>  
      <span class="k">with</span><span class="err"> </span><span class="n">ctx</span><span class="p">:</span>  
          <span class="n">out</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="nf">model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="err"> </span><span class="n">Y</span><span class="p">)</span>  
          <span class="n">loss</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">out</span><span class="p">.</span><span class="n">last_loss</span><span class="err"> </span><span class="o">/</span><span class="err"> </span><span class="n">args</span><span class="p">.</span><span class="n">accumulation_steps</span>  
          <span class="n">loss_mask</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">loss_mask</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  
          <span class="n">loss</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">loss</span><span class="err"> </span><span class="o">*</span><span class="err"> </span><span class="n">loss_mask</span><span class="p">)</span><span class="err"> </span><span class="o">/</span><span class="err"> </span><span class="n">loss_mask</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>  
 <span class="err">​</span>  
      <span class="n">scaler</span><span class="p">.</span><span class="nf">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="nf">backward</span><span class="p">()</span>  
 <span class="err">​</span>  
      <span class="k">if</span><span class="err"> </span><span class="p">(</span><span class="n">step</span><span class="err"> </span><span class="o">+</span><span class="err"> </span><span class="mi">1</span><span class="p">)</span><span class="err"> </span><span class="o">%</span><span class="err"> </span><span class="n">args</span><span class="p">.</span><span class="n">accumulation_steps</span><span class="err"> </span><span class="o">==</span><span class="err"> </span><span class="mi">0</span><span class="p">:</span>  
          <span class="n">scaler</span><span class="p">.</span><span class="nf">unscale_</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>  
          <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span><span class="err"> </span><span class="n">args</span><span class="p">.</span><span class="n">grad_clip</span><span class="p">)</span>  
 <span class="err">​</span>  
          <span class="n">scaler</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>  
          <span class="n">scaler</span><span class="p">.</span><span class="nf">update</span><span class="p">()</span>  
<span class="err">​</span>      <span class="err">  </span> <span class="err"> </span> <span class="err"> </span><span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>    <span class="err">  </span> <span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="err"> </span> 
</code></pre></div>    </div>
  </li>
  <li>训练结果<img src="/assets/posts_assets/image-20241111185411264.png" alt="" /></li>
</ul>

<h3 id="loss-计算">Loss 计算</h3>

<p>交叉熵损失：</p>

<p>交叉熵损失（Cross-Entropy Loss）是一种衡量两个概率分布之间差异的损失函数，常用于分类任务，尤其是多分类问题中。计算交叉熵损失时，我们通常将模型的输出概率分布与真实的类别标签分布进行比较，若两者的分布越接近，损失越小。</p>

<p>下面通过一个具体的例子来介绍交叉熵损失的计算过程。</p>

<h4 id="交叉熵损失的公式">交叉熵损失的公式</h4>

<p>对于一个样本的交叉熵损失的计算公式为：</p>

\[L = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)\]

<p>其中：</p>

<ul>
  <li>
    <p>( C ) 是类别数。</p>
  </li>
  <li>
    <p>( y_i ) 是实际的类别分布（通常是 one-hot 编码，只有一个位置为 1，其余为 0）。</p>
  </li>
  <li>
    <p>$\hat{y}_i$ 是模型输出的预测概率（经过 softmax 层后得到的概率分布）。</p>
  </li>
</ul>

<p>在分类问题中，交叉熵损失会对每个样本计算一次，整个数据集的损失是所有样本的平均值。</p>

<h4 id="示例-1">示例</h4>

<p>假设我们有一个分类任务，需要将样本分为三类 ( C = 3 )，类别分别为 0、1 和 2。</p>

<h4 id="已知数据">已知数据：</h4>

<ul>
  <li>
    <p>真实类别为类别 2（即标签为 ([0, 0, 1])）。</p>
  </li>
  <li>
    <p>模型输出的概率分布为 $\hat{y} = [0.2, 0.3, 0.5]$。</p>
  </li>
</ul>

<h4 id="计算过程">计算过程：</h4>

<ol>
  <li>
    <p><strong>表示实际分布 ( y )</strong>：真实类别是类别 2，因此我们用 one-hot 编码表示 ( y = [0, 0, 1] )。</p>
  </li>
  <li>
    <p><strong>预测概率分布 ( \hat{y} )</strong>：模型的预测概率为 (\hat{y} = [0.2, 0.3, 0.5])。</p>
  </li>
  <li>
    <p><strong>代入交叉熵公式</strong>：
 \(L = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)\)</p>

    <p>将真实标签 ( y = [0, 0, 1] ) 和预测概率 $hat{y} = [0.2, 0.3, 0.5]$ 代入，只有类别 2 的位置 $y_3 = 1$ 时才会有贡献，得到：</p>

\[L = -[0 \cdot \log(0.2) + 0 \cdot \log(0.3) + 1 \cdot \log(0.5)]\]

    <p>简化为：</p>

\[L = -\log(0.5)\]
  </li>
  <li>
    <p><strong>计算交叉熵损失值</strong>：</p>

    <p>\(L = -\log(0.5) \approx 0.693\)</p>
    <h4 id="解释">解释</h4>
  </li>
</ol>

<p>在这个例子中，交叉熵损失值为 ( 0.693 )。如果模型的预测概率分布与实际分布越接近（例如模型输出为 ([0, 0, 1])），则交叉熵损失值会更接近 0，表示模型的预测越准确。而如果模型输出偏离真实标签的概率（如 ([0.5, 0.3, 0.2])），交叉熵损失值将更高。</p>

<h3 id="梯度累积"><strong>梯度累积</strong></h3>

<p>梯度累积是指在训练神经网络时，不是每次计算完损失后的反向传播都立即更新模型参数，而是累积多个小批量数据的梯度，再进行一次参数更新。这样做的主要目的是在显存有限的情况下，通过累积梯度来模拟更大的批量大小，从而稳定训练、提高模型性能。</p>

<p>在你的代码中：</p>

<pre><code class="language-pyhon"> if (step + 1) % args.accumulation_steps == 0:  
    scaler.unscale_(optimizer)  
    torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)  
    scaler.step(optimizer)  
    scaler.update()  
    optimizer.zero_grad(set_to_none=True)
</code></pre>
<p>这里的</p>

<p>args.accumulation_steps</p>

<p>就是设置的累积步数。代码的含义是：</p>

<ul>
  <li>
    <p><strong>梯度累积</strong>：在每次前向和反向传播后，梯度并没有被清零，而是累积起来。</p>
  </li>
  <li>
    <p><strong>条件判断</strong>：当达到设定的累积步数时（即 <code class="language-plaintext highlighter-rouge">(step + 1) % args.accumulation_steps == 0</code>），执行参数更新。</p>
  </li>
  <li>
    <p><strong>梯度剪裁</strong>：使用 <code class="language-plaintext highlighter-rouge">clip_grad_norm_</code> 对梯度进行剪裁，防止梯度爆炸。</p>
  </li>
  <li>
    <p><strong>优化器更新</strong>：调用 <code class="language-plaintext highlighter-rouge">scaler.step(optimizer)</code>和 <code class="language-plaintext highlighter-rouge">scaler.update()</code>来更新参数。</p>
  </li>
  <li>
    <p><strong>清零梯度</strong>：使用 <code class="language-plaintext highlighter-rouge">optimizer.zero_grad(set_to_none=True)</code>将梯度清零，准备下一轮的累积。通过这种方式，可以在不增加显存占用的情况下，模拟更大的批量训练，有助于优化模型的收敛性和性能。</p>
  </li>
</ul>

<h3 id="反放缩和裁剪"><strong>反放缩和裁剪?</strong></h3>

<h2 id="sft-1">SFT</h2>

<p>（Full SFT Instruction fine tuning？）</p>

<p>指令微调</p>

<ul>
  <li>
    <p>训练方式：有监督</p>
  </li>
  <li>
    <p>数据：<img src="/assets/posts_assets/截屏2024-11-11%2018.40.45.png" alt="" /></p>

    <p><img src="/assets/posts_assets/截屏2024-11-11%2018.41.12%201.png" alt="" /></p>
  </li>
  <li>
    <p>训练过程：与pre-train 相同</p>
  </li>
  <li>
    <p>核心代码: 与pre-train 相同</p>
  </li>
  <li>
    <p>训练结果：<img src="assets/posts_assets/截屏2024-11-14%2014.10.22.png" alt="" /></p>
  </li>
</ul>

<h2 id="lora">LORA</h2>

<h2 id="一些bug记录">一些bug记录</h2>

<h3 id="deepspeed运行指定gpu">deepspeed运行指定gpu</h3>

<p>单节点全部卡：–master_port=25684 –num_gpus=4</p>

<p>单节点部分卡：–include localhost:1,2,3</p>

<p>注意：不能使用CUDA_VISIBLE_DEVICES，无论使用 CUDA_VISIBLE_DEVICES=1,2,3 bash， 或者 CUDA_VISIBLE_DEVICES=1,2,3 deepspeed 都无效</p>

<p>例子： 使用cuda:0 cuda:3 显卡，单机多卡运行
host:1,2,3</p>

<p>注意：不能使用CUDA_VISIBLE_DEVICES，无论使用 CUDA_VISIBLE_DEVICES=1,2,3 bash， 或者 CUDA_VISIBLE_DEVICES=1,2,3 deepspeed 都无效</p>

<p>例子： 使用cuda: 0 cuda: 3 显卡，单机多卡运行</p>

<pre><code class="language-cmd"> deepspeed --include localhost:0,3 1-pretrain.py  
</code></pre>
<p> ​</p>

<h3 id="deepspeed运行出错">deepspeed运行出错</h3>

<pre><code class="language-cmd"> RuntimeError: DDP expects same model across all ranks, but Rank 0 has 237 params, while rank 1 has inconsistent 0 params.
</code></pre>

<p>显示通信超时</p>

<p>解决办法：在代码了添加如下几行</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">NCCL_DEBUG</span><span class="sh">"</span><span class="p">]</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="sh">"</span><span class="s">INFO</span><span class="sh">"</span>  
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">NCCL_IB_DISABLE</span><span class="sh">"</span><span class="p">]</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="sh">"</span><span class="s">1</span><span class="sh">"</span>  
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">NCCL_P2P_LEVEL</span><span class="sh">"</span><span class="p">]</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="sh">"</span><span class="s">NVL</span><span class="sh">"</span>
</code></pre></div></div>

<p>使用</p>]]></content><author><name>Xingjie Gao</name><email>xingjie-gao@outlook.com</email></author><category term="nlp" /><category term="自然语言处理" /><category term="#实验记录" /><summary type="html"><![CDATA[MiniMind训练记录 记录一下第一次训LLM，包括pretrain、SFT……，并把遇到的问题记录下来]]></summary></entry><entry><title type="html">Adaptive Note RAG 论文笔记</title><link href="http://localhost:4000/2024/11/05/Retriever-and-Memory-Towards-Adaptive-Note-Enhanced-Retrieval-Augmented-Generation%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.html" rel="alternate" type="text/html" title="Adaptive Note RAG 论文笔记" /><published>2024-11-05T00:00:00+08:00</published><updated>2024-11-05T00:00:00+08:00</updated><id>http://localhost:4000/2024/11/05/Retriever-and-Memory%20Towards%20Adaptive%20%20Note-Enhanced%20Retrieval-Augmented%20Generation%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0</id><content type="html" xml:base="http://localhost:4000/2024/11/05/Retriever-and-Memory-Towards-Adaptive-Note-Enhanced-Retrieval-Augmented-Generation%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.html"><![CDATA[<h1 id="retriever-and-memory-towards-adaptive--note-enhanced-retrieval-augmented-generation论文笔记">Retriever-and-Memory: Towards Adaptive  Note-Enhanced Retrieval-Augmented Generation论文笔记</h1>

<!---more-->

<blockquote>
  <p>论文来源：师兄给的</p>

  <p>问题：</p>
</blockquote>

<h2 id="不懂的知识">不懂的知识</h2>

<h3 id="long-form-qa--multi-hop-qa--open-qa">long-form QA / multi-hop QA / open QA</h3>

<p>Multi-hop QA、Long-form QA 和 OpenQA 是问答系统中的三种不同类型，各自有不同的关注点和特点，但在实际应用中，它们可以彼此融合或互补。以下是它们之间的关系及对比：</p>

<ol>
  <li>
    <p><strong>OpenQA</strong>：</p>

    <ul>
      <li><strong>定义</strong>：OpenQA 是开放领域问答系统，旨在回答任何主题的自然语言问题，通常依赖于大型知识库或非结构化文本（如网页和维基百科）进行信息检索。</li>
      <li><strong>核心特点</strong>：在不限定领域的前提下，OpenQA系统能够从大量数据中搜索、抽取和生成答案。其主要挑战在于从庞大的数据源中找到准确、相关的信息。</li>
      <li><strong>任务目标</strong>：快速准确地回答广泛、开放的问题，通常是简短、事实性的回答，但也可以是较长的内容。</li>
    </ul>
  </li>
  <li>
    <p><strong>Multi-hop QA</strong>：</p>

    <ul>
      <li><strong>定义</strong>：Multi-hop QA 是一种专注于多步推理的问答任务，需要从多个信息片段中获取和连接线索来得到答案。</li>
      <li><strong>核心特点</strong>：在信息整合和推理链条构建方面具有挑战性，因为要在多个文本或逻辑链条之间进行连接。尽管信息可能在不同的文本片段中分散，但系统需要整合它们来形成最终答案。</li>
      <li><strong>任务目标</strong>：通过多个步骤的推理来回答问题，适用于需要多层逻辑或因果关系的问题，通常生成简短且确定的答案。</li>
    </ul>
  </li>
  <li>
    <p><strong>Long-form QA</strong>：</p>

    <ul>
      <li><strong>定义</strong>：Long-form QA 关注生成详细、解释性的回答，尤其适用于需要背景、分析或综合性回答的问题。</li>
      <li><strong>核心特点</strong>：强调生成连贯的长篇内容，回答需要深入的解释和背景信息，而不仅仅是单一的事实答案。系统需要具备较强的文本生成能力，以确保回答的内容丰富性和逻辑连贯性。</li>
      <li><strong>任务目标</strong>：提供深度的回答，回答内容类似一篇小文章，适合用户寻求综合性、解释性或教育性答案的场景。</li>
    </ul>
  </li>
</ol>

<p>三者的对比与关系</p>

<table>
  <thead>
    <tr>
      <th>特点</th>
      <th><strong>OpenQA</strong></th>
      <th><strong>Multi-hop QA</strong></th>
      <th><strong>Long-form QA</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>目标</strong></td>
      <td>回答开放领域问题</td>
      <td>通过多步推理找到准确答案</td>
      <td>生成详细解释和背景的回答</td>
    </tr>
    <tr>
      <td><strong>关注点</strong></td>
      <td>信息检索与相关性</td>
      <td>多步逻辑推理和信息整合</td>
      <td>深度生成与连贯性</td>
    </tr>
    <tr>
      <td><strong>答案类型</strong></td>
      <td>简短或事实性答案</td>
      <td>简短但需要多步推理的答案</td>
      <td>连贯的长篇内容</td>
    </tr>
    <tr>
      <td><strong>关系</strong></td>
      <td>可结合 Multi-hop QA 或 Long-form QA 实现开放领域中的多步推理或长篇回答</td>
      <td>可以在 OpenQA 系统中实现多步推理</td>
      <td>可用于 OpenQA 系统中生成解释性答案</td>
    </tr>
  </tbody>
</table>

<p><strong>整合方式</strong>：
在复杂问答系统中，这三者可以结合使用。例如：</p>

<ul>
  <li><strong>OpenQA + Multi-hop QA</strong>：在开放领域中使用多步推理来找到答案，例如跨文档推理问题。</li>
  <li><strong>OpenQA + Long-form QA</strong>：在开放领域中生成解释性或背景丰富的长篇回答，例如百科式回答。</li>
</ul>

<p><strong>总结</strong>：</p>

<ul>
  <li><strong>OpenQA</strong> 关注领域广泛的问题。</li>
  <li><strong>Multi-hop QA</strong> 关注多步信息推理。</li>
  <li><strong>Long-form QA</strong> 关注详细、解释性的回答。</li>
</ul>

<h3 id="llm基本任务">LLM基本任务</h3>

<table>
  <thead>
    <tr>
      <th>任务</th>
      <th>英文名称</th>
      <th>简写</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>问答</td>
      <td>Question Answering</td>
      <td>QA</td>
    </tr>
    <tr>
      <td>文本生成</td>
      <td>Text Generation</td>
      <td>TG</td>
    </tr>
    <tr>
      <td>文本总结</td>
      <td>Text Summarization</td>
      <td>TS</td>
    </tr>
    <tr>
      <td>语言翻译</td>
      <td>Machine Translation</td>
      <td>MT</td>
    </tr>
    <tr>
      <td>文本分类</td>
      <td>Text Classification</td>
      <td>TC</td>
    </tr>
    <tr>
      <td>实体识别</td>
      <td>Named Entity Recognition</td>
      <td>NER</td>
    </tr>
    <tr>
      <td>文本改写与纠错</td>
      <td>Text Rewriting and Error Correction</td>
      <td>TREC</td>
    </tr>
    <tr>
      <td>对话生成</td>
      <td>Dialogue Generation</td>
      <td>DG</td>
    </tr>
    <tr>
      <td>代码生成和分析</td>
      <td>Code Generation and Analysis</td>
      <td>CGA</td>
    </tr>
    <tr>
      <td>知识提取</td>
      <td>Knowledge Extraction</td>
      <td>KE</td>
    </tr>
    <tr>
      <td>个性化推荐</td>
      <td>Personalized Recommendation</td>
      <td>PR</td>
    </tr>
    <tr>
      <td>文本推理</td>
      <td>Textual Inference</td>
      <td>TI</td>
    </tr>
  </tbody>
</table>

<h2 id="摘要">摘要</h2>

<blockquote>
  <p>除了QA任务还有哪些？</p>
</blockquote>

<h3 id="当前的rag">当前的RAG</h3>

<p>RAG：检索增强生成（RAG）通过引入外部知识，缓解大型语言模型（LLMs）在开放域问答任务（OpenQA）中产生的事实错误和虚构输出问题。</p>

<p>现有的RAG方法使用LLMs来预测检索时机，<strong>所以RAG的现有问题</strong>：</p>

<ul>
  <li>直接使用检索到的信息进行生成而不考虑检索时机是否准确反映实际信息需求</li>
  <li>也没有足够考虑先前检索到的知识</li>
  <li>后果：<strong>这可能导致信息收集和交互不足，产生质量低下的答案。</strong></li>
</ul>

<h3 id="解决办法">解决办法</h3>

<p><strong>Adaptive Note-Enhance RAG（Adaptive-Note）</strong>,用于复杂的QA任务</p>

<ul>
  <li>迭代信息收集器</li>
  <li>自适应记忆审阅器</li>
  <li>任务导向生成器</li>
</ul>

<p>新的范式：<strong>Retriever-and-Memory</strong></p>

<p>具体：</p>

<ol>
  <li>给出了一个总体的的方法观点来解决知识增长的问题
    <ol>
      <li>以note的形式迭代收集新信息</li>
      <li>并将其更新到现有的最佳知识结构中，</li>
      <li>目的：从而增强高质量的知识互动。</li>
    </ol>
  </li>
  <li>给出了一种基于注释的自适应停止探索策略，以决定“检索什么以及何时停止”，
    <ol>
      <li>目的：以鼓励充分的知识探索。</li>
    </ol>
  </li>
</ol>

<h3 id="结果实验">结果&amp;实验</h3>

<p>我们在五个复杂的QA数据集上进行了大量实验，结果表明我们的方法及其组件的优越性和有效性。</p>

<h2 id="引言">引言</h2>

<h3 id="rag当前的问题">RAG当前的问题</h3>

<ul>
  <li>LLM遇到幻觉和事实性错误，用RAG解决</li>
  <li>RAG：这是一种利用外部非参数化知识资源来帮助LLMs推动其固有参数知识边界的有前途的技术</li>
  <li>当前的RAG范式：Retriever-and-Reader</li>
  <li>当前RAG的缺点：<strong>无法收集足够的信息来完成long-form QA和 multi-hop QA 这些复杂的QA</strong>
    <ul>
      <li>原因：</li>
      <li>mutil-hop QA：通常涉及广泛或深入的信息检索需求，这些需求可能没有明确反映在初始查询中，或者在单次检索尝试中很容易实现。
        <ul>
          <li>例子：在多跳QA中，要回答“丹麦足球联盟所属组织的首字母缩写是什么？”，我们必须首先搜索首字母缩写“FIFA”，然后搜索“FIFA”代表什么。</li>
        </ul>
      </li>
      <li>long-form QA ：处理<strong>含糊查询</strong>需要探讨各个相关方面并深入细节，生成全面和长格式的答案。</li>
    </ul>
  </li>
</ul>

<h3 id="当前的解决办法">当前的解决办法</h3>

<p>ARAG：为了解决“何时检索以及检索什么内容”的问题，试图通过灵活的知识捕获机制来提升回答质量。</p>

<p>缺点：</p>

<ol>
  <li><strong>逐步生成问题</strong>：每次检索都会立即生成一个输出，这会导致每个输出段仅反映当前检索到的有限知识，而忽略了跨不同检索步骤的信息整合和交互。</li>
  <li><strong>检索时机的预测偏差</strong>：ARAG利用LLM预测检索的时机，但由于LLM的内部认知与实际检索需求的差异，可能会错过关键的检索时机，导致知识收集不充分。</li>
</ol>

<h3 id="作者的解决办法">作者的解决办法</h3>

<p>针对这种复杂的QA问题</p>

<p>提出了基于新范式<strong>Retriever-Memory</strong>的模型<strong>Adaptive Note-Enhance RAG（Adaptive-Note）</strong></p>

<h4 id="模型介绍">模型介绍</h4>

<p>模型由两部分组成：<strong>Iterative Information Collector (IIC)</strong>、 <strong>Adaptive Memory Reviewer (AMR)</strong></p>

<p>作用：自适应地从一个成长的视角收集信息，并灵活地将新知识与所有已收集的知识整合，实现良好的性能。</p>

<h5 id="iic模块">IIC模块</h5>

<blockquote>
  <p>note 是哪种形式呢？</p>

  <p>知识结构是如何被优化的？</p>

  <p>IIC是怎么确保最大化调整与原始查询相关的所有细节？</p>

  <p>新note是如何被加入原本note中的？</p>
</blockquote>

<ol>
  <li><strong>知识整合问题</strong>：为解决不同检索步骤间信息交互不足的问题，IIC引入了“笔记（note）”作为知识的存储载体，而不是在每次检索后立即生成结果。</li>
  <li><strong>最大化搜索：</strong>IIC不是在检索后立即生成结果，而是最大化调整与原始查询相关的所有细节，并将它们存储在笔记中。</li>
  <li><strong>动态更新</strong>：每当有新的检索信息，IIC会将其以文本形式适应性地添加到已有的笔记中，以保证知识结构的连续性和优化。（知识结构会被持续更新和优化，整合之前所有检索步骤中有效的信息，使得最终的知识结构更全面、更准确。）</li>
</ol>

<p>总之，这段描述了IIC如何通过笔记存储和持续优化的方式，使得各检索步骤之间的信息能够有效整合，避免了因即时生成输出而导致的知识片面性。</p>

<h5 id="amr模块">AMR模块</h5>

<blockquote>
  <p>疑问一：<strong>和主动检索有什么区别呢？</strong></p>

  <p>论文中的 <strong>note-based stop-exploration strategy（基于笔记的停止探索策略）</strong> 和 <strong>active retrieval（主动检索）</strong> 的区别在于知识收集的方式和控制检索过程的策略：</p>

  <ol>
    <li>
      <p><strong>检索触发方式</strong>：</p>

      <ul>
        <li><strong>Active Retrieval</strong>：模型在回答过程中动态地判断是否需要额外检索，依据的是当前回答生成的反馈或置信度来决定是否触发新的检索。</li>
        <li><strong>Note-based Stop-Exploration</strong>：则是基于一个“笔记”作为知识的存储和整合载体。在检索过程中，所有获得的有用信息都会被存储到笔记中，并逐步整合和优化。当模型发现笔记中已有的知识结构已经足够完善时，就会触发停止条件，从而不再继续检索。</li>
      </ul>
    </li>
    <li>
      <p><strong>停止机制</strong>：</p>

      <ul>
        <li><strong>Active Retrieval</strong> 关注的是在生成过程中的动态需求，检索可能是分阶段触发的，不一定有明确的停止点，因此容易受到时机偏差（timing bias）的影响。</li>
        <li><strong>Note-based Stop-Exploration</strong> 使用一个“笔记”来不断积累和评估知识的完整性，只有在知识增长不足或达到特定条件时才停止检索。这种方法可以有效规避时机偏差，确保所有检索的知识都得到充分整合，停止条件更明确和系统化。</li>
      </ul>
    </li>
    <li>
      <p><strong>使用目的</strong>：</p>

      <ul>
        <li><strong>Active Retrieval</strong> 更适合用于逐步生成答案时需要特定信息的情况。</li>
        <li><strong>Note-based Stop-Exploration</strong> 则更适合在复杂问题中进行广泛信息整合和更新，以确保所有相关知识都完整且无冗余地存储在笔记中，用于提供高质量、全面的回答。</li>
      </ul>
    </li>
  </ol>

  <p>总结来说，active retrieval 是一种动态的、基于反馈的检索触发机制，而 note-based stop-exploration 是一种通过笔记积累和评估来确定停止检索的策略，更侧重于避免检索过度并确保知识整合的全面性。</p>

  <p>疑问二：<strong>the timing bias of active retrieval predictions是什么意思？</strong></p>

  <p>模型在回答过程中会主动判断何时需要检索额外信息，但由于模型对检索需求的预测可能不准确，可能会导致以下两种情况：</p>

  <ol>
    <li><strong>过早或过晚检索</strong>：模型可能在不需要时启动检索，或在需要时却未能及时检索，导致检索结果的时机与实际信息需求不匹配。</li>
    <li><strong>知识缺失</strong>：错过关键检索时机会造成知识收集不充分，影响最终回答的质量。</li>
  </ol>

</blockquote>

<p>用自适应基于note的停止搜索策略（note-base stop-exploration strategy）替换不确定的主动检索</p>

<p>具体：</p>

<ol>
  <li>它确定最佳笔记作为最佳记忆，并用它来决定“什么时候检索以及何时停止”</li>
  <li>我们的自适应策略允许首先进行贪婪勘探。如果在特定时刻，笔记不再获得知识增益，则信息收集停止。</li>
</ol>

<p>好处：这种策略有效地避免了主动检索预测的时机偏见，并确保持续的知识增益</p>

<h3 id="实验结果">实验&amp;结果</h3>

<blockquote>
  <p>零样本就可，无需少样本训练是怎么实现的？</p>
</blockquote>

<h4 id="实验">实验</h4>

<p>我们在五个复杂的QA数据集上进行了大量实验，涉及总体性能比较、定量消融研究和参数分析。结果突出了Adaptive-Note的优越性、有效性和普适性，同时确认了核心组件的有效性。</p>

<h4 id="贡献">贡献</h4>

<ol>
  <li><strong>模型：</strong>我们探索了LLMs处理复杂QA任务的能力，并首次提出了一种基于新范式（Retrieverand-Memory）的称为Adaptive-Note的方法。大量实验结果表明，Adaptive-Note在五个复杂QA数据集上显著超越现有方法，改进达到了8.8%。</li>
  <li><strong>策略：</strong>我们引入了一种基于笔记的自适应知识探索策略，通过知识增长的视角加强。因此，Adaptive-Note可以持续收集相关知识并自适应地记忆最优知识，产生高质量答案。</li>
  <li><strong>设计实现：</strong>我们将Adaptive-Note设计为一种通用且即插即用的方法，可以轻松适应任何现有的LLMs而无需额外训练，而且它可以在仅仅零样本设置下取得出色的性能，而无需使用精心设计的少样本示例。</li>
</ol>

<h2 id="相关工作">相关工作</h2>

<h3 id="openqa">OpenQA</h3>

<p>开放域问答（OpenQA）。开放域问答（OpenQA）（Voorhees 1999）旨在利用大规模和非结构化信息以自然语言形式回答与领域无关的问题（Zhu et al. 2021）。</p>

<p>现代开放QA任务的方法通常遵循Retriever和Reader范式（Chen et al. 2017; Das et al. 2019; Wang et al. 2024）。</p>

<h4 id="retriever">Retriever</h4>

<p>“Retriever”负责根据给定的问题检索相关文档，主流方法包括：</p>

<ol>
  <li>基于稀疏向量的检索，如TFIDF和BM25；</li>
  <li>最近开发的基于密集向量的检索，如DPR（Karpukhin等人，2020）和Contriever（Izacard等人，2022）。</li>
</ol>

<h4 id="reader">Reader</h4>

<p>Reader致力于理解检索到的段落，然后推断出正确答案。</p>

<p>过去几年，Reader广泛采用了Fine-tuned预训练模型，建立在基于transformer的体系结构之上。</p>

<p>最近，<strong>LLMs涌现的能力-无需微调</strong>，<strong>基于提示的上下文学习</strong>（ICL）（Brown等人，2020）。这使得LLMs仅仅通过提供少量演示样本便能在OpenQA任务中取得极具竞争力的表现。</p>

<p>我们的工作主要集中在利用LLMs解决OpenQA任务，并提出了一种新颖有效的范式，即Retriever-and-Memory。</p>

<h3 id="rag">RAG</h3>

<p>这段内容讨论了<strong>检索增强生成（RAG）</strong>技术的演变及其在复杂问答任务中的应用和挑战，并引入了<strong>自适应检索增强生成（ARAG）</strong>概念作为改进。具体解释如下：</p>

<ol>
  <li><strong>传统RAG方法及其局限</strong>：
    <ul>
      <li>传统的RAG方法基于“检索器-阅读器”结构，检索器（Retriever）从外部知识库中检索到与问题相关的段落，然后将这些段落输入到语言模型（LLM）中生成答案。</li>
      <li>早期的方法是单次检索（single-time RAG），即一次性检索所需的段落并直接生成答案。但在复杂的问答任务（如multi-hop QA和long-form QA）中，单次检索往往不足以获取完整的信息，导致回答内容不充分。</li>
    </ul>
  </li>
  <li><strong>多次检索（multi-time RAG）方法的尝试和问题</strong>：
    <ul>
      <li>为了解决单次检索信息不足的问题，一些方法（如Trivedi et al. 2023；Borgeaud et al. 2022）在生成过程中尝试多次检索，即在生成答案时连续检索新的信息。</li>
      <li>但是，多次检索可能导致模型不加区分地持续获取信息，这样如果某个检索步骤引入了错误或不相关的信息，会导致低质量的回答。</li>
    </ul>
  </li>
  <li><strong>自适应RAG（ARAG）方法的提出</strong>：
    <ul>
      <li>为了解决上述问题，提出了自适应RAG（ARAG）方法。ARAG方法通过不同的反馈机制自动决定“何时检索、检索什么”，以更加精准地满足问题的知识需求。</li>
      <li>目前主流的ARAG方法包括<strong>Flare</strong>（Jiang et al. 2023）和<strong>Self-RAG</strong>（Asai et al. 2024）。例如，Flare方法在生成过程中，如果检测到低置信度的词汇（即不确定的词），则会触发检索；但这种置信度判断可能无法完全反映实际检索需求。Self-RAG方法则通过自反性（self-reflective）的标记来判断是否需要检索及评估检索内容的质量，但这对LLM生成的自反性输出准确性要求较高。</li>
    </ul>
  </li>
  <li><strong>本文提出的解决办法</strong>：
    <ul>
      <li>由于上述ARAG方法在时机判断上的局限性，作者的目标是从“知识增长”的角度建立一个更高效、稳定的自适应RAG框架，以更加合理地控制检索过程，提高回答质量。</li>
    </ul>
  </li>
</ol>

<h2 id="方法">方法</h2>

<blockquote>

  <p>最佳记忆是什么意思？Mopt</p>

  <p>AMR怎么触发的终止条件？</p>
</blockquote>

<p>模型分为三部分：一个迭代信息收集器，一个自适应记忆审阅者和一个面向任务的生成器。</p>

<h3 id="总体工作流程">总体工作流程</h3>

<h4 id="初始化阶段">初始化阶段</h4>

<ol>
  <li>输入问题q，然后在IIC中进行检索，得到前k个段落p</li>
  <li>然后k个段落输入到LLm中生成一个初始note$N_0$</li>
  <li>$N_0$作为初始记忆$M_0$，视为$M_{opt}$</li>
</ol>

<h4 id="迭代阶段">迭代阶段</h4>

<ol>
  <li>利用当前的$M_{opt}$和原始查询q生成第t步时的新查询$q_t$</li>
  <li>然后检索新的相关文章$p_t^i \in P_t$ 然后作为现在当前的状态$N_t$</li>
  <li>然后AMR 对$N_t$和$M_{opt}$进行多维度评估得到一个二进制结果
    <ol>
      <li>如果为true表示$N_t$应该替换记忆的内容</li>
      <li>反之就不用</li>
    </ol>
  </li>
  <li>直到AMR触发迭代的终止条件。</li>
</ol>

<h4 id="最终阶段">最终阶段</h4>

<p>使用面向任务的生成器，用$M_{opt}$作为上下文输入，通过LLM zero-shot 上下文学习（ICL）输出最终答案$a\in A$</p>

<h3 id="模块1iic模块">模块1：IIC模块</h3>

<p><img src="/assets/posts_assets/Retriever-and-Memory%20Towards%20Adaptive%20%20Note-Enhanced%20Retrieval-Augmented%20Generation%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.assets/image-20241102171606149.png" alt="image-20241102171606149" /></p>

<h4 id="阶段一note初始化">阶段一：Note初始化</h4>

<ol>
  <li>
    <p>将q作为输入给BM25得到k个文章 $P_0$</p>
  </li>
  <li>
    <p>然后用LLM生成note $N_0$并且作为$M_0$直接赋给$M_{opt}$,将被用作后续迭代信息收集阶段的输入</p>

    <p><strong>(作者没有对笔记的具体格式或特定信息方面施加限制，并且使用的是zero-shot的LLM)</strong>
\(N_{0}=\mathrm{LLM}(prompt_{init}(q,P_{0}))
\\N_{0}=M_{0}\rightarrow M_{opt}\)</p>
  </li>
</ol>

<h4 id="阶段二迭代的信息检索">阶段二：迭代的信息检索</h4>

<p>我们利用最佳记忆来预测下一个查询。</p>

<blockquote>
  <p>基于现有信息准确预测下一个高质量查询对有效探索语料库未知语义空间至关重要。</p>

  <p>类似于人类行为，我们根据当前的最佳理解提出新问题。</p>

  <p>这些promt都是什么？</p>

  <p>怎么判断Nt相对于Mopt是否具有新的或额外的非冗余信息</p>
</blockquote>

<p>输入：原始查询q，$M_{opt}$ 和已经问过的问题列表（防止重复问）</p>

<p>生成下一步的query
\(q_{t}=\mathrm{LLM}(prompt_{q}(q,M_{opt},Q_{ask})),q_{t}\notin Q_{ask}\\Q_{ask}=\{q_{1},q_{2},\ldots,q_{t-1}\}\)
$prompt_q$表示用于产生下一个检索的提示模板。已提问的查询列表$Q_{ask}$包含所有先前生成的查询。</p>

<h4 id="阶段三更新note阶段">阶段三：更新note阶段</h4>

<p>目标：利用新的查询来探索语料库中潜在的与查询相关的语义子空间，并将新检索到的段落无缝地整合到现有知识中。</p>

<h5 id="怎么设定prompt">怎么设定prompt？</h5>

<p><strong>无固定模式</strong>：与笔记初始化阶段类似，在整合新知识时，不会预定义特定的模式（如简单地附加或插入信息），以避免限制大语言模型（LLM）的多样性和灵活性。这意味着新信息的整合过程是开放的，可以根据实际需求灵活调整。</p>

<p><strong>信息增长的视角</strong>：该方法的核心在于如何有效促进信息的增长，而不是强制遵循固定的结构。这种灵活性允许模型在处理复杂信息时，能够收集、提炼和更新知识，而无需严格的格式限制。</p>

<h5 id="流程">流程</h5>

<ol>
  <li>我们首先使用新查询$q_t$检索k个新段$P_t$。</li>
  <li>接下来，我们将q、$Pt$和$M_{opt}$输入到LLM中，采用zero-shot上下文学习（ICL），最终生成更新的笔$N_t$。</li>
</ol>

\[N_{t}=\mathrm{LLM}(prompt_{update}(q,P_{t},M_{opt}))\]

<p>$prompt_{update}(·)$表示用于更新笔记的提示模板，在三个关键方面为LLM提供多维指导：</p>

<ol>
  <li>忠实性。所收集的信息应符合检索文档的风格，尽可能地使用原始摘录；</li>
  <li>有效性。所收集的信息应有助于解决原始查询；</li>
  <li>可扩展性。所收集的信息不应已包含在内存中；换句话说，只需添加新的语义内容。我们还提供了信息增长的观点来描述这一更新过程，即基于现有的$M_{opt}$知识收集新信息，生成更新的$Nt$。</li>
</ol>

\[N_t=M_{opt}\cup\left(\bigcup_{i=1}^{k}p_{t}^{i}\right)\]

<p>这里，$M_{opt}$是通过逐步更新得出的，逐渐比较和选择最高质量的笔记作为记忆，而不是依赖于一次性的比较。</p>

<h3 id="模块2amr模块">模块2：AMR模块</h3>

<p>自适应记忆审核者专注于两个关键问题：“$M_{opt}$是什么”和“何时停止检索”。</p>

<h4 id="m_opt是什么样子的">$M_{opt}$是什么样子的</h4>

<p>我们指导LLM仔细审查在笔记更新阶段生成的更新笔记$N_t$的内容和最佳记忆$M_{opt}$的内容，然后比较它们的内容质量。</p>

<p>如果$N_t$的质量高于$M_{opt}$，则$N_t$将替换原始记忆内容成为最新的最佳记忆。</p>

<p>否则，$M_{opt}$的内容保持不变。
\(f_{c}(N_{t},M_{opt})=\begin{cases}\mathrm{True},&amp;\mathrm{if~}N_{t}\mathrm{~is~better~than~}M_{opt}\\\mathrm{False},&amp;\mathrm{if~}M_{opt}\mathrm{~is~better~than~}N_{t}\end{cases}\\ \\
N_{t}\to M_{opt}, \mathrm{if} f_{c}(N_{t},M_{opt})=\mathrm{True}\)
其中，$f_c(.)$ 表示内容质量比较函数，→ 表示“存储在”。</p>

<ol>
  <li>
    <p>首先判断Nt相对于Mopt是否具有新的或额外的非冗余信息。</p>
  </li>
  <li>
    <p>此外，为了进行多维度比较，我们在$f_c(.)$的prompt中使用zero-shot 设置制定质量比较的评价标准。</p>

    <p>评价标准包括：</p>

    <ol>
      <li>内容是否包含与问题直接相关的关键信息</li>
      <li>内容是否具有多个方面</li>
      <li>内容是否包含充分的细节</li>
      <li>内容是否足够实用。</li>
    </ol>
  </li>
</ol>

<h4 id="什么时候停">什么时候停?</h4>

<p>我们建立了三个停止条件来自适应地控制笔记更新，从而间接控制信息收集。</p>

<h5 id="三种停止条件">三种停止条件：</h5>

<p>条件 1：信息更新质量不佳</p>

<ul>
  <li><strong>描述</strong>：如果更新后的笔记质量未能超过当前最佳记忆 $M_{\text{opt}} $的质量，则视为一次无效的信息收集。</li>
  <li><strong>无效更新计数（IU）</strong>：设定一个无效更新计数阈值 $ T_{\text{IU}} $，即允许的最大无效信息收集轮数。如果无效更新次数达到此阈值，则停止迭代。</li>
</ul>

<p>条件 2：信息收集步数过多</p>

<ul>
  <li><strong>描述</strong>：某些查询可能涉及大量细节性信息（“长尾知识”），这些信息在多次迭代后会不断增加到笔记中。然而，这些细节可能对回答原始问题并无帮助。</li>
  <li><strong>收集迭代计数（CI）</strong>：设定一个最大信息收集步数阈值 $ T_{\text{CI}} $，即允许的最大信息收集步骤数。如果迭代步数达到此阈值，则停止迭代。</li>
</ul>

<p>条件 3：检索的段落数过多</p>

<ul>
  <li><strong>描述</strong>：如果检索的段落数持续增加，但信息对回答原始问题没有显著帮助，也会导致信息冗余。</li>
  <li><strong>检索段落计数（RP）</strong>：设定一个最大去重检索段落数阈值 $ T_{\text{RP}} $，即最多允许去重后的段落数。如果检索的段落数达到此阈值，则停止迭代。</li>
</ul>

<h5 id="停止机制">停止机制：</h5>

<ul>
  <li>这三个阈值（$ T_{\text{IU}} $、$ T_{\text{CI}} $ 和 $ T_{\text{RP}} $）作为迭代停止条件。如果满足其中任一条件，即会触发终止机制，停止信息收集和更新过程。</li>
</ul>

\[\langle\mathbf{Stop}\rangle=\mathbf{1}\left\{IU\geq T_{IU}\vee CI\geq T_{CI}\vee RP\geq T_{RP}\right\}\]

<ul>
  <li>停止符号⟨Stop⟩是一个布尔值，表示是否停止迭代过程。符号≥表示满足任何触发条件都会返回1。</li>
</ul>

<h3 id="模块3面向任务的生成器">模块3：面向任务的生成器</h3>

<p>任务导向型生成器读取最佳内存$M_{opt}$，并输出原始查询q的答案α。</p>

<p>由于不同QA任务的输出风格（例如长或短的生成），我们将prompt定制为任务导向型。</p>

<blockquote>
  <p>例如，多跳QA任务需要简短精确的输出，通常只有几个词，而我们内存中的知识则呈现为长文本。因此，我们指导LLM仅输出关键答案，不包括多余的词语。相比之下，对于长篇QA任务，我们指导响应风格而非严格的限制。</p>
</blockquote>

\[\alpha=\mathrm{LLM}(prompt_{g}(q,M_{opt}))\]

<p>$prompt_g(·) $代表任务导向生成器的promt模板集合</p>

<h2 id="实验-1">实验</h2>

<h3 id="数据集和评估指标">数据集和评估指标</h3>

<ul>
  <li>
    <p><strong>Multi-hop QA任务</strong>：选择了三个具有挑战性的英语数据集进行评估，即 HotpotQA、2WikiMultiHopQA（2WikiMQA）和 MuSiQue。每个数据集使用了Trivedi等（2023）发布的一个包含500个随机选择样本的子集。评估指标包括F1分数（F1）和精确匹配（EM），用于衡量答案的准确性和完整性。</p>
  </li>
  <li>
    <p><strong>Long-form QA任务</strong>：选择了一个英语数据集ASQA和一个中文数据集CRUD。ASQA数据集使用ALCE（Gao等，2023）重新编译的948个查询，并使用其官方的评估指标，即字符串精确匹配（str-em）和字符串命中率（str-hit）。对于CRUD数据集，只使用其QA任务的数据集（包含单文档和多文档QA样本），并采用CRUD提出的RAGQuestEval评估指标，包括问题级别的召回率（Q-R）、标记级别的F1（T-F1）、召回率（T-R）和精确率（T-P）。</p>
  </li>
</ul>

<h3 id="基线模型和llms">基线模型和LLMs</h3>

<ul>
  <li>比较了三种基线模型：
    <ol>
      <li><strong>No Retrieval (NoR)</strong>：直接将查询输入到LLM中生成答案，不进行任何检索过程。</li>
      <li><strong>Single-time RAG (STRAG)</strong>：一次性检索知识用于回答原始查询。</li>
      <li><strong>Adaptive RAG (ARAG)</strong>：采用自适应的前向探索策略，逐步检索知识以提升答案质量。</li>
    </ol>
  </li>
  <li>使用GPT-3.5作为NoR的内置LLM。对于STRAG，选择了Vanilla RAG、Chain-of-note、Self-refine和Self-rerank作为对比模型。ARAG中包含FLARE、Self-RAG和ReAct三种方法进行比较。具体实现方面，Self-RAG和ReAct使用了Langchain框架。此外，还进行了多种LLM的实验，包括GPT-3.5-turbo-0125、Qwen2-7b和Llama3-8b，默认情况下使用GPT-3.5作为模型。</li>
</ul>

<h3 id="检索器和语料库">检索器和语料库</h3>

<ul>
  <li>为了确保所有基线模型之间的公平比较，在每个数据集中对检索器和语料库进行了统一设置。</li>
  <li>对于所有Multi-hop数据集，使用BM25作为检索器（通过Elasticsearch实现），语料库为Trivedi等（2023）发布的数据集对应语料。</li>
  <li>对于ASQA，使用密集检索器GTR-XXL，并使用ALCE提供的语料库（基于2018年12月20日的Wikipedia快照，分割为100词的段落）。</li>
  <li>对于CRUD，使用了包含80000个中文新闻文档的语料库，遵循CRUD的检索配置（块大小为512，top-k为2）。除CRUD外，其他数据集的默认top-k为5。</li>
</ul>

<h3 id="提示的设置">提示的设置</h3>

<ul>
  <li>实验方法在零-shot的设置下进行所有LLM推理。由于不同数据集需要不同的输出格式，因此为每个数据集设计了相应的提示（prompt）。为确保所有方法在同一数据集上的公平性，统一了生成时的提示。</li>
  <li>对于三个具有相似输出格式的Multi-hop QA数据集，设计了相同的提示，引导LLM基于给定内容直接生成答案，减少不必要的词语。对于ASQA和CRUD，遵循作者提供的提示配置，去掉了ALCE的引用输出功能和CRUD的few-shot设置。</li>
</ul>

<h3 id="自适应过程的设置">自适应过程的设置</h3>

<ul>
  <li>在自适应过程中，为了在较低预算内达到最佳效果，将检索段落数量（RP）限制为15。默认情况下，收集器迭代计数（CI）和无效更新计数（IU）分别设置为3和1。</li>
</ul>

<h2 id="结果与分析">结果与分析</h2>

<p><img src="/assets/posts_assets/Retriever-and-Memory%20Towards%20Adaptive%20%20Note-Enhanced%20Retrieval-Augmented%20Generation%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.assets/image-20241102233616465.png" alt="image-20241102233616465" /></p>

<h3 id="1-主要结果">1. <strong>主要结果</strong></h3>
<ul>
  <li><strong>与单次检索RAG方法的比较</strong>：使用GPT-3.5作为所有方法的内置LLM，实验结果显示，所提出的方法显著优于单次检索方法。特别是在2WikiMQA数据集上，相比Vanilla RAG，性能提升达12.2%。这是因为单次检索方法依赖于单次检索的质量，而Adaptive-Note方法则可以在语料库中自适应地探索更多知识，提升答案质量。</li>
  <li><strong>与其他自适应RAG方法的比较</strong>：与FLARE、Self-RAG和ReAct等自适应RAG方法相比，Adaptive-Note在多跳QA和长篇QA任务上持续取得最高表现。这是因为Adaptive-Note方法采用了贪婪的信息收集策略，充分整合每个检索到的段落，避免了忽略关键知识的问题。</li>
</ul>

<h3 id="2-不同llm上的表现">2. <strong>不同LLM上的表现</strong></h3>
<ul>
  <li>为验证方法的通用性，作者在GPT-3.5、Qwen2-7b和Llama3-8b上进行了实验，结果表明Adaptive-Note在所有模型上均显著优于Vanilla RAG，验证了方法的鲁棒性和通用性。</li>
</ul>

<h3 id="3-公平top-k下的深入比较">3. <strong>公平top-k下的深入比较</strong></h3>

<p>Vanilla RAG 在公平的 top-k 设置下的整体性能。</p>

<p><img src="/assets/posts_assets/Retriever-and-Memory%20Towards%20Adaptive%20%20Note-Enhanced%20Retrieval-Augmented%20Generation%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.assets/image-20241104150516311.png" alt="image-20241104150516311" /></p>

<p>由于自适应RAG方法（如Adaptive-Note）在检索过程中会进行多次、动态的检索，可能最终获取的信息量比一次性检索的RAG（如Vanilla RAG）要多，直接比较两者的检索结果会存在不公平性。因此，为了进行公平比较，作者引入了“公平top-k”的概念。</p>

<ul>
  <li>为了在相同top-k设置下进行更公平的性能比较，作者计算了自适应检索步骤中去重后的平均检索段落数量作为“公平top-k”。结果显示，在所有数据集上，Adaptive-Note在公平top-k设置下的表现仍然优于Vanilla RAG。</li>
</ul>

<h3 id="4-消融实验">4. <strong>消融实验</strong></h3>
<ul>
  <li><strong>迭代信息收集器（IIC）的有效性</strong>：实验表明，仅使用IIC模块就能提升性能，尤其在多跳QA任务中效果更明显，因为该模块能生成基于前序知识的新查询，有助于多跳推理。</li>
  <li><strong>自适应记忆审查器（AMR）的有效性</strong>：与不使用AMR模块相比，加入AMR可以进一步提高性能，因为AMR模块能够在高质量笔记生成后提前停止检索，减少不相关噪音。</li>
</ul>

<h3 id="5-参数分析">5. <strong>参数分析</strong></h3>
<ul>
  <li><strong>CI和IU的影响</strong>：在固定IU为1的情况下，CI增加时，多跳QA数据集的性能提升；对于长篇QA任务，CI=2已能达到最佳性能。</li>
  <li><strong>top-k的影响</strong>：在不同top-k设置下，Adaptive-Note始终优于Vanilla RAG，验证了其在不同检索段落数量下的稳定性。</li>
</ul>

<p>总结来说，Adaptive-Note在复杂QA任务中展现了显著的性能优势和良好的通用性，通过有效的信息收集和整合策略，提升了答案的准确性和覆盖度。</p>]]></content><author><name>Xingjie Gao</name><email>xingjie-gao@outlook.com</email></author><category term="nlp" /><category term="自然语言处理" /><category term="论文笔记" /><summary type="html"><![CDATA[Retriever-and-Memory: Towards Adaptive Note-Enhanced Retrieval-Augmented Generation论文笔记]]></summary></entry><entry><title type="html">WebCPM 论文精读</title><link href="http://localhost:4000/2024/10/28/WEBCPM-Interactive-Web-Search%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.html" rel="alternate" type="text/html" title="WebCPM 论文精读" /><published>2024-10-28T00:00:00+08:00</published><updated>2024-10-28T00:00:00+08:00</updated><id>http://localhost:4000/2024/10/28/WEBCPM%20Interactive%20Web%20Search%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0</id><content type="html" xml:base="http://localhost:4000/2024/10/28/WEBCPM-Interactive-Web-Search%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.html"><![CDATA[<h1 id="webcpm-interactive-web-search-for-chinese-long-form-question-answering-论文笔记">WEBCPM: Interactive Web Search for Chinese Long-form Question Answering 论文笔记</h1>

<p><!---more--></p>

<blockquote>
  <p>论文来源：老师发的</p>

</blockquote>

<h2 id="摘要">摘要</h2>

<h3 id="背景"><strong>背景：</strong></h3>

<ul>
  <li>长格式问题回答（LFQA）：用详细的段落长度的回答来回答复杂的开放性问题。
    <ul>
      <li>LFQA的事实范式需要两个步骤：信息检索，搜索相关支持事实，信息综合，将这些事实整合成连贯的答案。</li>
    </ul>
  </li>
</ul>

<h3 id="工作"><strong>工作：</strong></h3>

<ul>
  <li>我们介绍了WebCPM，这是第一个中文LFQA数据集。
    <ul>
      <li>WebCPM的一个独特特点是，它的信息检索基于实时交互式网络搜索，与搜索引擎进行互动。</li>
      <li>数据来源：在WebGPT的基础上，我们开发了一个网络搜索界面。我们招募注释员使用我们的界面进行相关信息的搜索，然后回答问题。同时，我们会记录注释员的网络搜索行为。总共，我们收集了5500个高质量的问题-回答对，以及15372条支持性事实和125954个网络搜索操作。</li>
    </ul>
  </li>
  <li>
    <p>我们对预训练语言模型进行微调，以模仿人类行为进行网络搜索，并生成基于收集到的事实的答案。</p>

    <p>结果：我们的LFQA流程是基于这些经过微调的模型构建的，在我们的数据集和DuReader（He等人，2018）中的案例分别在32.5%和47.5%的情况下生成的答案不比人类编写的答案差。</p>
  </li>
</ul>

<h2 id="引言">引言</h2>

<h3 id="lfqa">LFQA：</h3>

<ul>
  <li>长篇问题回答（LFQA）（Fan等，2019）的目标是用详细的段落长度回答复杂的开放式问题。</li>
  <li>当前的LFQA解决方案通常遵循检索-综合范式，包括两个核心要素：信息检索和信息综合。</li>
  <li>前者搜索外部知识源（例如网络）以获取各种相关支持事实，后者将收集到的事实整合到一致的答案中。</li>
</ul>

<p>缺点：传统LFQA范式的一个缺陷是通常倾向于使用非交互式检索方法，即使用原问题作为查询来检索一大堆未筛选的信息。</p>

<p>改进：人类能够通过与搜索引擎实时互动进行交互式网络搜索。</p>

<ul>
  <li>对于复杂问题，人类往往会将其分解成多个子问题，并按顺序提问。通过识别和浏览相关信息，人类可以改善对主题的理解，并通过提出后续问题或相关术语来完善他们的搜索。</li>
  <li>这一迭代过程能够扩大他们的搜索范围并提升他们的搜索结果。<strong>总的来说，交互式网络搜索不仅提供了多样的信息来源，而且反映了人类解答问题的认知过程，从而提高了可解释性。</strong></li>
</ul>

<h3 id="webgpt">WebGPT</h3>

<p>关于交互式网络搜索大模型的研究WebGPT：</p>

<ul>
  <li>优点：在实验中，WebGPT展现出了优异的LFQA能力，甚至超过了人类专家。</li>
  <li>问题：尽管其性能令人印象深刻，但WebGPT对于社区仍然是个谜。这是因为WebGPT的界面、数据集和训练模型并不公开，其核心设计元素的内部运作仍然不透明。这些因素使得社区很难理解LFQA互动网络搜索的挑战，并继续探索这一研究领域。</li>
</ul>

<h3 id="所做的工作">所做的工作</h3>

<ol>
  <li>
    <p>构建了一个接口</p>

    <p>用户可以执行预定义的操作来进行多轮搜索和浏览。当在网页上找到相关信息时，他们可以将其记录为支持事实。同时，他们的网络浏览行为将被记录下来。收集到足够的信息后，用户可以完成网络搜索并基于所收集的事实回答问题。</p>
  </li>
  <li>
    <p>构建数据集WebCPM</p>

    <p>基于界面，我们选择中文作为测试平台，并构建WebCPM，重点关注具有中文预训练模型的交互式Web搜索。WebCPM是第一个涉及交互式网络搜索的公共QA数据集，也是针对中文LFQA的第一个数据集。WebCPM包含5,500个问答对，以及15,372个支持事实和125,954个网络搜索操作。表1总结了WebCPM与相关QA数据集之间的差异。在现有的中文QA数据集中，WebCPM具有最长的问题、支持事实和答案，显示出问题的复杂性和答案的丰富性。</p>
  </li>
</ol>

<p><img src="/assets/posts_assets/WEBCPM%20Interactive%20Web%20Search%20for%20Chinese%20Long-form%20Question%20Answering%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.assets/image-20241023162941047.png" alt="image-20241023162941047" /></p>

<ol>
  <li>构建模型，提出模型架构
    <ol>
      <li>一个搜索模型，模拟人类网络搜索行为进行信息检索。具体来说，搜索模型包括三个模块，在我们的界面上执行一系列预定义的操作：动作预测模块、搜索查询生成模块和支持事实提取模块；</li>
      <li>一个综合模型，根据收集到的事实生成连贯的答案。</li>
    </ol>
  </li>
</ol>

<h4 id="结果">结果</h4>

<p>选择了8个代表性的预训练语言模型（PLMs），参数规模高达10B，并评估它们在交互式网络搜索和信息合成方面的能力。</p>

<ul>
  <li>人类评估显示，我们的管道在测试集上的答案生成比人类差的情况占32.5％。当应用于问题的答案标注长度超过400个汉字的DuReader（He等，2018）时，我们的管道在47.5％的情况下生成比黄金标注答案更好的答案，表现出令人满意的超出分布的泛化性能。</li>
  <li>我们还展示了我们的搜索模型超越了传统的非交互检索方法。</li>
</ul>

<p>最后，我们分析了我们的框架的核心设计元素以及我们的模型获得的类人行为的贡献。我们希望这些资源可以作为其他研究主题的试验平台，比如行为克隆（Bain和Sammut，1995）和工具学习（Qin等，2023）。</p>

<h2 id="相关工作">相关工作</h2>

<h3 id="信息检索">信息检索</h3>

<ul>
  <li>
    <p>以往的工作通常借助本地存储库（例如，维基百科）。最近，利用整个网络作为知识源的兴趣激增</p>
  </li>
  <li>
    <p>如何将检索到的事实结构化为LFQA中合理且细致的答案仍然未被深入探索。</p>

    <p>一些研究了人类如何构建复杂答案，通过研究长篇答案的功能结构（徐等，2022年）或探索如何在答案中组织举例（王等，2022年）; 其他人重新考虑了LFQA的现有评估标准（Krishna等，2021年）。</p>
  </li>
</ul>

<h3 id="与webgpt的比较">与WebGPT的比较</h3>

<p>我们在很大程度上遵循了WebGPT，并提出了改进的设计元素（详细内容见附录E），包括</p>

<ul>
  <li>接口：我们修改了WebGPT定义的动作，使其更易于模型学习和更用户友好；</li>
  <li>框架：我们将网络搜索分解为3个子任务，并实现了一个模块化的搜索模型。我们还探讨了如何教导合成模型忽略无关的事实（§6.3）并生成新内容（附录F.1）；</li>
  <li>评估和分析：除了按照WebGPT评估整个流水线（§6.2）之外，我们还评估了每个单独的模块（§6.1和§6.3）。这种细粒度的评估有助于我们更好地了解我们框架的核心设计元素和模型学习的人类行为所做出的贡献。</li>
</ul>

<h3 id="工具学习">工具学习</h3>

<p>PLM具有有希望的工具操作能力，即工具学习（Qin等，2023年）。</p>

<p>PLM可以在复杂的交互环境中做出顺序决策，例如在机器人任务规划（Huang等，2022a; Ahn等，2022; Huang等，2022b）、搜索引擎操作（Nakano等，2021）、在电子商务网站上购物（Yao等，2022）等方面。</p>

<p>通过利用预训练期间学到的丰富世界知识，PLM能执行扎根行动与真实世界进行互动。</p>

<p>我们期望我们的基准能够成为未来在这一领域进行探索的试验田。</p>

<h2 id="web-search">Web Search</h2>

<p>我们搭建了一个纯文本界面，用于记录人类在长篇问题中收集相关信息时的网络搜索行为。我们的界面由Bing搜索API支持，支持如图1所示的10种主流网络搜索操作。当执行某个操作时，我们的界面会响应窗口的变化。</p>

<ul>
  <li>
    <p>当执行搜索操作时，界面进入搜索模式（图1），显示了Bing针对特定查询<query>推荐的链接。</query></p>
  </li>
  <li>
    <p>每个链接由标题和特定网页的简要快照组成。</p>
  </li>
  <li>
    <p>每个窗口一次显示三个链接，可以通过执行向下滚动操作访问更多链接。</p>
  </li>
  <li>
    <p>当在当前窗口中找到第i个链接与相关时，用户可以执行加载页面 <i> 操作（i ∈ {1, 2, 3}）。</i></p>
  </li>
  <li>
    <p>界面将进入浏览模式（见附录的图6）并呈现从第<i>个网页的HTML中清除的文本。</i></p>

    <p>窗口中用户一次可以查看的内容限制在500个中文字符，可以使用滚动操作访问更多内容。用户可以利用引用操作提取当前窗口中的连续句子作为支持事实。</p>
  </li>
  <li>
    <p>为了提取跨两个窗口的文字，合并操作旨在将最后两个事实合并为一个事实（更多细节见附录A.2）。我们还为用户显示所有已提取的支持事实。</p>
  </li>
  <li>
    <p>浏览第i页后，用户可以使用“返回”动作返回到先前的搜索模式，以访问其他链接。同时</p>
  </li>
</ul>

<h2 id="数据收集">数据收集</h2>

<p>我们雇用了来自不同领域，有搜索引擎操作经验的23名注释员。我们要求他们首先利用我们的界面搜索相关信息，然后撰写细致的答案来回答长篇问题。为了质量控制，我们招募了8名熟悉QA研究领域的专家作为质量检查员。</p>

<h3 id="问题构建">问题构建</h3>

<ul>
  <li>从头开始创作新的长篇问题而没有任何参考是事倍功半的，因此我们转向公共问答论坛作为问题的来源。</li>
  <li>我们请批注者参考一个名为Reddit的英文问答论坛上的问题，并创造用中文书写的新问题。</li>
</ul>

<p>结果：通过这种方式创作的问题通常需要多轮搜索和浏览来收集足够的信息。</p>

<h3 id="交互式网络搜索">交互式网络搜索</h3>

<ul>
  <li>
    <p>给定一个问题，我们要求标注者使用我们的界面从可信的来源搜索准确和相关的信息。这个过程可能涉及多次向必应发送精炼的查询，以及探索他们认为相关的各种网页。</p>

    <p>我们要求标注者在提取信息之前仔细判断信息的事实准确性。</p>
  </li>
  <li>
    <p>搜索过程直到收集到足够的支持性事实为止。</p>
  </li>
  <li>
    <p>在我们创建的问题中，26.2%是无法回答的，最终被丢弃，因为标注者找不到足够有用的信息。</p>
  </li>
</ul>

<h3 id="答案标注">答案标注</h3>

<p>在收集到足够的支持性事实后，标注员将根据他们收集到的信息编写自包含的答案。</p>

<p>我们为他们提供答案标注的指导，包括写与问题相关且内容丰富的答案，保持逻辑一致性、清晰度和连贯性，并以公正的方式提供观点。</p>

<h3 id="质量控制">质量控制</h3>

<p>每个带注释的实例在被选入最终数据集之前都要经过质量检查员的检查和批准。</p>

<ul>
  <li>首先，检查员会手动检查在接口上记录的操作序列，并丢弃质量低劣的序列（例如，在提出的查询中存在明显的行政错误的序列）。</li>
  <li>其次，他们会仔细检查收集到的支持事实。如果这些事实明显不足以回答问题，与问题无关或事实错误，相应的操作序列将被放弃。</li>
  <li>以上流程会删除25%的收集实例。</li>
  <li>对于剩下的实例，检查员会仔细检查其注释答案。如果答案与上述要求相矛盾，检查员会将其退回给注释员，并指出哪个要求没有得到满足。注释员可能会多次修改答案，直到修改后的答案符合标准为止。</li>
</ul>

<h3 id="数据统计">数据统计</h3>

<p>最终，我们收集了5,500个实例，每个实例以(question,网页搜索行为,支持事实,答案)的元组格式记载，并记录每个动作执行的观察结果。</p>

<p>我们在图2中提供了一个示例，其中我们呈现以下内容：原始问题、简化的动作序列、收集到的支持事实和标注的答案。</p>

<p>我们将数据集划分为{4,700,400,400}个训练、开发和测试集。</p>

<p>平均而言，每个问题需要执行22.9个动作，发送2.5个查询，并加载3.3个网页。每个动作的详细比例见附录的图7。</p>

<h2 id="模型结构">模型结构</h2>

<h3 id="搜索模型">搜索模型</h3>

<p>我们将网络搜索分为3个子任务：动作预测，搜索查询生成和支持事实提取。</p>

<blockquote>
  <p>每个任务都被构造为文本到文本格式，我们使用生成式PLM训练3个单独的模块。</p>
</blockquote>

<p>如果该模块预测搜索或引用，则调用其他两个模块生成查询内容或支持事实。每个模块执行的推理都受到条件的限制。</p>

<p>每个模块都是根据当前状态$S_t$进行条件推断。</p>

<blockquote>
  <p>S_t 的内容</p>

  <ul>
    <li>$Q_0$ 原始问题，$Q_t$当前查询问题 原始问题，$Q_t$当前查询</li>
    <li>$A_t = {a_1,\dots a_{t-1} }$过去动作序列</li>
    <li>窗口Wt−1和Wt中显示的上一个和当前内容，</li>
    <li>
      <table>
        <tbody>
          <tr>
            <td>当前支持事实Ft = {f1, …, f</td>
            <td>Ft</td>
            <td>}，</td>
          </tr>
        </tbody>
      </table>
    </li>
    <li>剩余动作的数量</li>
  </ul>
</blockquote>

<p>执行了一个动作，St的组成部分将会更新。</p>

<blockquote>
  <p>W可以是搜索模式中的三个链接，也可以是浏览模式中的具体页面内容。我们只保留窗口中显示的最近两个观察值（Wt−1和Wt），而不是连接所有过去的观察值，因为后者可能会超过大型语言模型的输入长度限制。</p>
</blockquote>

<h4 id="动作预测模块">动作预测模块</h4>

<p>本模块预测下一步执行的操作。</p>

<p>由于总共有10种可能的操作，因此动作预测可以看作是一个10类别的分类任务。</p>

<p>以搜索操作为例，将{x1, …, xN}表示为动作名称“搜索”的令牌化序列，其中x∗表示特定的令牌。搜索操作的概率可以分解如下：这个公式描述了在给定当前状态 $ S_t $的情况下，“Search”动作的整体概率是如何计算的。公式分解了生成“Search”标记序列的概率，具体解释如下：</p>

<p>\(\mathcal{P}(\text{Search} \mid S_t) = \mathcal{P}(x_1 \mid S_t) \times \prod_{i=2}^{N} \mathcal{P}(x_i \mid S_t, x_1, ..., x_{i-1})\)
公式分解</p>

<ol>
  <li>
\[\mathcal{P}(x_1 \mid S_t)\]
    <ul>
      <li>这个部分表示在当前状态 $ S_t $下，第一个标记 $ x_1 $出现的概率。</li>
      <li>它是生成序列的起点。</li>
    </ul>
  </li>
  <li>
\[\prod_{i=2}^{N} \mathcal{P}(x_i \mid S_t, x_1, ..., x_{i-1})\]
    <ul>
      <li>这里的乘积符号 \(\prod\) 表示从第2个标记 \(x_2\) 一直到第 $ N $个标记的条件概率的连乘积。</li>
      <li>每一个 \(\mathcal{P}(x_i \mid S_t, x_1, ..., x_{i-1})\) 表示在当前状态 \(S_t\)以及之前生成的所有标记 \(x_1, ..., x_{i-1}\)给定的情况下，第 i个标记 $x_i$出现的概率。</li>
      <li>这个部分反映了生成整个序列的过程是一步步进行的，后续的标记生成依赖于之前已经生成的标记。</li>
    </ul>
  </li>
</ol>

<p>这个公式描述的是如何在当前状态下逐步生成一个完整的“Search”动作名称的标记序列，并且最终的“Search”动作概率是由所有标记的条件概率相乘得到的。</p>

<p>在推断过程中，我们选择具有最高概率的操作来在界面上执行。</p>

<blockquote>
  <p>如何将分类任务变成了生成任务？</p>
</blockquote>

<h4 id="查询问题模块">查询问题模块</h4>

<p>根据St生成query</p>

<h4 id="事实提取模块">事实提取模块</h4>

<table>
  <tbody>
    <tr>
      <td>在 <strong>Supporting Fact Extraction</strong> 模块中，假设当前处于浏览模式，内容窗口 $ \mathcal{W}<em>t $包含了一系列词 $ {w_1, …, w</em>{</td>
      <td>\mathcal{W}_t</td>
      <td>}}$。目标是从 $ \mathcal{W}_t$ 中提取一个支持事实 $ f = {w_i, …, w_j} $，其中 $ 1 \leq i \leq j \leq</td>
      <td>\mathcal{W}_t</td>
      <td>$。</td>
    </tr>
  </tbody>
</table>

<p>模块的关键点</p>

<ol>
  <li>
    <p><strong>问题</strong>：一个简单的方案是逐步生成 $ f $的所有标记，但这种自回归的方式在实践中推理速度较慢。</p>
  </li>
  <li>
    <p><strong>替代方案</strong>：为了提高效率，该模块仅生成 $ f $的起始和末尾几个标记（记作 $ N_f $），并在给定状态 $ S_t $下最大化概率：
\(\mathcal{P}([\text{s}], w_i, ..., w_{i+N_f-1}, [\text{e}], w_{j-N_f+1}, ..., w_j \mid S_t)\)</p>
  </li>
</ol>

<p>其中，$ [\text{s}] $和 $ [\text{e}] $分别表示支持事实的起始和结束标记。</p>

<p>具体步骤</p>

<ul>
  <li>在推理过程中，当起始和结束标记被解码后，通过文本匹配可以定位到所需的序列。</li>
  <li>如果起始或结束标记出现在 $ \mathcal{W}_t $的多个位置，该模块会提取最长的序列。</li>
  <li>增大 $ N_f $可以减少这种多位置问题的频率。</li>
  <li>如果需要从 $ \mathcal{W}_t $中提取不连续的片段，则可以连续执行多个 <strong>Quote</strong> 动作。</li>
</ul>

<h3 id="综合模型">综合模型</h3>

<p>信息综合任务学习将一系列支持事实组织成连贯的答案。</p>

<h4 id="训练">训练</h4>

<p>训练有素的搜索模型偶尔会收集无关的噪音，这会影响生成答案的质量。</p>

<p>为了解决这个问题，我们通过引入噪音来破坏综合模型的训练数据中收集到的事实。</p>

<p>具体而言，给定一系列人类提取的事实{f1，…，fN}，我们随机从其他训练实例中选择几个不相关的事实{f1’，…，f’N’}。在随机排列所有事实后，我们将它们连接起来作为最终输入。</p>

<p>在训练过程中，模型被优化为在受损的支持事实条件下生成人工注释的答案，</p>

<p>即最大化</p>

<table>
  <tbody>
    <tr>
      <td>$P(Answer</td>
      <td>Q0, f1, …, fN, f1′, …, f′N′)$。</td>
    </tr>
  </tbody>
</table>

<p>由于注释的答案不包含f∗′的信息，模型学习忽略无关的事实，仅关注生成过程中的重要信息。</p>

<h2 id="实验">实验</h2>

<h3 id="模块单独评估">模块单独评估</h3>

<h4 id="t5">T5</h4>

<ul>
  <li>$mT5_{BASE}$，mC4</li>
  <li>$mT0<em>{BASE}$,  fine tunes  $mT5</em>{BASE}$ on diverse downstream tasks.</li>
  <li>$Mengzi-T5_{BASE}$ : a 220M model pre-trained on 300G internet corpora.</li>
</ul>

<h4 id="bart">BART</h4>

<ul>
  <li>$mBart_{LARGE}$ : a 680M model pre-trained on monolingual corporaa of mutiple languages.</li>
  <li>$C-BART_{LARGE}$: a 406M model pre-trained on 200G web texts</li>
</ul>

<h4 id="cpm">CPM</h4>

<ul>
  <li>$CPM_{2.6B}$</li>
  <li>$CPM_{7B}$</li>
  <li>$CPM_{10B}$</li>
</ul>

<h4 id="评估指标">评估指标</h4>

<p>对于评估指标，我们将动作预测视为一个包含10个类别的分类任务，并选择Micro-F1和Macro-F1作为指标。</p>

<p>我们将另外三个任务视为文本生成，并计算生成序列和参考答案的Rouge-L</p>

<h4 id="结果-1">结果</h4>

<p><img src="/assets/posts_assets/WEBCPM%20Interactive%20Web%20Search%20for%20Chinese%20Long-form%20Question%20Answering%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.assets/image-20241024201517303.png" alt="image-20241024201517303" /></p>

<ol>
  <li>mT0BASE在动作预测、查询生成和支持事实提取方面优于mT5BASE，但在信息综合方面表现较差。我们推测这是因为mT0BASE在多任务微调过程中更加注重与前三个任务相关的语言技能增强，而信息综合能力可能已经减弱。此外，尽管参数较少，孟子- T5BASE在所有任务上表现良好；</li>
  <li>总体而言，mBARTLARGE和C-BARTLARGE的性能都不及其他所有PLM，只是mBARTLARGE在信息综合方面表现出色；</li>
  <li>比较CPM2.6B，CPM7B和CPM10B的结果，我们发现随着模型大小的增加，性能总体上得到了改善。根据缩放定律（Kaplan et al。，2020），较大的PLM具有更强的理解和生成能力，并能实现更好的下游性能。</li>
</ol>

<h3 id="整体评估">整体评估</h3>

<h4 id="参考">参考</h4>

<p>对于WebCPM的每个测试问题，我们将注释的答案与我们综合模型生成的3种类型的答案进行比较。</p>

<p>具体来说，这3种类型的答案在支持事实的来源上有所不同，包括</p>

<ol>
  <li>
    <p>我们的搜索模型收集的事实，</p>
  </li>
  <li>
    <p>真实人类收集的事实</p>
  </li>
  <li>
    <p>使用常用的非交互式网络搜索方法收集的事实。</p>

    <p>我们直接将原问题输入到必应中，提取所有检索链接中的段落，并使用TF-IDF对它们进行排名。然后我们将排名靠前的前k个段落连接在一起，直到总字数超过3072个令牌</p>
  </li>
</ol>

<h4 id="评估方法">评估方法</h4>

<p>我们邀请8名注释者根据人类偏好手动比较不同的答案。给定一个问题和一对答案，我们要求他们进行整体评估，并根据多种因素，包括整体有用性、连贯性和与问题的相关性，决定他们更喜欢哪个答案。</p>

<p>由于所有三种检索方法使用相同的搜索引擎，它们收集的事实有时具有很高的重叠性，导致类似的答案。因此，如果两个答案质量相当，我们允许注释者将两个答案标记为等效。</p>

<h4 id="结果-2">结果</h4>

<p><img src="/assets/posts_assets/WEBCPM%20Interactive%20Web%20Search%20for%20Chinese%20Long-form%20Question%20Answering%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.assets/image-20241024211212289.png" alt="image-20241024211212289" /></p>

<ul>
  <li>我们从图4(a)的结果中得出以下结论：(1)纯粹通过我们的流程获得的答案在19.0%+13.5% = 32.5%的时间内被认为是首选或可比的。这一结果暗示着我们的流程在将来的努力中有充分的提升机会，这在附录G中进行了讨论。</li>
  <li>将我们的合成模型应用于人工收集的事实时，性能增长至16.0%+29.5% = 45.5%的优先级或等同级，这是由于收集到的事实质量的提高。</li>
  <li>通过非交互式搜索收集到的事实的性能略差（7.5%+18% = 25.5%）于我们的搜索模型。我们的搜索模型优于非交互式搜索的原因可能是：
    <ul>
      <li>(a)我们的模型多次向Bing发送不同的查询，以便检索更丰富的信息，</li>
      <li>(b)它决定一个网页是否包含重要信息的能力很关键，这要比TF-IDF效果更好。</li>
    </ul>
  </li>
</ul>

<p>接下来，我们将我们的管道（搜索模型和合成模型）应用于来自DuReader的2个中文QA数据集，即知道和搜索。尽管DuReader不是专门为LFQA设计的，但它包含各种类型的问题，我们随机抽取了400个测试问题，这些问题的注释答案超过400个中文字符。</p>

<p>对于这些问题，我们请标注员将我们的管道生成的答案与DuReader的黄金注释进行比较。从图4（b）中的结果可以看出，我们的管道在搜索和知道上有44.0%和51.0%的时间比注释优秀（平均为47.5%），显示出令人满意的分布外泛化性能。</p>

<p>同一个管道在我们的数据集上超过的人工书面答案少于DuReader也反映了我们注释答案的高质量。请注意，等价比例为0％，因为这两个答案基于完全不同的支持事实，很容易确定哪一个更好。</p>

<h3 id="其他分析">其他分析</h3>

<h4 id="消融分析1">消融分析1</h4>

<p>我们评估了通过引入无关事实来破坏综合模型的训练数据是否可以提高其忽略噪声事实的能力。我们训练一个基准模型，没有破坏训练数据，并保持其他设置与我们的模型相同。对于每个测试问题，我们将搜索模型收集的支持事实馈送给两个综合模型，并生成两个答案。评估员将评估哪个答案更相关于原始问题（允许选取等效选项）。</p>

<p>根据图4（c），通过破坏训练数据，我们的模型的性能比基准模型更好的情况占43.7％，而更差的情况占18.0％。这表明我们的方法确实增强了模型忽略嘈杂信息的能力，使生成的答案与原问题更相关。在附录F.1中，我们进一步探讨了另一种灵活平衡生成新内容和复制支持事实的破坏方法的使用。</p>

<h4 id="消融分析2">消融分析2</h4>

<p><img src="/assets/posts_assets/WEBCPM%20Interactive%20Web%20Search%20for%20Chinese%20Long-form%20Question%20Answering%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.assets/image-20241024215714230.png" alt="image-20241024215714230" /></p>

<p>St组件的影响。我们对St的几个组件进行了消融研究，以检验它们对搜索模型的每个模块是如何做出贡献的。这是通过修改每个模块的训练和评估数据来实现的。对于动作预测和支持事实提取，我们移除以下内容之一：已有的收集事实Ft，上一个窗口Wt−1中显示的内容，或者过去的动作At−1。对于查询生成，我们从St中移除以下内容：已有的收集事实Ft，已经搜索过的查询，或者以前浏览过的链接的标题。最后两个项目的信息都包含在At−1中。具体来说，对于过去的动作Search / Load Page，At−1不仅包括动作名称，还记录了具体搜索的查询 / 加载的页面的标题。</p>

<p>结果列在表3中，从中我们观察到：</p>

<ul>
  <li>(1)对于动作预测，移除Ft或Wt−1只会导致最小的性能变化，而移除At−1会导致显著的性能下降。这表明过去的动作对于动作预测来说是关键因素；</li>
  <li>(2)对于支持事实提取，只有移除Ft会显著地影响性能(-5.1)。这表明，与人类一致，该模块考虑了已提取的信息来决定下一步提取哪些信息；</li>
  <li>(3)对于查询生成，移除At−1中的已搜索查询或已访问链接的标题会造成很大的负面影响(-2.5)，这意味着该模块可能已经学会根据已搜索和新观察到的信息来生成查询。此特性是类人的，因为人类也考虑了信息，以避免发送重复的查询，并就已访问链接进行后续问题的提问。</li>
</ul>]]></content><author><name>Xingjie Gao</name><email>xingjie-gao@outlook.com</email></author><category term="nlp" /><category term="自然语言处理" /><category term="论文笔记" /><summary type="html"><![CDATA[WEBCPM: Interactive Web Search for Chinese Long-form Question Answering 论文笔记]]></summary></entry><entry><title type="html">StructRAG 论文精读</title><link href="http://localhost:4000/2024/10/21/StructRAG-Boosting-Knowledge-Intensive-Reasoning-of-LLMs-via-Inference-time-Hybrid-Information-Structurization-%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB.html" rel="alternate" type="text/html" title="StructRAG 论文精读" /><published>2024-10-21T00:00:00+08:00</published><updated>2024-10-21T00:00:00+08:00</updated><id>http://localhost:4000/2024/10/21/StructRAG%20Boosting%20Knowledge%20Intensive%20Reasoning%20of%20LLMs%20via%20Inference-time%20Hybrid%20Information%20Structurization%20%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB</id><content type="html" xml:base="http://localhost:4000/2024/10/21/StructRAG-Boosting-Knowledge-Intensive-Reasoning-of-LLMs-via-Inference-time-Hybrid-Information-Structurization-%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB.html"><![CDATA[<h1 id="structrag-boosting-knowledge-intensive-reasoning-of-llms-via-inference-time-hybrid-information-structurization-论文精读">StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization 论文精读</h1>

<!---more-->

<blockquote>
  <p>论文来源：刘老师发的</p>

  <p>方向：RAG</p>

  <p>遇到的问题：</p>

</blockquote>

<h2 id="背景知识">背景知识</h2>

<h3 id="知识密集型推理任务knowledeg--intensive-reasoning-task">知识密集型推理任务（knowledeg- intensive reasoning task）:</h3>

<p>知识密集型推理任务（knowledge-intensive reasoning task）指的是需要大量先验知识或专业领域知识来进行推理和解决的任务。在这种任务中，推理和决策需要依赖于广泛的、深入的领域知识，通常涉及多个概念和关系的复杂推理过程。
举例来说，医学诊断就是一个知识密集型推理任务的典型案例。医生在对患者进行诊断时，需要结合患者的症状、病史、实验室检查结果等信息，并借助广泛的医学知识进行推理和判断，以最终做出准确的诊断。这种任务需要医生综合运用大量的医学知识，对病情进行推理和归纳，而且通常也需要考虑患者的个体差异和复杂情况，因此属于知识密集型推理任务的范畴</p>

<h3 id="认知负荷理论">认知负荷理论</h3>

<p>认知负荷理论是由John Sweller在1988年提出的，用于解释人类认知处理和学习的理论框架。该理论指出，<strong>人脑的认知系统有着有限的处理能力，因此在学习和完成任务时会面临认知负荷的问题。</strong>认知负荷理论通过研究认知处理的本质和人类学习的方式，提出了如何最大限度地减轻认知负荷，以便更有效地进行学习和任务处理。
认知负荷理论分为三种认知负荷：</p>

<ol>
  <li>内在认知负荷：指的是完成任务所必须的认知处理和思考负担。例如，当学习新的知识或解决复杂问题时，人们会面临内在认知负荷。</li>
  <li>外在认知负荷：指的是执行任务所需要的外部支持和资源。例如，学习辅助工具、老师的讲解和指导等都可以帮助分担外在认知负荷。</li>
  <li>增量认知负荷：指的是学习者额外承担的负担，用于处理不必要的信息和过多的任务要求。
认知负荷理论的应用包括设计更有效的教学方法、界面设计、工作流程等，以减轻学习者或用户在执行任务时所面临的认知负荷，从而提高学习效果和任务执行效率。</li>
</ol>

<h3 id="hotpotqa等多跳任务">HotpotQA等多跳任务</h3>

<p>HotpotQA是一项旨在评估自然语言处理模型多跳推理（multi-hop reasoning）能力的问答任务。多跳推理指的是模型在回答问题时，需要跨越多个中间步骤，整合来自不同来源的信息，才能得出正确的答案。</p>

<p><strong>HotpotQA的主要特点：</strong></p>

<ol>
  <li>
    <p><strong>多段落推理</strong>：问题需要从多个文档或段落中提取信息，而不仅仅是从单一来源。这要求模型能够关联和综合不同信息片段。</p>
  </li>
  <li>
    <p><strong>支持证据</strong>：除了提供答案外，模型还需要指出支持答案的证据段落。这增强了模型的可解释性，便于评估其推理过程。</p>
  </li>
  <li>
    <p><strong>多样化的问题类型</strong>：包括比较类、桥接类等问题，涉及不同的推理路径和策略。</p>
  </li>
  <li>
    <p><strong>开放域问答</strong>：问题涉及广泛的主题，要求模型具备广博的知识和灵活的推理能力。</p>
  </li>
</ol>

<p><strong>其他多跳任务：</strong></p>

<p>除了HotpotQA，还有其他一些多跳推理任务和数据集，用于评估和提高模型的推理能力：</p>

<ol>
  <li>
    <p><strong>WikiHop</strong>：需要从维基百科的多个文档中进行推理，找到连接问题和答案的路径。</p>
  </li>
  <li>
    <p><strong>ComplexWebQuestions</strong>：基于WebQuestions数据集，问题更复杂，需要多步推理才能回答。</p>
  </li>
  <li>
    <p><strong>QAngaroo</strong>：包括WikiHop和MedHop两个子数据集，专注于跨文档的多跳推理。</p>
  </li>
  <li>
    <p><strong>OpenBookQA</strong>：要求模型结合常识和科学知识，进行多步推理来回答问题。</p>
  </li>
  <li>
    <p><strong>NarrativeQA</strong>：基于故事和情节的问题，需要理解上下文并进行深度推理。</p>
  </li>
</ol>

<p><strong>多跳任务的挑战：</strong></p>

<ul>
  <li>
    <p><strong>信息整合</strong>：模型需要有效地从多个来源检索和整合信息。</p>
  </li>
  <li>
    <p><strong>推理路径</strong>：需要找到正确的推理路径，避免干扰信息。</p>
  </li>
  <li>
    <p><strong>计算复杂性</strong>：多跳推理增加了计算和时间成本，挑战模型的效率。</p>
  </li>
  <li>
    <p><strong>可解释性</strong>：提供清晰的推理过程和证据对于模型的可信度至关重要。</p>
  </li>
</ul>

<p><strong>研究意义：</strong></p>

<p>多跳任务对于推进自然语言理解和人工智能推理能力具有重要意义。它们促使模型超越简单的模式匹配，发展出更深层次的理解和推理能力。这对于构建更智能、更可靠的AI系统，如智能问答、对话系统和决策支持系统，具有深远的影响。</p>

<p><strong>总结：</strong></p>

<p>HotpotQA等多跳任务通过设计需要跨越多个信息源的问题，评估模型的推理和信息整合能力。这些任务推动了自然语言处理领域在理解、推理和可解释性方面的研究和发展。</p>

<h3 id="loong基准">Loong基准</h3>

<p><strong>Loong基准</strong>（Loong Benchmark）是一个专门为测试大语言模型（LLMs）的长上下文理解能力而设计的评估框架，重点是多文档问答（QA）任务，旨在创建更符合现实的复杂场景。这一基准与传统的长上下文评估不同，它从金融报告、法律案件和学术论文这三个领域精心选择文档，以确保上下文的完整性和相关性。</p>

<p>Loong基准包含四个主要评估类别：</p>
<ol>
  <li><strong>聚光定位</strong>（Spotlight Locating） - 测试模型在多个文档中定位关键信息的能力。</li>
  <li><strong>比较</strong>（Comparison） - 评估模型跨文档比较信息的能力。</li>
  <li><strong>聚类</strong>（Clustering） - 基于语义相似性对相关信息进行分组。</li>
  <li><strong>推理链</strong>（Chain of Reasoning） - 检验模型在长上下文下进行逻辑推理的能力。</li>
</ol>

<p>此外，Loong任务集覆盖了不同长度的输入，从10K到超过200K个token，允许对模型在不同上下文长度和任务复杂性下的性能进行细粒度评估。任务以中英文形式呈现，更接近实际应用场景【20†source】【21†source】【23†source】。</p>

<h3 id="pocast-transcripts">Pocast Transcripts</h3>

<p><strong>Podcast Transcripts</strong> 数据集是专为音频媒体的自然语言处理（NLP）任务而设计的，旨在研究和处理播客的语音转录文本。这个领域最近受到了越来越多的关注，因为播客内容形式丰富，涉及新闻、对话、故事讲述等多种风格。以下是几种主要的 Podcast Transcripts 数据集：</p>

<ol>
  <li>
    <p><strong>Spotify Podcast Dataset</strong>：这是一个规模较大的数据集，包含约10万集播客，包含音频文件及其对应的自动语音识别（ASR）转录文本。该数据集包含来自全球不同地区的播客，涵盖多种音质和长度的音频内容，总时长超过5万小时【32†source】【33†source】。</p>
  </li>
  <li>
    <p><strong>PodcastFillers Dataset</strong>：该数据集专注于英语播客中的语气词和填充词标注，包含199集完整播客，共计145小时的音频。这些转录文本也由自动语音识别系统生成，专门用于分析语音中的自然停顿和填充词【32†source】。</p>
  </li>
</ol>

<p>这些数据集不仅提供了音频文件和转录文本，还包括丰富的元数据，如播客节目的标题、描述、时长、发布者等信息，有助于多维度的分析和处理，尤其适合于语音识别、文本摘要和对话建模等研究方向【34†source】。</p>

<h2 id="摘要">摘要</h2>

<blockquote>
  <p>思考的问题：</p>

  <p>原始文档-&gt;结构化的信息是怎么样的?</p>

  <p>怎么利用结构化的信息的？</p>
</blockquote>

<p>背景：检索增强生成（RAG）是在许多基于知识的任务中有效增强大型语言模型（LLM）的关键手段</p>

<p>面对的问题：</p>

<ul>
  <li>现有的RAG方法在知识密集型推理任务中存在困难，因为这些任务所需的有用信息分散且不规律</li>
  <li>这一特点使得现有的RAG方法很难准确识别关键信息并进行全局推理，因为存在噪声干扰</li>
</ul>

<p>解决办法：</p>

<ul>
  <li>本文通过人类在应对知识密集型推理时将原始信息转化为各种<strong>结构化知识</strong>的认知理论的启发，提出了一个新的框架，名为StructRAG</li>
  <li>该框架可以在进行任务时识别最佳结构类型，将原始文档重新构建为这种结构化格式，并根据生成的结构进行推理得出答案</li>
</ul>

<p>实验结果</p>

<ul>
  <li>在各种知识密集型任务上进行的大量实验表明，StructRAG实现了最先进的性能</li>
  <li>特别擅长在具有挑战性的场景中表现，展示了它作为增强LLMs在复杂现实世界应用中的有效解决方案的潜力。</li>
</ul>

<h2 id="引言">引言</h2>

<blockquote>
  <p>知识密集型推理任务（knowledeg- intensive reasoning task）是什么意思？</p>

  <p>怎么根据根据任务需求以最合适的结构？</p>

  <p>作者的LLM分散知识化结构 LLM-based scattered knowledge structurize是什么？</p>

  <p>怎么构建偏好训练数据的？</p>

  <p>数据是怎么样的？</p>

  <p>如何使用DPO训练混合结构路由器？</p>
</blockquote>

<h3 id="rag背景">RAG背景</h3>

<ul>
  <li>随着深度学习技术的进步，大规模语言模型（LLMs）在自然语言任务中展现出相当的优势，并广泛应用于复杂的现实世界场景（OpenAI等，2024年；Yang等，2024年a）。</li>
  <li>然而，由于缺乏领域特定知识、实时更新信息和专有知识，它们在事实任务中仍然存在局限性</li>
  <li>解决办法：RAG，通常，RAG方法涉及将原始文档分割成较短的部分，根据查询检索出最相关的部分，用这些部分使LLMs能够生成可靠的答案。</li>
</ul>

<h3 id="本文针对的rag问题背景">本文针对的RAG问题背景</h3>

<ul>
  <li>
    <p>当前的RAG方法无法有效处理<strong>知识密集型推理任务</strong>，因为解决这些任务所需的相关信息的分散性质</p>

    <ul>
      <li>
        <p>具体来说，知识密集型推理任务通常需要大量有用的信息，这些信息分散在提供的文档中的许多位置</p>
      </li>
      <li>
        <p>与此同时，模型需要在检索到有用信息后执行综合推理</p>

        <blockquote>
          <p>以财务报告分析为例，考虑到大量的财务文件和比较多家公司发展趋势的需求，LLM需要挖掘原始文件中散落的所有相关财务指标，然后通过仔细比较和全面分析这些指标来生成见解。</p>
        </blockquote>
      </li>
    </ul>
  </li>
  <li>
    <p>标准的RAG方法面临着精确检索所有相关文本块的挑战</p>

    <ul>
      <li>这些文本块可能包含大量噪音</li>
      <li>并且集成多个关键信息用于推理，导致这些任务的性能不尽如人意。</li>
    </ul>
  </li>
</ul>

<h3 id="人类的思考方式的启发">人类的思考方式的启发</h3>

<p>人们并不是通过简单阅读原始文本来解决知识密集型的推理任务</p>

<ul>
  <li>正如认知负荷理论所建议的，人类通常将文档中的零散信息总结为结构化知识，进而缩短推理路径并实现更准确的判断</li>
  <li>认知匹配理论表明，人类更喜欢在不同任务中使用不同类型的结构化知识，例如表格用于统计分析任务，图表用于长链推理</li>
</ul>

<p>启发LLMs推理采用人类的思维过程，<strong>将零散信息转化为各种结构格式，从而更好地服务于知识密集型的推理任务。</strong></p>

<h3 id="作者提出的解决办法">作者提出的解决办法</h3>

<p>我们提出了StructRAG，它采用<strong>混合信息结构化机制，根据任务需求以最合适的格式构建和利用结构化知识。</strong></p>

<p><img src="/assets/posts_assets/StructRAG%20Boosting%20Knowledge%20Intensive%20Reasoning%20of%20LLMs%20via%20Inference-time%20Hybrid%20Information%20Structurization%20%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB.assets/%E6%88%AA%E5%B1%8F2024-10-19%2011.20.30.png" alt="结构图" /></p>

<p>StructRAG框架包括三个模块，旨在顺序识别最合适的结构类型，构建以该格式的结构化知识，并利用该结构化知识来推断最终答案。</p>

<ul>
  <li>首先，认识到不同的结构类型适用于不同的任务，提出了混合结构路由器，以根据当前任务的问题和文档信息确定最适当的结构类型。</li>
  <li>其次，考虑到构建结构化知识是复杂的，并需要强大的理解和生成能力，采用了基于LLM的分散知识结构化程序，将原始文档转换为最佳类型的结构化知识。</li>
  <li>最后，由于知识密集推理任务中的问题通常是复杂的组合问题，难以直接解决，所以使用了结构化知识利用程序，对问题进行分解，并进行精确知识提取，以便更准确地推断答案。</li>
</ul>

<h3 id="具体的主要工作">具体的主要工作</h3>

<p>混合结构路由器能够准确选择每个输入任务的最适合结构类型</p>

<ul>
  <li>
    <p>为了赋予路由器这种能力，我们提出了一种<strong>混合结构路由器的训练方法</strong>：受到强化学习在训练LLMs进行决策任务方面成功应用的启发，我们采用DPO算法来训练路由器模块，该算法遵循强化学习原则，无需额外的奖励模型。</p>
  </li>
  <li>
    <p>然而，模型学习如何选择最佳结构类型的训练数据不足，而且在现实世界中收集足够的这类数据也具有挑战性。</p>
    <ul>
      <li>为了解决这个问题，<strong>我们引入了一种新颖的流程</strong>，用于<strong>构建偏好训练数据</strong>，其中包括任务合成、解决方案模拟和偏好判断，以创建高质量的合成数据，从而增强路由器选择适当结构类型的能力。</li>
    </ul>
  </li>
</ul>

<h3 id="实验">实验</h3>

<p>在我们的实验中，我们评估了StructRAG在各种知识密集型推理任务中的表现，并将其与几个强大的RAG基线进行了比较。结果表明，StructRAG取得了最先进的性能，<strong>在任务复杂性增加时改进更加显著。</strong></p>

<p>此外，与最近的图形RAG方法相比，StructRAG不仅在更广泛的任务范围内表现出卓越性能，而且平均操作速度也更快。</p>

<h2 id="相关工作">相关工作</h2>

<blockquote>
  <p><strong>HotpotQA等多跳任务是什么意思？</strong></p>
</blockquote>

<h3 id="rag">RAG</h3>

<p>RAG技术通过提供外部知识来帮助回答问题并减少幻觉，在LLM时代取得了良好的表现</p>

<ul>
  <li>RAG的初始策略涉及使用检索器根据查询搜索和保留知识库中高度相关的块，然后将这些块作为外部知识输入到生成模块中，以提高性能</li>
  <li>为了改善RAG的效果，一些方法引入了迭代RAG，提出了各种增强方法，如查询扩展和重写</li>
  <li>其他尝试改进检索和生成之间的协作</li>
  <li>尽管现有方法在HotpotQA等多跳任务上取得了强大的性能，基于块的RAG在知识密集型任务上仍面临困难</li>
</ul>

<h3 id="graph-rag">Graph RAG</h3>

<ul>
  <li>一种方法利用预先构建的知识图，根据查询提取子图，然后将其编码为软提示或压缩成纯文本，用于生成模块。</li>
  <li>另一种方法涉及根据查询要求从给定文本文档中提取实体关系三元组以构建图结构，然后用于知识增强。</li>
  <li>尽管这些方法明显改善了多跳问答任务的性能，但它们仅关注基于图的知识通过三元组的格式，因此限制了它们在各种领域和知识密集型推理任务应用中的实际适用性。</li>
</ul>

<h2 id="通过混合信息结构化来构建structrag">通过混合信息结构化来构建STRUCTRAG</h2>

<blockquote>
  <p>要是不能表示成该结构怎么办？</p>

  <p>感觉就是把原本LLM黑盒中的一些部分拿出来，单独训练，在组合在一起。</p>

  <p>核心内容C是怎么来的</p>
</blockquote>

<h3 id="任务建模">任务建模</h3>

\[a = F(q,D),where \ D = \{d^{(i)}\}^m_{i=1}\]

<ul>
  <li>
    <p>知识密集型推理任务提供了一个问题 q 和一个大量的文档集合 D 作为输入，其目标是根据提供的文档得出答案 a</p>
  </li>
  <li>
    <p>m 是文档的数量，可以超过 20，导致总token数达到 200K</p>
  </li>
  <li>
    <p>这些任务最明显的特征是有用信息分散在提供的文档中，要求模型基于大规模相关数据进行复杂推理。</p>

    <blockquote>
      <p>例如，在比较使用一批财务报告的几家公司的发展趋势时，任务需要检索分布在文档中的各种财务指标，然后详细比较这些指标。这涉及考虑不同指标的相对重要性以及数值差异的大小等因素。因此，知识密集型推理任务具有重大挑战。</p>
    </blockquote>
  </li>
</ul>

<h3 id="hybird-structure-router">Hybird Structure Router</h3>

\[t = R(q,C),where \ C = \{ c^{(i)}\}^m_{i=1}\]

<p><strong>混合结构路由器R来选择最佳的结构类型。</strong></p>

<ul>
  <li>
    <p>路由器利用问题q和文档D的核心内容C来做出决策并生成最适合的结构类型t，因为一次性处理整个文档集是不现实的。</p>
  </li>
  <li>C 是来自每个文档 d(i) 的标题或前几句话的集中体现。</li>
  <li>t 有五种候选结构类型，分别适用于五种知识密集型任务：
    <ul>
      <li><strong>表格</strong>适用于统计任务</li>
      <li><strong>图表</strong>适用于长链任务</li>
      <li><strong>算法</strong>适用于规划任务</li>
      <li><strong>目录</strong>适用于总结任务</li>
      <li>以及<strong>块</strong>适用于简单的单跳任务。</li>
    </ul>
  </li>
  <li>考虑到路由器在整体框架中的核心作用，我们的工作设计了一种基于DPO的训练方法，以开发一个在知识类型决策方面表现优异的路由器，详情请参考第4节。</li>
</ul>

<h3 id="scattered-knowledge-structurizer">Scattered Knowledge Structurizer</h3>

\[k^{(i)}_t,b^{(i)}_t=S(q,t,d^{(i)})\]

<p><strong>从原始文档中提取分散的文本知识，并将其重构为结构化知识</strong></p>

<p>StructRAG采用了基于LLM的分散知识结构器</p>

<ul>
  <li>结构化器S将问题q、选定的类型t和每个原始文档$d^{(i)}$作为输入，</li>
  <li>并通过LLM强大的理解和生成能力从文档中提取结构化知识$k^{(i)}_t$。此外，还生成了结构化知识$k^{(i)}_t$的描述$b^{(i)}_t$。</li>
  <li>然后，所有输出的结构化知识将被收集为整体知识$K_t = {k^{(i)}<em>t }^m</em>{i=1}$，并且整体结构化知识的描述将被构建为$B_t = {b^{(i)}<em>t }^m</em>{i=1}$。</li>
</ul>

<p>结构的形式</p>

<ul>
  <li>表格使用markdown语法</li>
  <li>图表使用头部-关系-尾部三元组列表，</li>
  <li>块使用普通文本</li>
  <li>算法使用伪代码</li>
  <li>目录使用带有层次编号的文本（例如，第一节，1.1，1.1.2）作为明确的章节标识符。</li>
</ul>

<h3 id="structured-knowledge-utilizer">Structured Knowledge Utilizer</h3>

<p>基于LLM的结构化知识利用器来促进问题分解、精确知识提取和最终答案推理
\(\hat{Q} = U_{decompose}(q, B_t) = \{\hat{q}^{(j)}\}^n_{j=1} \\ 
\hat{K_t} = {U_{extract}(\hat{q}^{(j)}, K_t)}^n_{j=1} = \{\hat{k}^{(j)}_t \}^n_{j=1}  \\
a = U_{infer}(q, \hat{Q}, \hat{K}_t)\)</p>

<ul>
  <li>利用者的分解过程以原始问题q和结构化知识$B_t$的整体描述作为输入，将问题分解为几个简单直观的子问题$\hat{q}^{(j)}$。</li>
  <li>接下来，提取过程旨在从整个结构化知识$K_t$中为每个子问题$\hat{q}{(j)}$找到精确的知识$\hat{k}^{(j)}_t$。最后，推理过程整合所有子问题及其提取的精确知识，生成最终答案a，</li>
  <li>N 是子问题的数量，$\hat{Q}$是所有子问题的集合，$\hat{K}<em>t$是所有子问题的完整精确知识，而$U</em>{decompose}$、$U_{extract}$和$U_{infer}$分别是分解、提取和推理过程。</li>
</ul>

<h2 id="训练混合结构路由器">训练混合结构路由器</h2>

<ul>
  <li>训练方法：鉴于强化学习在决策场景中具有强大的能力，我们使用DPO算法训练路由器，该算法实现了类似于强化学习的结果，同时避免了额外奖励模型的需求。</li>
  <li>训练数据：由于不存在用于最佳结构类型选择任务的现有偏好数据，我们设计了一种<strong>合成-模拟-判断</strong>方法，以便高效地构建训练的优先对。在以下段落中提供了详细的解释，附录A.1中提供了示例和提示。</li>
</ul>

<h3 id="数据构建">数据构建</h3>

<p><img src="/assets/posts_assets/StructRAG%20Boosting%20Knowledge%20Intensive%20Reasoning%20of%20LLMs%20via%20Inference-time%20Hybrid%20Information%20Structurization%20%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB.assets/%E5%9B%BE%E7%89%87.png" alt="图片" /></p>

<ol>
  <li>给定涵盖可能结构类型的几个手工收集的种子任务</li>
  <li>首先使用LLMs通过上下文学习方法合成一组新任务，其中每个任务包含一个问题和文档的核心内容。</li>
  <li>然后，对于每个合成任务，LLMs被用来模拟以不同类型的结构知识解决此任务的过程，从而获得不同的模拟解决方案</li>
  <li>最后，基于LLM的评委比较这些模拟解决方案来解决任务，生成关于结构类型的偏好对。</li>
</ol>

<p>结果：每个构建的数据条目包括一个问题，文档的核心内容，选择的结构类型和拒绝的结构类型</p>

\[D_{synthenic}= \{q^{(k)},C^{(k)},t_w^{(k)},t_l^{k}\}^N_{k=1}\]

<p>$t_w$和$t_l$分别是选择的结构类型和被拒绝的结构类型。合成偏好对包括英语和中文数据，以提高普适性。</p>

<h3 id="偏好训练">偏好训练</h3>

<p>这个公式是关于<strong>DPO (Direct Preference Optimization)</strong> 的损失函数，通常用于训练强化学习中的策略模型。该损失函数的目标是使模型更符合用户偏好。</p>

\[\mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(q, C, t_w, t_l) \sim D_{\text{synthetic}}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(t_w \mid q, C)}{\pi_{\text{ref}}(t_w \mid q, C)} - \beta \log \frac{\pi_\theta(t_l \mid q, C)}{\pi_{\text{ref}}(t_l \mid q, C)} \right) \right]\]

<p>πθ和πref分别是目标策略和参考策略，β是超参数。</p>

<p>如后分析，这种偏好训练使模型能够区分各种类型的知识以及它们对于特定任务的适用性，从而实现更好的性能，相较于零样本和少样本设置。</p>

<h2 id="实验-1">实验</h2>

<h3 id="评估数据集">评估数据集</h3>

<ul>
  <li>Loong benchmark</li>
  <li>Pocast Transcripts</li>
</ul>

<h3 id="实现细节">实现细节</h3>

<p>我们基于Qwen2系列模型构建了框架（杨等人，2024a）。</p>

<ul>
  <li>
    <p>对于混合结构路由器，StructRAG使用Qwen2-7B-Instruct作为基础模型，并通过trl2实现DPO训练。</p>

    <p>关于混合结构路由器训练的细节，StructRAG构建并使用了总共900份偏好数据，将学习率设置为1e-5，将训练轮数设置为3，β参数保持默认。</p>
  </li>
  <li>
    <p>对于散乱知识结构化器和结构化知识利用器，StructRAG直接使用Qwen2-72B-Instruct作为基础模型，并使用vllm3将模型部署为API，按照Loong（王等人，2024a）的设置进行</p>
  </li>
</ul>

<h3 id="实验结果">实验结果</h3>

<p><img src="/assets/posts_assets/StructRAG%20Boosting%20Knowledge%20Intensive%20Reasoning%20of%20LLMs%20via%20Inference-time%20Hybrid%20Information%20Structurization%20%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB.assets/%E5%9B%BE%E7%89%87-9508638.png" alt="图片" /></p>

<ul>
  <li>StructRAG对于复杂任务尤为合适，性能提升在信息更分散的场景中变得更加重要。</li>
  <li>StructRAG是应对知识密集型推理任务的强大解决方案。</li>
</ul>

<h3 id="消融实验">消融实验</h3>

<p>消融（即逐步移除）模型中的不同模块或组件，来观察模型性能的变化。这样可以帮助他们了解模型中不同部分对最终性能的贡献，并且有助于确定哪些模块对模型性能起到了关键作用</p>

<p><img src="/assets/posts_assets/StructRAG%20Boosting%20Knowledge%20Intensive%20Reasoning%20of%20LLMs%20via%20Inference-time%20Hybrid%20Information%20Structurization%20%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB.assets/image-20241021193022347.png" alt="image-20241021193022347" /></p>

<p>“w/o router”指的是随机路由，“w/o structurizer”意味着仅使用块，“w/o utilizer”是指直接将结构化知识与原始问题连接起来以生成答案。以下是结论：</p>

<p>结论</p>

<ul>
  <li>所有三个模块都对整体框架起到积极的作用。</li>
  <li>选择合适的结构类型并将文件构建为结构化知识比设计复杂的利用方法更为重要。</li>
</ul>

<h3 id="其他细节分析">其他细节分析</h3>

<p><img src="/assets/posts_assets/StructRAG%20Boosting%20Knowledge%20Intensive%20Reasoning%20of%20LLMs%20via%20Inference-time%20Hybrid%20Information%20Structurization%20%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB.assets/image-20241021200255458.png" alt="image-20241021200255458" /></p>

<p>为了探讨构建数据和进行DPO培训的必要性，以及混合结构路由器的性能与整体StructRAG之间的关系，我们首先将我们的路由器与原始LLMs进行比较，然后绘制路由器的曲线和整体StructRAG分数。得出以下结论：</p>

<ol>
  <li>
    <p><strong>对于没有接受特殊培训的原始LLMs来说，根据任务选择最佳类型的知识是具有挑战性的。</strong></p>

    <p>根据表4中的实验结果，基于Qwen2-7B-Instruct模型训练的路由器在少样本设置下明显优于72B模型。这表明，即使模型规模达到72B，LLMs仍需要接受一些特殊培训才能具备根据任务需求选择最佳结构类型的能力。</p>
  </li>
  <li>
    <p><strong>混合结构路由器的性能与StructRAG最终性能有显著相关性。</strong>如图3所示，我们选择Qwen2-72B-Instruct（零样本）作为弱路由器，并设计了一个完全随机的路由器和一个完全不正确的糟糕路由器。图中的曲线明确显示了路由器准确性与StructRAG框架整体性能之间的正相关关系。这进一步证明，在知识密集型推理任务中，选择与任务需求相匹配的知识类型对于增强至关重要。</p>
  </li>
</ol>

<h4 id="固定结构的缺点分析">固定结构的缺点分析</h4>

<p>仅使用一种固定类型的知识无法在多样化的任务上取得良好表现。</p>

<p><img src="/assets/posts_assets/StructRAG%20Boosting%20Knowledge%20Intensive%20Reasoning%20of%20LLMs%20via%20Inference-time%20Hybrid%20Information%20Structurization%20%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB.assets/image-20241021195930009.png" alt="image-20241021195930009" /></p>

<h4 id="几个em表现不佳的原因">几个EM表现不佳的原因</h4>

<p>根据表1中的实验结果，StructRAG在总体得分上超过了基准线，但在七种子情况的确切匹配率上表现不佳。</p>

<p>因此，我们分析了一些StructRAG方法得分较高但未能达到精确匹配的情况。</p>

<p>原因主要是关于结构化过程可能会改变原始信息的文本格式。如表5所示，结构化知识和原始信息之间存在一些措辞的差异（例如，从原始“$1,308,463”变为表中的“138463”）。直观上，这符合常识，结构化程序是一种概率语言模型，而不是基于规则的模型，因此可能无法避免一些可能的文本损失，GraphRAG方法的输出也存在类似问题。</p>

<h3 id="性能报告">性能报告</h3>

<p><img src="/assets/posts_assets/StructRAG%20Boosting%20Knowledge%20Intensive%20Reasoning%20of%20LLMs%20via%20Inference-time%20Hybrid%20Information%20Structurization%20%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB.assets/image-20241021201118603.png" alt="image-20241021201118603" /></p>

<p>第一部分是构建延迟，指的是为RQ-RAG迭代检索块，为GraphRAG构建图形，并确定最佳知识类型并为StructRAG构建相应结构的过程。</p>

<p>第二部分是阅读延迟，指的是使用增强知识生成最终答案的过程。与RQ-RAG相比，StructRAG的延迟略高，但显然比GraphRAG快。</p>

<p>因此，StructRAG是一种具有可用实现速度的高性能框架。</p>]]></content><author><name>Xingjie Gao</name><email>xingjie-gao@outlook.com</email></author><category term="nlp" /><category term="自然语言处理" /><category term="论文笔记" /><summary type="html"><![CDATA[StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization 论文精读]]></summary></entry><entry><title type="html">博客的开头</title><link href="http://localhost:4000/2024/10/10/%E5%8D%9A%E5%AE%A2%E7%9A%84%E5%BC%80%E5%A4%B4.html" rel="alternate" type="text/html" title="博客的开头" /><published>2024-10-10T00:00:00+08:00</published><updated>2024-10-10T00:00:00+08:00</updated><id>http://localhost:4000/2024/10/10/%E5%8D%9A%E5%AE%A2%E7%9A%84%E5%BC%80%E5%A4%B4</id><content type="html" xml:base="http://localhost:4000/2024/10/10/%E5%8D%9A%E5%AE%A2%E7%9A%84%E5%BC%80%E5%A4%B4.html"><![CDATA[<h1 id="博客的开头">博客的开头</h1>
<p>一直想要搞一个博客，但是奈何没有时间，现在终于闲下来了，找到网上的教程，做了这么个简陋的博客。
记忆会消失，文字不会，所以想要用博客来记录自己的成长，记录自己的学习，同时希望可以能帮助到你。</p>]]></content><author><name>Xingjie Gao</name><email>xingjie-gao@outlook.com</email></author><summary type="html"><![CDATA[博客的开头 一直想要搞一个博客，但是奈何没有时间，现在终于闲下来了，找到网上的教程，做了这么个简陋的博客。 记忆会消失，文字不会，所以想要用博客来记录自己的成长，记录自己的学习，同时希望可以能帮助到你。]]></summary></entry></feed>